## 计算机

### 硬件

#### CPU与显卡

电脑中只有两个是真正的运算硬件，一个是CPU，另外一个就是显卡。

如果把所有的运算都交给CPU，那么CPU表示压力很大，尤其是巨大的图形运算，如果有一块显卡的话，CPU就可以把图形命令交给显卡，自己用更多的资源处理其他更重要的事情，而专业处理图形命令的显卡就显得物尽其才了，把CPU下达的图形运算命令接受由SP接手处理图形数据，然后转化为模拟信号储存到显存容量中，然后用到了就传输到显示器，通过显示器解码变为物理信号呈现画面。 我们感觉的系统流畅度就是因为CPU压力太大，不仅要处理其他层序还要处理图形数据，压力太大，没有更多资源来处理当前事物机会感觉卡了。

SP单元是显卡的核心。SP单元也叫流处理单元（Stream Processor），这是NVIDIA对其新一代显卡的核心架构，采用统一渲染架构能有效而充分的利用显卡资源，不再需要上一代显卡依靠渲染管线执行命令排队并逐个执行，统一渲染架构可以做到充分利用闲置显卡资源。SP单元个数越多，则处理能力越强，一般成正比关系。

GPU长得和CPU 有点像，如果是独立的显卡，一般它就在显卡的那个板上，位置在风扇下面。 如果是集成显卡，这个时候一般GPU就和CPU整合在一起的，它这个时候和CPU共用风扇还有内存 。 因此GPU实际上就是显卡的核心部件，显卡主要就是靠它来工作的。

#### CPU与GPU

CPU(Central Processing Unit-中央处理器),是一块超大规模的集成电路，其中包含ALU算术逻辑运算单元、Cache高速缓冲存储器以及Bus总线。CPU是一台计算机的运算核心（Core）和控制核心（ Control Unit）。它的功能主要是解释计算机指令以及处理计算机软件中的数据。

GPU（Graphics Processing Unit-图形处理器），是一种专门在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上图像运算工作的微处理器。

CPU基于低延时，GPU基于大的吞吐量。二者出于设计目的的不同，呈现出不同的架构。总体上来说二者都是由控制器（Control），寄存器（Cache、DRAM）和逻辑单元（ALU：Arithmetic Logic Unit）构成。但是三者的比例却有很大的不同。

![img](https://gss0.baidu.com/-fo3dSag_xI4khGko9WTAnF6hhy/zhidao/wh%3D600%2C800/sign=2539828a3bd12f2ece50a6667ff2f95a/9922720e0cf3d7ca251edc2bff1fbe096a63a987.jpg)

可以看出，GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了Cache。而CPU不仅被Cache占据了大量空间，而且还有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是CPU很小的一部分。

应用方向

CPU所擅长的像操作系统这一类应用，需要快速响应实时信息，需要针对延迟优化，所以晶体管数量和能耗都需要用在分支预测、乱序执行、低延迟缓存等控制部分。

GPU适合对于具有极高的可预测性和大量相似的运算以及高延迟、高吞吐的架构运算。

#### GPU与DSP



#### 数据存储单位

计算机中的数据和指令都是用二进制表示的，共有3个单位：位、字节、和字。

- 位（bit）

位是指二进制的一位，是计算机存储数据的最小单位。bit是位的英文名成，音译比特，常用b表示，计算机中一位只能表示1和0两个状态。**与传输速度有关的b一般指的是bit。** bps 是 bits per second 的简称。一般数据机及网络通讯的传输速率都是以bps为单位。如56Kbps、100.0Mbps 等等。 

- 字节（byte）

字节是最基本的数据单位，8位二进制为一个字节，英文名称byte。常用B表示。一个英文字符和英文标点占用一个字节，一个中文字符和中文标点占用2个字节。**与容量有关的b一般指的是byte。**Bps即是Byte per second 的简称。而电脑一般都以Bps 显示速度，如1Mbps 大约等同 128 KBps。

- 字（word）

字是计算机进行数据处理时，一次存取、加工和传送的数据长度。对于一种给定的计算机设计，字(word)是自然的存储单位。一般说来，计算机在同一时间内处理的一组二进制数称为一个计算机的“字”，而这组二进制数的位数就是“字长”。字长常常成为一台计算机性能的标志，字长直接反映了一台计算机的计算精度，为适应不同的要求及协调运算精度和硬件造价间的关系，大多数计算机均支持变字长运算，即机内可实现半字长、全字长（或单字长）和双倍字长运算。

字长有一定的位数，可分成若干字长段，各段的编码表示不同的含义，例如某台计算机字长为16位，即有16个二进制数合成一条指令或其它信息。16个0和1可组成各种排列组合，通过线路变成电信号，让计算机执行各种不同的操作。32位CPU就是在同一时间内处理字长为32位的二进制数据。字长受软件系统的制约，例如，在32位软件系统（如操作系统）中64位字长的CPU只能当32位用。常用字长8位、16位、32位、64位。

- 存储容量单位

位和字节是最常用的信息单位。计算单位有：B、KB、GB、TB。1KB=1024B=2^10 bytes；1MB=1024KB=2^20 bytes；1GB=1024MB；1TB=1024GB。

#### 存储体系

1. 计算机的存储体系（Memory hierarchy）金字塔

![img](https://img-blog.csdn.net/20160730162655871)

- 寄存器是中央处理器内的组成部份。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。寄存器是CPU内部的元件，寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。容量小于1kb。
- 内存包含的范围非常广，一般分为只读存储器（ROM）、随机存储器（RAM）和高速缓存存储器（cache）。
- Cache ：即高速缓冲存储器，是位于CPU与主内存间的一种容量较小但速度很高的存储器。高速缓存是内存的部分拷贝。由于CPU的速度远高于主内存，CPU直接从内存中存取数据要等待一定时间周期，Cache中保存着CPU刚用过或循环使用的一部分数据，当CPU再次使用该部分数据时可从Cache中直接调用,这样就减少了CPU的等待时间,提高了系统的效率。Cache又分为一级Cache(L1 Cache)和二级Cache(L2 Cache)，L1 Cache集成在CPU内部，L2 Cache早期一般是焊在主板上,现在也都集成在CPU内部，常见的容量有256KB或512KB L2 Cache。L1缓存分两种：L1指令缓存(L1-icache)和L1数据缓存(L1-dcache)。L1指令缓存用来存放已解码指令，L1数据缓存用来放访问非常频繁的数据。L2缓存用来存放近期使用过的内存数据。更严格地说，存放的是很可能将来会被CPU使用的数据。

总结：大致来说数据是通过内存-Cache-寄存器，Cache缓存则是为了弥补CPU与内存之间运算速度的差异而设置的的部件。

2. 寄存器与内存的区别：

寄存器的工作方式很简单，只有两步：（1）找到相关的位，（2）读取这些位。

内存的工作方式就要复杂得多：

（1）找到数据的指针。（指针可能存放在寄存器内，所以这一步就已经包括寄存器的全部工作了。）

（2）将指针送往内存管理单元（MMU），由MMU将虚拟的内存地址翻译成实际的物理地址。

（3）将物理地址送往内存控制器（memory controller），由内存控制器找出该地址在哪一根内存插槽（bank）上。

（4）确定数据在哪一个内存块（chunk）上，从该块读取数据。

（5）数据先送回内存控制器，再送回CPU，然后开始使用。

内存的工作流程比寄存器多出许多步。每一步都会产生延迟，累积起来就使得内存比寄存器慢得多。

为了缓解寄存器与内存之间的巨大速度差异，硬件设计师做出了许多努力，包括在CPU内部设置缓存、优化CPU工作方式，尽量一次性从内存读取指令所要用到的全部数据等等。

#### CPU和程序的执行

[取指、解码、执行](https://blog.csdn.net/Rong_Toa/article/details/98449895)三个过程构成一个CPU的基本周期。

当程序要执行的部分被装载到内存后，CPU要从内存中取出指令，然后指令解码(以便知道类型和操作数，简单的理解为CPU要知道这是什么指令)，然后执行该指令。再然后取下一个指令、解码、执行，以此类推直到程序退出。为了改善性能，CPU已经不是单条取指-->解码-->执行的路线，而是分别为这3个过程分别提供独立的取值单元，解码单元以及执行单元。这样就形成了流水线模式。

例如，流水线的最后一个单元——执行单元正在执行第n条指令，而前一个单元可以对第n+1条指令进行解码，再前一个单元即取指单元可以去读取第n+2条指令。这是三阶段的流水线，还可能会有更长的流水线模式。

更优化的CPU架构是superscalar架构（超标量架构）。这种架构将取指、解码、执行单元分开，有大量的执行单元，然后每个取指+解码的部分都以并行的方式运行。比如有2个取指+解码的并行工作线路，每个工作线路都将解码后的指令放入一个缓存缓冲区等待执行单元去取出执行。

![img](https://img-blog.csdnimg.cn/20190804105729209.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JvbmdfVG9h,size_16,color_FFFFFF,t_70)

由于CPU访问内存以得到指令或数据的时间要比执行指令花费的时间长很多，因此在CPU内部提供了一些用来保存关键变量、临时数据等信息的通用寄存器。所以，CPU需要提供 一些特定的指令，使得可以从内存中读取数据存入寄存器以及可以将寄存器数据存入内存。此外还需要提供加法、减、not/and/or等基本运算指令，而乘除法运算都是推算出来的。

每个CPU都有一套自己可以执行的专门的指令集(注意，这部分指令是CPU提供的，CPU-Z软件可查看)。正是因为不同CPU架构的指令集不同，使得x86处理器不能执行ARM程序，ARM程序也不能执行x86程序。（Intel和AMD都使用x86指令集，手机绝大多数使用ARM指令集）。

注：指令集的软硬件层次之分：硬件指令集是硬件层次上由CPU自身提供的可执行的指令集合。软件指令集是指语言程序库所提供的指令，只要安装了该语言的程序库，指令就可以执行。

在CPU进行进程切换的时候，需要将寄存器中和当前进程有关的状态数据写入内存对应的位置(内核中该进程的栈空间)保存起来，当切换回该进程时，需要从内存中拷贝回寄存器中。即上下文切换时，需要保护现场和恢复现场。

除了嵌入式系统，多数CPU都有两种工作模式：内核态和用户态。这两种工作模式是由PSW寄存器上的一个二进制位来控制的。

内核态的CPU，可以执行指令集中的所有指令，并使用硬件的所有功能。用户态的CPU，只允许执行指令集中的部分指令。一般而言，IO相关和把内存保护相关的所有执行在用户态下都是被禁止的，此外其它一些特权指令也是被禁止的，比如用户态下不能将PSW的模式设置控制位设置成内核态。

用户态CPU想要执行特权操作，需要发起系统调用来请求内核帮忙完成对应的操作。其实是在发起系统调用后，CPU会执行trap指令陷入(trap)到内核。当特权操作完成后，需要执行一个指令让CPU返回到用户态。除了系统调用会陷入内核，更多的是硬件会引起trap行为陷入内核，使得CPU控制权可以回到操作系统，以便操作系统去决定如何处理硬件异常。

#### CPU的基本组成

除了通用寄存器，还有一些特殊的寄存器。典型的如：

- PC：program counter，表示程序计数器，它保存了将要取出的下一条指令的内存地址，指令取出后，就会更新该寄存器指向下一条指令。

- 堆栈指针：指向内存当前栈的顶端，包含了每个函数执行过程的栈帧，该栈帧中保存了该函数相关的输入参数、局部变量、以及一些没有保存在寄存器中的临时变量。

- PSW：program status word，表示程序状态字，这个寄存器内保存了一些控制位，比如CPU的优先级、CPU的工作模式(用户态还是内核态模式)等。

- MAR: memory address register，保存将要被访问数据在内存中哪个地址处，保存的是地址值

- MDR: memory data register，保存从内存读取进来的数据或将要写入内存的数据，保存的是数据值

- AC: Accumulator，保存算术运算和逻辑运算的中间结果，保存的是数据值

- CIR: current instruction register，保存当前正在执行的指令

CPU还要将一些常用的基本运算工具(如加法器)放进CPU，这部分负责运算，称为算术逻辑单元(ALU, Arithmetic Logic Unit)。

CPU中还有一个控制器(CU, Control Unit)，负责将存储器中的数据送到ALU中去做运算，并将运算后的结果存回到存储器中。控制器还包含了一些控制信号。

控制器之所以知道数据放哪里、做什么运算(比如是做加法还是逻辑运算?)都是由指令告诉控制器的，每个指令对应一个基本操作，比如加法运算对应一个指令。

例如，将两个MDR寄存器(保存了来自内存的两个数据)中的值拷贝到ALU中，然后根据指定的操作指令执行加法运算，将运算结果拷贝会一个MDR寄存器中，最后写入到内存。

下图是冯诺依曼结构图，也就是现在计算机的结构图。

![img](https://img-blog.csdnimg.cn/20190804105824451.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JvbmdfVG9h,size_16,color_FFFFFF,t_70)

#### CPU的多核和多线程

CPU的物理个数由主板上的插槽数量决定，每个CPU可以有多核心，每核心可能会有多线程。

多核CPU的每核(每核都是一个小芯片)，在OS看来都是一个独立的CPU。

对于超线程CPU来说，每核CPU可以有多个线程(数量是两个，比如1核双线程，2核4线程，4核8线程)，每个线程都是一个虚拟的逻辑CPU(比如windows下是以逻辑处理器的名称称呼的)，而每个线程在OS看来也是独立的CPU。

多线程没有提供真正意义上的并行处理，每核CPU在某一时刻仍然只能运行一个进程，因为线程1和线程2是共享某核CPU资源的。可以简单的认为每核CPU在独立执行进程的能力上，有一个资源是唯一的，线程1获取了该资源，线程2就没法获取。

但是，线程1和线程2在很多方面上是可以并行执行的。比如可以并行取指、并行解码、并行执行指令等。所以虽然单核在同一时间只能执行一个进程，但线程1和线程2可以互相帮助，加速进程的执行。

并且，如果线程1在某一时刻获取了该核执行进程的能力，假设此刻该进程发出了IO请求，于是线程1掌握的执行进程的能力，就可以被线程2获取，即切换到线程2。这是在执行线程间的切换，是非常轻量级的。(WIKI: if resources for one process are not available, then another process can continue if its resources are available)



#### CPU用户态与内核态



#### CPU主频

CPU的主频指的是CPU内部的数字时钟信号频率，又称为时钟频率，不等于一秒钟可以执行的指令个数，每个指令的执行成本是不同的。因此它并不能代表CPU的真实性能水平，4GHz的CPU不一定就比3GHz的强，至少我们不能一概而论。但是时钟频率的高低确实关系到一个CPU的运算速度。

在CPU这个复杂的数字系统中，为了确保内部所有硬件单元能够协同快速工作，CPU架构工程师们往往会设计一套时钟信号与系统同步进行操作。时钟信号是由一系列的脉冲信号构成，并且总是按一定电压幅度、时间间隔连续发出的方波信号，它周期性地在0与1之间往复变化。如下图所示。

![img](http://img.expreview.com/review/2017/10/CPU%20Clock/clock.jpg)

在第一脉冲和第二个脉冲之间的时间间隔称之为周期，它的单位是秒（s）。但单位时间1s内所产生的脉冲个数称之为频率，频率的最基本计量单位就是赫兹Hz。也就是说，时钟频率（f）与周期（T）两者互为倒数：f=1/T。

这个公式表明的就是频率表示时钟在1秒钟内重复的次数，而目前的CPU普遍已经处于GHz级，也就是说每秒钟产生10亿个脉冲信号。

**时钟周期作为CPU操作的最小时间单位**，内部的所有操作都是以这个时钟周期作为基准。一般来说CPU都是以时钟脉冲的上升沿作为执行指令的基准，频率越高，CPU执行的指令数越多，工作速度越快。比如对于一个操作内存的加法，就需要5个时钟周期，换句话说，500Mhz主频的CPU，最多执行100MHz条指令。

#### 总线 流水线



数据总线DB用于传送数据信息。数据总线是双向三态形式的总线，即它既可以把CPU的数据传送到存储器或I/O接口等其它部件，也可以将其它部件的数据传送到CPU。数据总线的位数是微型计算机的一个重要指标，通常与微处理的字长相一致。例如Intel8086微处理器字长16位，其数据总线宽度也是16位。需要指出的是，数据的含义是广义的，它可以是真正的数据，也可以指令代码或状态信息，有时甚至是一个控制信息，因此，在实际工作中，数据总线上传送的并不一定仅仅是真正意义上的数据。

地址总线AB是专门用来传送地址的，由于地址只能从CPU传向外部存储器或I/O端口，所以地址总线总是单向三态的，这与数据总线不同。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微机的地址总线为16位，则其最大可寻址空间为2^16=64KB，16位微型机的地址总线为20位，其可寻址空间为2^20=1MB。一般来说，若地址总线为n位，则可寻址空间为2^（n-10）千字节。

控制总线CB用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和I/O接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的，比如：中断申请信号、复位信号、总线请求信号、限备就绪信号等。因此，控制总线的传送方向由具体控制信号而定，一般是双向的，控制总线的位数要根据系统的实际控制需要而定。实际上控制总线的具体情况主要取决于CPU。



#### 半导体、芯片、集成电路

一般情况下，半导体、集成电路、芯片这三个东东是可以划等号的，因为讲的其实是同一个事情。

芯片，又称微电路（microcircuit）、微芯片（microchip）、集成电路（integrated circuit， IC），是指内含集成电路的硅片，体积很小，常常是计算机或其他电子设备的一部分。

芯片（chip）就是半导体元件产品的统称，是集成电路（IC， integrated circuit）的载体，由晶圆分割而成。

半导体（ semiconductor），指常温下导电性能介于导体（conductor）与绝缘体（insulator）之间的材料。如二极管就是采用半导体制作的器件。半导体是指一种导电性可受控制，范围可从绝缘体至导体之间的材料。常见的半导体材料有硅、锗、砷化镓等，而硅更是各种半导体材料中，在商业应用上最具有影响力的一种。

集成电路是20世纪50年代后期一60年代发展起来的一种新型半导体器件。它是经过氧化、光刻、扩散、外延、蒸铝等半导体制造工艺，把构成具有一定功能的电路所需的半导体、电阻、电容等元件及它们之间的连接导线全部集成在一小块硅片上，然后焊接封装在一个管壳内的电子器件。其封装外壳有圆壳式、扁平式或双列直插式等多种形式。



### 操作系统

操作系统（OS）是管理计算机硬件并为程序提供服务的软件集合。具体来说，它隐藏硬件复杂性，管理计算资源，并提供隔离和保护。最重要的是它可以直接访问底层硬件。操作系统的主要组件是文件系统、调度程序和设备驱动程序。

#### 进程内存布局

每个进程都有独立的虚拟地址空间，进程访问的虚拟地址并不是真正的物理地址。

虚拟地址可通过每个进程上的页表(在每个进程的内核虚拟地址空间)与物理地址进行映射，获得真正物理地址。

如果虚拟地址对应物理地址不在物理内存中，则产生缺页中断，进程会陷入内核态，真正分配物理地址，同时更新进程的页表；如果此时物理内存已耗尽，则根据内存替换算法淘汰部分页面至物理磁盘中。 

- 程序是静态的概念，表现形式为一个可执行文件
- 进程是动态的概念，程序由操作系统加载运行后得到进程

进程是程序的一次执行。进程的执行必须以顺序方式进行。简单来说，我们将计算机程序写入文本文件中，当我们执行该程序时，它将成为一个进程完成在程序中提及的所有任务。

**当一个程序被加载到内存中并成为一个进程时，它可以分为四个部分——堆栈、堆、文本和数据。**下图显示了主存中进程的简化布局。在Linux系统上，程序被载入内存时，内核为用户进程地址空间建立了代码段、数据段和堆栈段，在数据段与堆栈段之间的空闲区域用于动态内存分配。

![img](https://img-my.csdn.net/uploads/201209/19/1347986030_2883.gif)

堆栈stack：由编译器自动分配释放，进程堆栈包含临时数据，如方法/函数参数、返回地址和局部变量。其操作方式类似于数据结构中的栈。由系统管理，由高地址向低地址扩展。**栈用于存放局部变量和进程上下文，一般为 8M。**

堆heap：这是在进程运行期间动态分配给进程的内存。动态内存，由用户管理。通过malloc/alloc/realloc、new/new[]申请空间，通过free、delete/delete[]释放所申请的空间。若程序员不释放，程序结束时可能由 OS 回收。它与数据结构中的堆是两回事，但分配方式倒类似于链表。由低地址向高地址扩展。

C语言跟内存申请相关的函数主要有 alloc,calloc,malloc,free,realloc,sbrk等。其中alloc是向栈申请内存，因此无需释放。malloc分配的内存是位于堆中的，并且没有初始化内存的内容，因此**基本上malloc之后，调用函数memset来初始化这部分的内存空间**。calloc则将初始化这部分的内存，设置为0。而realloc则对malloc申请的内存进行大小的调整。申请的内存最终需要通过函数free来释放。而sbrk则是增加数据段的大小；malloc/calloc/free基本上都是C函数库实现的，跟OS无关，相应效率较低。

从操作系统角度来看，进程分配内存有两种方式，分别由两个**系统调用**完成：brk和mmap（不考虑共享内存）。两者在malloc与free时都有区别。

如图所示，当malloc小于128k的内存，使用brk分配内存，如A与B。malloc大于128k的内存，使用mmap分配内存，如C。

- brk是将虚拟内存的数据段(.data)的最高地址指针_edata往高地址推(只分配虚拟空间，不对应物理内存(因此没有初始化)，第一次读/写数据时，引起内核缺页中断，内核才分配对应的物理内存，然后虚拟地址空间建立映射关系)。当malloc/free申请释放内存小于某个阈值（一般操作系统设定为128K，可以修改）时，通过brk/sbrk系统调用，控制堆顶指针向高地址偏移（malloc）或者低地址偏移（free）；

- mmap是在进程的虚拟地址空间中（堆和栈中间，称为文件映射区域的地方）找一块空闲的虚拟内存(对应独立内存，而且初始化为0)。文件映射区也是动态内存，当malloc/free申请释放内存大于128K时，通过mmap系统调用分配一块虚拟地址空间。为了简单起见，图片中省略了内存映射文件。

brk分配的内存需要等到高地址内存释放以后才能释放（例如，在B释放之前，A是不可能释放的，这就是内存碎片产生的原因），而mmap分配的内存可以单独释放。堆内碎片不能直接释放，导致疑似“内存泄露”的问题。

- 如图所示。进程调用free(B)以后，B对应的虚拟内存和物理内存都没有释放，因为只有一个_edata指针，而B这块内存是可以重用的。堆是一个连续空间，并且堆内碎片由于没有归还 OS ，因此重用碎片时，再次访问该内存很可能不需产生任何系统调用和缺页中断，这将大大降低 CPU 的消耗。 **缺页中断是内核行为，会导致内核态CPU消耗较大。**当最高地址空间的空闲内存超过128K（可由M_TRIM_THRESHOLD选项调节）时，执行内存紧缩操作（trim）。当free时，发现最高地址空闲内存超过128K，于是内存紧缩。

- 进程调用free(C)以后，C对应的虚拟内存和物理内存一起释放。

![img](https://img-blog.csdn.net/20150114145652375?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ2ZnZHNn/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

数据data：全局变量和静态变量的存储是放在一块的。程序结束后由系统释放。

- 未初始化数据段：未初始化的全局变量以及静态变量，.bbs。该段中的变量在执行之前初始化为0或NULL。
- 初始化数据段：已初始化的全局变量和静态变量，.data。在 C++中，已经不再严格区分bss和 data了，它们共享一块内存区域。
- 文字常量区：只读文本段中的数据段，.rodata—read only data。字符串常量就是放在这里的，程序结束后由系统释放。

文本text：代码（存放函数体的二进制代码），包括由程序计数器值和处理器寄存器内容表示的当前活动。文本段是不可修改的，只读。

注意：

程序空间：

- 静态存储区通常指程序中的 .bss 和 .data 段
- 只读存储区通常指程序中的 .rodata 段
- 动态空间为堆中的空间
- 局部变量所占空间为栈上的空间
- 程序可执行代码存放于 .text 段

栈与堆的区别：

- 分配方式：**堆都是动态分配的，没有静态分配的堆；栈有 2 种分配方式：静态分配和动态分配。**静态分配是编译器完成的，比如局部变量的分配，动态分配由 alloca 函数进行分配，但是栈的动态分配和堆是不同的，它的动态分配是由编译器进行释放，不需要我们手工实现。

- 分配效率：栈是机器系统提供的数据结构，计算机会在底层分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。 堆则是 C/C++函数库提供的，它的机制是很复杂的。例如为了分配一块内存，库函数会按照一定的算法（具体的算法可以参考数据结构/操作系统）在堆内存中搜索可用的足够大小的空间，如果没有足够大小的空间（可能是由于内存碎片太多），就有可能调用系统功能去增加程序数据段的内存空间，然后进行返回。显然，堆的效率比栈要低得多。
- 无论是堆还是栈，都要防止越界现象的发生。

#### Linux内存管理

[Linux内存](https://mp.weixin.qq.com/s/LdY-UetHKicSeYNC1H9uUQ)包括虚拟内存、物理内存、共享内存。它们分别对应top输出中的VIRT、RES、SHR三列。

1. 物理内存

系统的物理内存被划分为许多相同大小的部分，也称作内存页。内存页的大小取决于CPU的架构和操作系统的配置，一般为4KB。物理内存的使用主要分为以下几方面：

- 内核使用

操作系统启动时，位于/boot目录下的压缩内核文件会被加载到内存中并解压。这部分内容在系统允许期间都会常驻在内存的起始位置。

- slab分配器 

操作系统的运行还需要更多的空间来分配给管理进程、文件描述符、socket和加载的类和模块等内容。所以内核会通过slab分配器动态分配内存。

slab是Linux操作系统的一种内存分配机制。其工作是针对一些经常分配并释放的对象，如进程描述符等，这些对象的大小一般比较小，如果直接采用brk系统调用来进行分配和释放，不仅会造成大量的碎片，而且也会影响性能。而slab分配器是基于对象进行管理的，相同类型的对象归为一类(如进程描述符就是一类)，每当要申请这样一个对象，slab分配器就从一个slab列表中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给操作系统，从而避免这些出现内存碎片。

- 进程使用

除去内核使用的部分，所有的进程都需要分配物理内存页给它们的代码、数据和堆栈。进程消耗的这些物理内存被称为“驻留内存”，RSS。

- 页缓存page cache

除去在内核和进程使用的部分，物理内存剩下的部分被称为页缓存，page cache。因为磁盘io的速度远远低于内存的访问速度，所以为了加快访问磁盘数据的速度，页缓存尽可能的保存着从磁盘读入的数据。page cache中还有一部分称为buffer，它的作用是缓存要写入到磁盘的数据。

页缓存的大小是在一直动态变化的。当系统内存充足时，页缓存会一直增大；当系统free内存不足时，这时如果有进程申请内存，操作系统会从page cache中回收内存页进行分配，如果page cache也已不足，那么系统会将当期驻留在内存中的数据置换到事先配置在磁盘上的swap空间中，然后空出来的这部分内存就可以用来分配了。这就是swap交换。

2. 虚拟内存

顾名思义，虚拟内存实际上并不存在，它只是存在于这套巧妙的内存管理机制中。**当一个进程启动时，内核会给新的进程建立一个虚拟地址空间。**这个虚拟地址空间代表了该进程可能使用到的所有内存，当然它是可以动态变化的。32 位系统典型的虚拟地址结构（进程内存布局）示意图如下图所示，从下往上地址增大。

32 位系统有4G 的地址空间。其中 0x08048000~0xbfffffff 是用户空间，0xc0000000~0xffffffff 是内核空间，包括内核代码和数据、与进程相关的数据结构（如页表、内核栈）等。另外，%esp 执行栈顶，往低地址方向变化；brk/sbrk 函数控制堆顶_edata往高地址方向变化。

![img](https://img-blog.csdn.net/20150114145446070?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ2ZnZHNn/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

由于成本的限制，物理内存往往无法做的很大，但是进程运行阶段所需申请的内存可能远远超过物理内存，并且系统不可能只跑一个进程，会有多个进程一起申请使用内存，如果都直接向物理内存进行申请使用肯定无法满足。通过引入虚拟内存，**每个进程都有自己独立的虚拟地址空间**，这个空间理论上可以无限大，因为它并不要钱。一个进程同一时刻不可能所有变量数据都会访问到，**只需要在访问某部分数据时，把这一块虚拟内存映射到物理内存**，其他没有实际访问过的虚拟地址空间并不会占用到物理内存，这样对物理内存的消耗就大大减少了 。

因此出现虚拟内存物理内存的映射机制。系统内核为每个进程都维护了一份从虚拟内存到物理内存的映射表，称为页表。页表根据虚拟地址，查找出锁映射的物理页位置和数据在物理页中的偏移量，便得到了实际需要访问的物理地址，如下图所示。

![img](https://mmbiz.qpic.cn/mmbiz_png/ibmAwmYxgDBNyImVcCloNClic7WI9Z71fCXtJBwqPibAWHBnU6J7ickCqnr2YQQFURb9Dle3WzJ7F35zZvVzQ2a76g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里还要提到一个概念，驻留内存，这是指虚拟内存中实际映射到物理内存的那部分，也就是进程实际占用的物理内存大小。所以判断一个进程使用的内存大小，主要是看占用的物理内存，也就是驻留内存的大小，即RSS。

3. 共享内存

进程在运行过程中，会加载许多操作系统的动态库，比如 libc.so、libld.so等。这些库对于每个进程而言都是公用的，它们在内存中实际只会加载一份，这部分称为共享内存。如上图中的A4和B3部分即为共享内存，实际都映射到同一块物理内存。注意，进程占用的共享内存也是计算到驻留内存中的。



### 基础

#### essearch

ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。

ElasticSearch 是一个分布式、高扩展、高实时的搜索与数据分析引擎。它能很方便的使大量数据具有搜索、分析和探索的能力。充分利用ElasticSearch的水平伸缩性，能使数据在生产环境变得更有价值。ElasticSearch 的实现原理主要分为以下几个步骤，首先用户将数据提交到Elastic Search 数据库中，再通过分词控制器去将对应的语句分词，将其权重和分词结果一并存入数据，当用户搜索数据时候，再根据权重将结果排名，打分，再将返回结果呈现给用户。

[Elasticsearch－基础介绍及索引原理分析](https://www.cnblogs.com/dreamroute/p/8484457.html)



#### 深拷贝与浅拷贝

**所有Python对象都拥有三个属性：身份（id）、类型、值。**

在计算机语言中，有两种赋值方式：按引用赋值、按值赋值。其中按引用赋值也常称为按指针传值(当然，它们还是有点区别的)，后者常称为拷贝副本传值。[在python中，无论是直接的变量赋值，还是参数传递，都是按照引用进行赋值的。](https://www.cnblogs.com/f-ck-need-u/p/10123145.html)

引用就是对象在内存中的数字地址编号，变量就是方便对引用的表示而出现的，变量指向的就是此引用。**赋值的本质就是让多个变量同时引用同一个对象的地址。**可变对象保存的并不是真正的对象数据，而是对象的引用。

对象的引用赋值都是引用计数加1，id相同，无论是可变对象还是不可变对象。同一个引用不同的计数指向相同的对象。

为了解决原始数据在函数传递后被修改的问题，提出深拷贝与浅拷贝。

[深拷贝与浅拷贝](https://mp.weixin.qq.com/s/Eb3fXkfM58XiCOuo8kPTpg)对于可变对象来说有区别，和对象的赋值不同，id不同，不同的引用指向相同的对象。；

对于不可变对象拷贝和对象的赋值没区别，id相同。

一般来说，浅拷贝或按引用赋值就是我们所期待的操作。只有少数时候(比如数据序列化、要传输、要持久化等)，才需要深拷贝操作，但这些操作一般都内置在对应的函数中，无需我们手动去深拷贝。

- 可变对象与不可变对象

**对于不可变对象，变量之间不会相互影响。对于可变对象，变量之间是相互影响的。**

可变对象：列表、字典、集合。所谓可变是指可变对象的值可变，身份是不变的。

不可变对象：数字、字符串、元组。不可变对象就是对象的身份和值都不可变。新创建的对象被关联到原来的变量名，旧对象被丢弃，垃圾回收器会在适当的时机回收这些对象。

在 Python 程序中，每个对象都会在内存中申请开辟一块空间来保存该对象，该对象在内存中所在位置的地址被称为引用。在开发程序时，所定义的变量名实际就对象的地址引用。因此变量就是方便对引用的表示而出现的，变量指向的就是此引用。赋值的本质就是让多个变量同时引用同一个对象的地址。  

对不可变对象赋值，实际就是在内存中开辟一片空间指向新的对象，原不可变对象不会被修改。

可变对象保存的并不是真正的对象数据，而是对象的引用。当对可变对象进行赋值时，只是将可变对象中保存的引用指向了新的对象。

- 小整数对象池

python中会为每个对象分配内存，哪怕他们的值完全相等。id(object)函数是返回对象object在其生命周期内位于内存中的地址，id函数的参数类型是一个对象。如：c, d 和 2.0 地址不同，但值相等。

为了优化程序的执行效率，python使用了小整数对象池。由于程序会频繁使用一些整数，有了这个小整数对象池，就会把小整数常驻内存不会被垃圾回收机制处理，避免了频繁地在内存中创建和销毁整数带来的耗费。

小整数对象池，其实就是一个指针数组，该数组的大小为262（257+5），表示的小整数的范围为[-5，257)（包括-5，不包括257），即是说该数组包含了指向这262个小整数的指针。python程序执行时，首先判断数值是否在[-5，257)内，如果在这个范围，就直接从内存中的小整数对象池中获取；如果不在这个范围，就从通用整数对象池中初始化并获取（如果当前通用整数对象池不存在或则已满，则新建一个对象池加入维护行列）。

- 浅拷贝

为了解决函数传递后被修改的问题，就需要拷贝一份副本，将副本传递给函数使用，就算是副本被修改，也不会影响原始数据 。

copy() 函数在拷贝对象时，只是将指定对象中的所有引用拷贝了一份，如果这些引用当中包含了一个可变对象的话，那么数据还是会被改变。 这种拷贝方式，称为浅拷贝。浅拷贝的优点：拷贝速度快，占用空间少，拷贝效率高。

![img](https://mmbiz.qpic.cn/mmbiz_png/LkiagibHt1iaklMQkgsyxYm54obv6F4EAMrRe94BCibad1CddgicAqItJJJUMjTUZEU7nLAfPbPjPvYYSds2yP6j5NA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 深拷贝

区别于浅拷贝只拷贝顶层引用，深拷贝会逐层进行拷贝，直到拷贝的所有引用都是不可变引用为止。

![img](https://mmbiz.qpic.cn/mmbiz_png/LkiagibHt1iaklMQkgsyxYm54obv6F4EAMrAQkVADEqU9nsiaElQUy6quJNMocDkONYjxVfaUHmRO6Ubts0ye1ZAXA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

~~~python
# 不可变对象
>>> a = 2
>>> b = a
>>> id(a)
1451208960
>>> id(b)
1451208960
>>> a = 1
>>> id(a)
1451208928
>>> id(b)
1451208960

# 可变对象
>>> s = [7,8,9]
>>> t = s # 按引用赋值
>>> id(s)
2295302120776
>>> id(t)
2295302120776
>>> t = copy.copy(s) # 浅拷贝与引用赋值的区别
>>> t[0] = 1;
>>> t
[1, 8, 9]
>>> s
[7, 8, 9]
>>> s[0] = 1 # 列表在"原处修改"数据对象(注意加了双引号)，地址不会变
>>> id(s)
2295302120776
>>> id(t)
2295302120776
>>> s
[1, 8, 9]
>>> t
[1, 8, 9]
>>> s = [1,2,3] # 按值赋值，地址变化
>>> id(s)
2295302120520
>>> id(t)
2295302120776

# 按值赋值。值相同，通常id不同
>>> a = 2.0
>>> b = 2.0
>>> print(id(a),id(b),id(2.0))
2295298138784 2295298138736 2295298138448
>>> print('a == b:',a==b)
a == b: True
>>> print('a is b:',a is b)
a is b: False
# 引用赋值。值相同，id也相同
>>> c = a
>>> c == a
True
>>> c is a
True

# 小整数对象池 [-5，257)。值相同，id也相同
>>> m = 256
>>> id(m)
1451217088
>>> n = 256
>>> id(n)
1451217088
>>> m = 257
>>> id(m)
2295301780560
>>> n = 257
>>> id(n)
2295301782224
# 整数是不可变对象
# 并不是先获取n原来指向的对象的地址，再把内存中的值更改为258；
# 而是新申请一段内存来存储对象258，再让n去指向对象258，所以两次id(x)的值不同。
>>> n = 258 
>>> id(n)
2295301781008

# 列表是可变对象
# 浅拷贝 copy.copy
>>> s1 = [1,2,3,[4,5]]
>>> id(s1)
2295302121480
>>> s2 = copy.copy(s1)
>>> s2 is s1 # 浅拷贝不同于引用赋值
False
>>> id(s2)
2295302111176
>>> s1[-1][1] = 2
>>> s1
[1, 2, 3, [4, 2]]
>>> id(s1)
2295302121480
>>> s2
[1, 2, 3, [4, 2]]
>>> id(s2)
2295302111176
# 深拷贝 copy.deepcopy
>>> s3 = copy.deepcopy(s2)
>>> s3
[1, 2, 3, [4, 2]]
>>> id(s3)
2295302107912
>>> s2[-1][1] = 10
>>> s2
[1, 2, 3, [4, 10]]
>>> s3
[1, 2, 3, [4, 2]]
# 引用赋值，默认浅拷贝传递对象的引用而已，引用计数，使用同一份id（内存地址）
>>> s4 = s3
>>> id(s4)
2295302107912
>>> s3[-1][1] = -5
>>> s3
[1, 2, 3, [4, -5]]
>>> s4
[1, 2, 3, [4, -5]]
# 列表内存与列表中元素的内存不同
>>> id(s4)
2295302107912
>>> id(s4[1])
1451208960
>>> s4[1] # 变量和变量指向的引用就是一个东西
2
>>> id(2)
1451208960
~~~

#### 作用域

True False

<https://www.cnblogs.com/f-ck-need-u/p/9925021.html#blogaaa3>



#### 列表

记录与列表相关的常见函数。

- append

Python中，列表是可以进行修改的：赋值、删除元素、分片等等。在给列表添加元素时，有两个常见的方法：append和extend。append在列表的最后添加元素，但是每次只能添加一个元素。extend更像一个连接操作，即用一个列表扩充另一个列表（依然在末尾位置）。

在实现这两个方法时，出现结果为None的情况。原因是append方法和其他一些方法类似，只是在恰当的位置修改原来的列表。这意味着，它不是返回一个修改过的列表，而是直接修改原来的列表，所以list.append()只是对列表进行了修改，不会有返回值。如：

~~~python
filepath = ["./well_data/csv_layer_59/"]
valid_filepath = ['./well_data/valid_59/']
# filepaths = filepath.append(valid_filepath) # None
filepath.extend(valid_filepath)
~~~

- zip(iterables)

zip在英文中有拉链的意思，我们由此可以形象的理解它的作用：将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同。

~~~python
>>> s1 = [1,2,3]
>>> s2 = [4,5,6]
>>> zip(s1, s2) # 打包为元组的列表
<zip object at 0x000002168923F788>
>>> type(zip(s1, s2))
<class 'zip'>
>>> list(zip(s1, s2))
[(1, 4), (2, 5), (3, 6)]
>>> s3 = [7,8,9]
>>> list(zip(s1, s2, s3))
[(1, 4, 7), (2, 5, 8), (3, 6, 9)]
>>> list(zip(s1))
[(1,), (2,), (3,)]
>>> list(zip(*zip(s1, s2))) # 与 zip 相反，可理解为解压，为zip的逆过程，可用于矩阵的转置
[(1, 2, 3), (4, 5, 6)]
>>> s3 = [7,8,9,10]
>>> list(zip(s1, s3)) # 元素个数与最短的列表一致
[(1, 7), (2, 8), (3, 9)]
~~~



#### 字典





## 数据分析

#### 其他

“分裂条件”（criterion）



~~~python
# 查验训练样本的数量和类别分布。
y_train=pd.Series(y_train)
y_train.value_counts()

# 查验测试样本的数量和类别分布。
y_test=pd.Series(y_test)
y_test.value_counts()
~~~



#### anaconda

[Python - 安装并配置Anaconda环境](https://www.cnblogs.com/anliven/p/9998662.html)

~~~python
# pycharm--terminal
localhost:bin kai$ python
Python 2.7.10 (default, Feb 22 2019, 21:55:15) 
[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.37.14)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> quit()
localhost:bin kai$ python3
Python 3.7.3 (default, Mar 27 2019, 16:54:48) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy

localhost:bin kai$ pwd
/Users/kai/anaconda3/bin
localhost:bin kai$ pip list
Package                            Version 
---------------------------------- --------
alabaster                          0.7.12  
anaconda-client                    1.7.2 
localhost:bin kai$ python2 list
bash: python2: command not found
    
# bash
(base) localhost:~ kai$ conda env list # 查看所有环境信息，确认环境所在目录
# conda environments:
#
base                  *  /Users/kai/anaconda3

(base) localhost:~ kai$ conda info -e # 查看当前系统下的环境
# conda environments:
#
base                  *  /Users/kai/anaconda3

(base) localhost:oil-classification-demo kai$ jupyter notebook # 当前路径打开jupyter notebook
  
# 修改镜像地址。默认添加引号
(base) localhost:~ kai$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
(base) localhost:~ kai$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
(base) localhost:~ kai$ conda config --set show_channel_urls yes
    
# 解决 CondaHTTPError: HTTP 404 NOT FOUND for url 问题
# 显示隐藏文件，打开.condarc文件，可以添加源与删除源

# 为了保证代码可以正确运行，分享代码的同时，也需要将运行环境分享；
# 通过conda可将当前环境下的 package 信息存入YAML 文件， 当执行他人的代码时，可使用此YAML文件创建同样的运行环境
# conda env export > BackupEnv.yaml    将当前运行环境的package信息导出到名为BackupEnv的YAML文件
# conda env create --force BackupEnv.yaml   使用YAML文件创建运行环境
~~~

conda 常用命令

- conda：验证conda是否安装；
- conda create —name env_name python==3.6：创建环境；
- source activate <env_name>：激活（进入）环境；windows中activate <env_name>；
- conda env list：查看当前环境已安装的包信息；
- pip list -r requirement.txt：快速安装包；
- conda list --name <env_name>：查看指定环境的已安装的包信息；
- sudo vi ~/.bash_profile：检查环境变量；
- export PATH="/Users/kai/anaconda3/bin:$PATH"：手动添加环境变量；
- source ~/.bash_profile：刷新环境变量。

当前环境安装的库的存储路径：F:\Anaconda\envs\oil\Lib\site-packages



更换Jupyter Notebook 内核Python版本

~~~python
# jupyter kernelspec list 命令输出内核指定的Python环境位置，进入该文件夹 vi kernel.json 
{
 "argv": [
  # "/Users/kai/anaconda3/bin/python",
  "/Users/kai/anaconda3/envs/sss/bin/python",
  "-m",
  "ipykernel_launcher",
  "-f",
  "{connection_file}"
 ],
 "display_name": "Python 3",
 "language": "python"
}
~~~



Pycharm最新激活码

<https://blog.csdn.net/weixin_43407092/article/details/89848074>

报错：

Warm-start fitting without increasing n_estimators does not fit new trees.



Mac一个窗口多个终端，一个窗口多个终端的创建

1.一个窗口多个终端，command+ N代开一个新的窗口

2.一个窗口多个终端，command+ T，即可实现



想法：

单个模型：可以根据输入的数字随机取出几个特征，并打印。训练模型，输出结果。结果包括score与future_importance。可以遍历出所有的可能性。

多个模型：

多层模型：

过拟合

交叉验证的几种方法

第一口井的模型用于第二口井

原始数据不做改动，全部作为特征传入训练模型

线程、进程实现并发

包括深度在内的10个参数的回归模型，非线性拟合，10个向量



下一步：

- 特征值预处理
- 特征参数选取
- 一维，地震波



地层分类，判断是否含油 1234

标准化时fit(X)与transform()；

训练时fit(X,y)与predict()。

train shape:(308, 13),test shape:(317, 15)



#### 标准化、归一化、中心化、正则化

基于距离的模型都会有“该不该缩放/标准化”的问题，也就是说通常都需要进行标准化。

**归一化处理数据针对的是行样本，标准化处理数据针对的是列样本。**

归一化/标准化实质是一种线性变换，线性变换不会改变原始数据的数值排序。

- 归一化：

１）把数据变成[0,1]或者[-1, 1]之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。这样样本的所有特征，在特征空间中，对样本的距离产生的影响是同级的。

２）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。使用sklearn.preprocessing.Normalizer类。归一化处理数据针对的是行样本，在数据样本向量进行点乘运算或其它核函数计算相似性时，拥有统一的标准（权重）时使用。

- 标准化：

主要包括z-score标准化与max-min标准化.，其中z-score标准化是最常见的特征预处理方式。

在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，资料标准化后会使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1，这个方法被广泛的使用在许多机器学习算法中，使用sklearn.preprocessing.StandardScaler类。公式为(X - X_mean)/X_std； 计算时对每个属性/每列分别进行。

数据集标准化对有些机器学习算法是很有必要的手段，之所以进行标准化，是因为两个原因：其一，对于同一特征中，最大最小值之差过大，将数据缩放在合适的范围，比如手机包月流量使用情况，有些数值是500M，有些是1G；其二，有些机器学习算法中目标函数的基础为假设特征均值为0，方差在同一介数的情况，sklearn官网说这类算法比如：SVM的RBF内核或线性模型的l1和l2正则化，如果某些特征的方差比其它的特征方差大几个数量级别，A方差是1，B特征方差是1000，那么会导致B特征对此算法占主导地位，导致学习器不是你所期望的结果。

- 中心化：平均值为0，对标准差无要求。

主要是在PCA降维的时候，此时我们求出特征x的平均值mean后，用x-mean代替原特征，也就是特征的均值变成了0, 但是方差并不改变。这个很好理解，因为PCA就是依赖方差来降维的。

归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。

标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。

什么时候用归一化？什么时候用标准化？
  （1）如果对输出结果范围有要求，用归一化。
  （2）如果数据较为稳定，不存在极端的最大最小值，用归一化。
  （3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。

- 正则化

~~~python
# 两种标准化的区别，输入数据的格式都是ndarray
>>> ls = [1,2,3,4,5,6]
>>> x = np.array(ls)
>>> x
array([1, 2, 3, 4, 5, 6])
>>> min(x)
1
>>> min(x).shape # 一个数字，存在array中。shape为空。通常在nparray中传入数组，而不是一个数字
()
>>> y = []
>>> y = np.array(y)
>>> y.shape # ()不等于(0,)
(0,)
>>> 1.shape
SyntaxError: invalid syntax
>>> id(min(x)) # id(min(x)) >> id(1)
1533338618472
>>> id(1)
1368895712
>>> 1 == min(x)
True
>>> 1 is min(x)
False
>>> m = 1
>>> n = 1
>>> m == n
True
>>> m is n
True
>>> m.shape
AttributeError: 'int' object has no attribute 'shape'
>>> 1.shape
SyntaxError: invalid syntax
>>> type(min(x)) # ndarray中每个元素都是numpy类型
<class 'numpy.int32'>

>>> x
array([ 1,  2,  3,  4, 5, 6])
>>> x - 1
array([ 0,  1,  2,  3, 4, 5])
>>> mean(x)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'mean' is not defined
>>> np.mean(x)
3.5
>>> np.std(x)
1.707825127659933

>>> x_maxmin = (x-min(x)) / (max(x)-min(x)) # 优点是不需要写循环遍历
>>> x_maxmin # 最大最小归一化是将变量的变化范围固定在[0, 1]内
array([0. , 0.2, 0.4, 0.6, 0.8, 1. ])
>>> x_std = (x-np.mean(x)) / np.std(x)
>>> x_std
array([-1.46385011, -0.87831007, -0.29277002,  0.29277002,  0.87831007,
        1.46385011])
>>> np.mean(x_std) # 均值为0
-3.700743415417188e-17
>>>
>>> np.std(x_std) # 方差为1
1.0
>>> np.mean(x_maxmin) # 均值随机
0.5
>>> np.std(x_maxmin) # 方差随机
0.3415650255319866

def norm(X):
    # 先做初始化！
    x = np.zeros(X.shape)
	# 每列各个特征分别进行归一化
    x[:, 0] = (X[:, 0] - 2300) / 1900  # depth
~~~



#### 流程

```python
# 原始参数，深度均匀
train_9T = pd.read_csv(train_9T, sep='\s+', na_values= NAN_values, header=Nrows_header)
train_label = pd.read_csv(train_label, sep='\s+',header = Nrows_header)
test_9T = pd.read_csv(test_9T, sep='\s+', na_values= NAN_values, header=Nrows_header)
test_label = pd.read_csv(test_label, sep='\s+',header = Nrows_header,encoding="ANSI")

# 预测值初始化，后边没有用到
train_predict = pd.DataFrame(data = train_9T["DEPTH"],columns = ['DEPTH'])
test_predict = pd.DataFrame(data = test_9T[test_9T.columns[0]],columns = ['DEPTH'])

# 选取的三个特征：'SP_cwt','DEN','RT_RXO'
train_9T['RT_RXO'] = train_9T['LOG10_RT'] - train_9T['LOG10_RXO']
test_9T['RT_RXO'] = test_9T['LOG10_RT'] - test_9T['LOG10_RXO']

train_Depth = train_9T['DEPTH'].values
train_SP = train_9T['SP'].ravel()
test_Depth = test_9T['DEPTH'].values
test_SP = test_9T['SP'].ravel()

# 对SP曲线去线性化linear_model.LinearRegression()，对SP进行时频分析pywt.cwt()，提取最大波峰值
cwtmatr_train, freqs_train = pywt.cwt(train_9T['SP_quxianxing'], widths, 'mexh')
cwtmatr_test, freqs_test = pywt.cwt(test_9T['SP_quxianxing'], widths, 'mexh')

# 这块是什么原因？与小波分析有关
N = 6
for ii in range(N):
    train_9T['SP_cwt'] = cwtmatr_train[6+ii,:]
    test_9T['SP_cwt'] = cwtmatr_test[6+ii,:]

# 标准化连续值特征preprocessing.StandardScaler()。修改时将oil列删除，因此减一；第一列DEPTH不变，其余列发生变化。
# 选用皮尔森相关系数test_9T.astype(float).corr()作为特征选择(排序)方法，用于理解特征和响应变量之间的关系。可视化函数热力图(sns.heatmap)与散布图(Scatter Plot)可以互相对映（sns.pairplot()矩阵散点图）

# 数据进行矩阵化为模型的输入做准备。小写x_train代表总的训练集，后续有X_train表示切割后用于训练的训练集，是训练集中的一部分
y_train = train_9T[u'oil'].ravel()
# 在这里选取了其中三个特征
x_train = train_9T[['SP_cwt','DEN','RT_RXO']].values # Creates an array of the train data
y_test = test_9T[u'oil'].ravel()
x_test = test_9T[['SP_cwt','DEN','RT_RXO']].values # Creates an array of the train data

# 开始训练。首先进行多个简单分类器进行训练，最后以ensamble集成方式进行最后的预测
# 定义模型类，包括模型，拟合、预测、以及特征重要性函数feature_importances(x_train,y_train)。
# get_oof(clf, x_train, y_train, x_test)方法返回训练集和测试集的预测分类
```



#### NumPy

Numpy的核心功能是高维数组，Numpy库中的ndarray(N-dimensional array object)对象支持多维数组，数组类型的对象本身具有大小固定、数组内元素数据类型相同等特性。Numpy也提供了大量数值运算函数，能够直接有效的进行向量、矩阵运算。

ndarray描述相同类型的元素集合。 可以使用基于零的索引访问集合中的项目。ndarray中的每个元素在内存中使用相同大小的块。ndarray中的每个元素是数据类型对象的对象（称为dtype）。当你在本书中看到“数组”、“NumPy数组”、"ndarray"时，基本上都指的是同一样东西，即ndarray对象。

NumPy数组使你可以将许多种数据处理任务表述为简洁的数组表达式（否则需要编写循环）。用数组表达式代替循环的做法，通常被称为矢量化。一般来说，矢量化数组运算要比等价的纯Python方式快上一两个数量级（甚至更多），尤其是各种数值计算。

NumPy的数组最大的缺点就是不可动态扩展，数组没有这种动态改变大小的功能，numpy.append()函数每次都会重新分配整个数组，并把原来的数组复制到新数组中。

axis参数1代表行，0代表列。

~~~python
>>> a = np.array([np.arange(3),np.arange(3)]) # 二维数组，两行
>>> a
array([[0, 1, 2],
       [0, 1, 2]])
>>> a.shape
(2, 3)
>>> a.ndim
2
>>> b = np.arange(3) # 一维数组，三行
>>> b
array([0, 1, 2])
>>> b.shape
(3,)
>>> c = np.array(np.arange(3)) # 一维数组，三行
>>> c
array([0, 1, 2])
>>> c.shape
(3,)
>>> np.random.randint(0,10,size=(5,5)) # 初始化。如果没有写参数high的值，则返回[0,low)的值。
array([[3, 9, 7, 3, 3],
       [6, 3, 1, 5, 0],
       [6, 5, 2, 2, 0],
       [1, 7, 2, 9, 4],
       [0, 4, 7, 5, 9]])
~~~



range()和xrange()函数

在 python 2.x 版本中，同时存在range（）和xrange（）函数，其中，range()返回值是一个列表，xrange（）返回值是一个迭代器；

在 python 3.x 版本中，取消了xrange（）的定义，仅保留了range（）函数，且range（）函数的返回值也改为迭代器；

xrange和range的语法格式相同，语法：range(start, stop[, step])，无论如何配置，返回值中均不包括stop值。如果要将生成的range（）对象变为列表就需要利用list(range(..))方式。



range()和np.arange()、np.linspace()函数

生成的都是等差数列；

range()是python的内置函数，其返回值是range可迭代对象；

arange()是Numpy库中的函数，其返回值是数组对象；

语法：np.arange([start,] stop[, step,], dtype=None)

linspace()是Numpy库中的等差数列函数，stop有时包含有时不包含，根据endpoint来选择，默认True包含，不包含就False。num是指定均分的数量，默认为50。

语法：linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)



np.random.randint() 与 random.randint()：

randint(low, high=None, size=None, dtype=’1’)中range范围前闭后开 [low, high) 或 [0, low)；

random.randint(low,high)中range范围 前闭后闭 [a, b]。

np.random.randint() 作为numpy库的一种方法，参数更丰富，相比于ranom.randint() 具备size参数 可指定outpud的元素的shape属性。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/5mt0ewv9OS3fDVWH8SnuhCfCtrdACoSQeBhEeolAXjJiaaoB98WvytlbBuXBDxuGCCLBGnHPf5jslTXzOKsLaibg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/5mt0ewv9OS3fDVWH8SnuhCfCtrdACoSQ8oKNL79yCOT7ibLiaOHwNHncgN7icd7ud8Cj9ul4jjFGSSlmK21akLib9g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

ndarray常见操作

~~~python
>>> n = np.arange(9)
>>> n
array([0, 1, 2, 3, 4, 5, 6, 7, 8])
>>> n.dtype # 通过dtype属性查看数组内元素的数据类型
dtype('int32')
>>> n = np.arange(9, dtype=np.float64) # 指定数据类型
>>> n
array([0., 1., 2., 3., 4., 5., 6., 7., 8.])
>>> n.dtype
dtype('float64')
>>> nn = n.astype("int32") # 改变已经创建好了的数组的数据类型
>>> nn
array([0, 1, 2, 3, 4, 5, 6, 7, 8])
>>> id(n)
2028769352080
>>> id(nn)
2028770138512
>>> nn.dtype
dtype('int32')
>>> n2 = np.array(['1', '2', '3']) # 将unicode数字字符类型转为相应的整型或浮点型
>>> n2
array(['1', '2', '3'], dtype='<U1')
>>> n2.dtype # < : 表示字节顺序，小端模式；U : 表示Unicode类型；1 : 表示元素的长度
dtype('<U1')
>>> n3 = n2.astype("int32")
>>> n3
array([1, 2, 3])
>>> n3.dtype
dtype('int32')
>>> n4 = np.array([5,2,3,1,1,2,1,3,5,4])
>>> n4
array([5, 2, 3, 1, 1, 2, 1, 3, 5, 4])
>>> n5 = np.unique(n4) # 一维的集合运算，计算其中的唯一元素，并返回有序结果
>>> n5
array([1, 2, 3, 4, 5])

~~~



np.concatenate()与.np.append

传参时的不同点在于np.concatenate()能够一次完成多个数组的拼接，而np.append对于参数规定，要么一个数组和一个数值；要么两个数组，不能三个及以上数组直接append拼接。numpy的数组没有动态改变大小的功能，numpy.append()函数每次都会重新分配整个数组，并把原来的数组复制到新数组中。concatenate()效率更高，适合大规模的数据拼接。

~~~python
a = np.arange(5)
a
Out[20]: array([0, 1, 2, 3, 4])
np.append(a,10)
Out[21]: array([ 0,  1,  2,  3,  4, 10])
b = np.array([4,5,6])
b
Out[23]: array([4, 5, 6])
np.append(a,b)
Out[24]: array([0, 1, 2, 3, 4, 4, 5, 6])
a
Out[25]: array([0, 1, 2, 3, 4])
b
Out[26]: array([4, 5, 6])
np.concatenate(a,b)
TypeError: only integer scalar arrays can be converted to a scalar index
np.concatenate((a,b))
Out[28]: array([0, 1, 2, 3, 4, 4, 5, 6])
np.concatenate((a,b),axis=0)
Out[29]: array([0, 1, 2, 3, 4, 4, 5, 6])
np.concatenate((a,b),axis=1)
numpy.AxisError: axis 1 is out of bounds for array of dimension 1
~~~



TypeError: 'numpy.ndarray' object is not callable

类型错误：callable()是python的内置函数，用来检查对象是否可被调用，可被调用指的是对象能否使用()括号的方法调用



视图与复制：

视图view共享数据存储空间，数据不会被复制，视图上的任何修改都会直接反映到原始数组；

复制copy进行浅复制，分配内存保存结果。

如实现相同功能的两个函数：

用于数据展平的函数：

- ravel()：视图，返回的是一个数组的视图。视图是数组的引用(说引用不太恰当，因为原数组和ravel()返回后的数组的地址并不一样)，在使用过程中应该注意避免在修改视图时影响原本的数组；

- flatten()：复制，分配了新的内存，平时使用的时候flatten()更为合适。

用于维度改变的函数：

- reshape()视图

- resize()复制。

numpy返回array中元素的index，使用函数 np.argwhere()。如用于选取指定标签所在的行。

~~~python
>>> y = [0,0,1,1,2,2,3,3,1,1,0,0,2]
>>> y = np.array(y)
>>> y
array([0, 0, 1, 1, 2, 2, 3, 3, 1, 1, 0, 0, 2])
>>> it = np.argwhere(y==1)
>>> it
array([[2],
       [3],
       [8],
       [9]], dtype=int64)
>>> it.shape
(4, 1)
>>> it.tolist()
[[2], [3], [8], [9]]
>>> type(it.tolist()[1])
<class 'list'>

# ravel()与flatten()
>>> ir = it.ravel()
>>> ir
array([2, 3, 8, 9], dtype=int64)
>>> if = it.flatten()
  File "<stdin>", line 1
    if = it.flatten()
       ^
SyntaxError: invalid syntax
>>> iff = it.flatten()
>>> iff
array([2, 3, 8, 9], dtype=int64)
# 平时使用的时候flatten()更为合适
>>> iff[1] = 0
>>> iff
array([2, 0, 8, 9], dtype=int64)
>>> it
array([[2],
       [3],
       [8],
       [9]], dtype=int64)
>>> ir[1] = 0
>>> ir
array([2, 0, 8, 9], dtype=int64)
>>> it
array([[2],
       [0],
       [8],
       [9]], dtype=int64)
~~~







numpy.mean(a, axis, dtype, out，keepdims)求取均值方法：

经常操作的参数为axis，以m * n矩阵举例：

- axis 不设置值，对 m*n 个数求均值，返回一个实数

- axis = 0：压缩行，对各列求均值，返回 1* n 矩阵

- axis =1 ：压缩列，对各行求均值，返回 m *1 矩阵

可以这么理解，axis是几，那就表明哪一维度被压缩成1。

keepdims参数

```python
>>> x = np.array([[1,2],[4,5],[7,8]])
>>> x
array([[1, 2],
       [4, 5],
       [7, 8]])
>>> np.mean(x,axis=0)
array([4., 5.])
>>> np.mean(x,axis=0,keepdims=True)
array([[4., 5.]])
>>> np.mean(x,axis=0,keepdims=True).shape
(1, 2)
>>> np.mean(x)
4.5
>>> np.mean(x,keepdims=True)
array([[4.5]])
>>> np.mean(x,keepdims=True).shape
(1, 1)
>>> np.mean(x,axis=1)
array([1.5, 4.5, 7.5])
>>> np.mean(x,axis=1,keepdims=True)
array([[1.5],
       [4.5],
       [7.5]])
>>> np.mean(x,axis=1,keepdims=True).shape
(3, 1)
```



np.dot



ndarray 与 list 互相转换

- list 转 numpy：np.array(a)
- ndarray 转 list：a.tolist()



对python numpy.array插入一行或一列的方法



list、array和matrix的区别

list是Python中的普通列表对象，支持append和attend操作，没有shape属性；array和matrix是numpy数据库中的对象，不支持append和attend操作，具有shape属性。

一个list中可以存放不同类型的数据，如int、float、str，或者布尔型；而array和matrix中只能存放相同类型的数据。

在list中的数据类型保存的是数据所存放的地址，简单的说就是指针，并非数据，这样保存一个list就太麻烦了，例如list1=[1,2,3,'a']需要4个指针和四个数据，增加了存储和消耗cpu。

list不支持乘法操作；array和matrix支持乘法操作。

list对象不支持一次性读取一行或一列，只能通过指针进行元素的索引；array和matrix即支持一次性读取一行或一列，也支持通过指针来进行元素的索引。

~~~python
>>> ls = [1,3,4]
>>> n = np.array(ls)
>>> n
array([1, 3, 4])
>>> ls.shape
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'list' object has no attribute 'shape'
>>> n.shape
(3,)
>>> nn = n.values
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'numpy.ndarray' object has no attribute 'values'
>>> nn = n.reshape(-1,1)
>>> nn
array([[1],
       [3],
       [4]])
>>> nn.shape
(3, 1)

>>> m = np.arange(9)
>>> m
array([0, 1, 2, 3, 4, 5, 6, 7, 8])
>>> m = m.reshape(3,3)
>>> m
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
>>> m.reshape(-1,1)
array([[0],
       [1],
       [2],
       [3],
       [4],
       [5],
       [6],
       [7],
       [8]])

>>> n
array([1, 3, 4])
>>> n[1]
3
>>> n[1] = 'a'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: invalid literal for int() with base 10: 'a'
>>> n[1] = 1.1
>>> n
array([1, 1, 4])
>>> n[1]
1

>>> ls
[1, 3, 4]
>>> np.array(ls)
array([1, 3, 4])
>>> np.mat(ls)
matrix([[1, 3, 4]])
>>> lt = [[1,2,3],[4,5,6],[7,8,9]]
>>> lt
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]
>>> lt[1][1]
5
>>> lt[1,1]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: list indices must be integers or slices, not tuple
>>> ls = np.array(lt)
>>> ls
array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])
>>> ls[1,1]
5
~~~









#### pandas

Python Data Analysis Library或pandas是基于NumPy的一种工具，该工具是为了解决数据分析任务而创建的。Pandas的名称来自于面板数据（panel data）和python数据分析（data analysis）。在 Pandas 中，一条记录对应着一行。

pandas是基于NumPy数组构建的，特别是基于数组的函数和不使用for循环的数据处理。虽然pandas采用了大量的NumPy编码风格，但二者最大的不同是pandas是专门为处理表格和混杂数据设计的。而NumPy更适合处理统一的数值数组数据。

1. Pandas中的数据结构

Series：一维数组（列），与Numpy中的一维Array类似。二者与Python基本的数据结构List也很相近，其区别是，List中的元素可以是不同的数据类型，而Array和Series中则只允许存储相同的数据类型，这样可以更有效的使用内存，提高运算效率。Series和一维数组最主要的区别在于Series类型有一组与之相关的数据标签（即索引），可以和另一个编程中常见的数据结构哈希（Hash）联系起来。因此可以通过索引的方式选取元素。

Series的字符串表现形式为：索引在左边，值在右边。由于我们没有为数据指定索引，于是会自动创建一个0到N-1（N为数据的长度）的整数型索引。你可以通过Series 的values和index属性获取其数组表示形式和索引对象。还可以将Series看成是一个定长的有序字典，因为它是索引值到数据值的一个映射。在pandas中，NaN（即“非数字”（not a number）用于表示缺失或NA值。对于许多应用而言，Series最重要的一个功能是，它会根据运算的索引标签自动对齐数据。

Time- Series：以时间为索引的Series。

DataFrame：二维的表格型数据结构。它含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔值等）。很多功能与R中的data.frame类似。可以将DataFrame理解为Series的容器。以下的内容主要以DataFrame为主。其中index为行，columns为列。axis的默认值为0代表行，1代表列。

建DataFrame的办法有很多，最常用的一种是直接传入一个由等长列表或NumPy数组组成的字典。字典的键是自定义的columns，DataFrame会自动加上索引index（跟Series一样），且全部列会被有序排列。如果指定了列序列（即colunms参数），则DataFrame的列就会按照指定顺序进行排列。获取DataFrame的列得到一个Series。

Panel：三维的数组，可以理解为DataFrame的容器。

Pandas中一般的数据结构构成为DataFrame -> Series -> ndarray

2. 常见I/O：

read_csv() 读取以‘，’分割的文件到DataFrame；

read_table()读取以‘/t’分割的文件到DataFrame 。

实质上是通用的，在实际使用中可以通过对sep参数的控制来对任何文本文件读取。

read_csv() / read_table()函数的参数：

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180714205358426?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW5lcl9t/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- header：None表示原来的数据是没有列索引的，就算数据里面有列索引，这时就把原来的列索引当成了数据。没有该项时认为没有列索引。

- index_col：默认为数据添加行索引，即会把原来的行索引当成数据，自己再添加新的行索引。如果不想添加新的行索引，使用index_col=0。

```python
# test_label = pd.read_table(test_label.decode('utf8'), sep='\s+',header = Nrows_header)
# read_table is deprecated, use read_csv instead.
#　pd.read_csv可以读取txt，也就是说只要是csv格式，后缀无影响
train_9T = pd.read_csv(train_9T, sep='\s+', na_values= NAN_values, header=Nrows_header)
```

3. 数据帧(DataFrame)

数据帧(DataFrame)是二维数据结构，即数据以行和列的表格方式排列。pandas中的DataFrame可以使用以下构造函数创建，pandas.DataFrame( data, index, columns, dtype, copy)。其中index为行，columns为列。

创建DataFrame：

~~~python
w = pd.DataFrame(np.arange(9).reshape(3,3)) # 快速创建
w
Out[102]: 
   0  1  2
0  0  1  2
1  3  4  5
2  6  7  8
d = pd.DataFrame([1,2,3])
d
Out[59]: 
   0
0  1
1  2
2  3
d = pd.DataFrame([[1,2,3],[4,5,6]])
d
Out[61]: 
   0  1  2
0  1  2  3
1  4  5  6
d = pd.DataFrame([1,2,3],[4,5,6])
d
Out[63]: 
   0
4  1
5  2
6  3
~~~



DataFrame合并：

concat、append与merge

append竖方向合并df，没有axis属性，不会就地修改，而是会创建副本；

merge默认合并后只保留有共同列项并且值相等行（即交集），如left和right的k1=y分别有2个，最终构成了2*2=4行；

concat有axis属性，原df中，取并集的行/列名称不能有重复项，即axis=0时columns不能有重复项，axis=1时index不能有重复项。

实际情况中往往合并的数据有不同的列名，concat有参数用于此种情况，即sort=False参数。

此处使用reindex修改列名。

join属性可以选择非合并方向取交集（inner）或取并集（outer），默认值outer并集，如，axis=0时join='inner'，columns取交集。

~~~python
x = pd.DataFrame({"one":[1,2],"two":[3,4]})
y = pd.DataFrame({"one":[5,6],"two":[7,8]})
z = pd.concat([x,y]) # 默认值：axis=0
z
Out[34]: 
   one  two
0    1    3
1    2    4
0    5    7
1    6    8
z = pd.concat([x,y],axis=0) # 竖方向（index）合并，合并方向index作列表相加，非合并方向columns取并集
z
Out[36]: 
   one  two
0    1    3
1    2    4
0    5    7
1    6    8
z = pd.concat([x,y],axis=1) # 横方向（columns）合并，合并方向columns作列表相加，非合并方向index取并集
z
Out[38]: 
   one  two  one  two
0    1    3    5    7
1    2    4    6    8
~~~



DataFrame切片：

常见的选取行列的方法有以下三种，直接选取、loc（标签）、iloc（整数）。

- df['column_name'] ，df[row_start_index, row_end_index] 选取指定整列数据；
- loc属性，在知道列名字的情况下，df.loc[index,column] 选取指定行，列的数据；
- iloc属性，在column name特别长或者index是时间序列等各种不方便输入的情况下，可以用iloc (i = index), iloc完全用数字来定位 iloc[row_index, column_index]。

**当需要选中一列时，直接使用df[col]的方法选中一列，而需要选中一行时，直接使用df.loc[row]来选中一行，使用df.iloc[row]会报错。因为iloc是基于行/列的position。**

通过类似字典标记的方式或属性的方式，可以将DataFrame的列获取为一个Series。利用标签的切片运算与普通的Python切片运算不同，其末端是包含的。如obj['b':'c']包含，obj[2:4]不包含。

可以通过将行标签（行索引）传递给loc来选择行。也可以按整数位置（行号）选择，通过将整数位置传递给iloc来选择行。可以使用：运算符选择多行，也就是行切片。同样利用loc、iloc也可以用于提取列数据。

利用loc函数的时候，当index相同时，会将相同的Index全部提取出来，优点是：如果index是人名，数据框为所有人的数据，那么我可以将某个人的多条数据提取出来分析；缺点是：如果index不具有特定意义，而且重复，那么提取的数据需要进一步处理。

删除行、删除列使用DataFrame.drop()函数，默认删除行，列需要加axis = 1。DataFrame.astype() 方法可对整个DataFrame或某一列进行数据格式转换，支持Python和NumPy的数据类型。附加行时使用append()函数将新行添加到DataFrame。 

~~~python
t[:] # 所有数据
Out[61]: 
   one  two  three
0    5    7      1
1    6    8      2
0    5    7      3
1    6   10      4
t[["one","two"]] # 两列
Out[59]: 
   one  two
0    5    7
1    6    8
0    5    7
1    6   10
t.one # 通过类似字典标记的方式或属性的方式，可以将DataFrame的列获取为一个Series。
Out[90]: 
0    5
1    6
0    5
1    6
Name: one, dtype: int64
t[0:2] # 前两行
Out[63]: 
   one  two  three
0    5    7      1
1    6    8      2
t[-1:] # 最后一行
Out[64]: 
   one  two  three
1    6   10      4
t.loc[1] # 选取行
Out[70]: 
   one  two  three
1    6    8      2
1    6   10      4
t.loc[1,"two"] # 坐标定位，使用索引
Out[71]: 
1     8
1    10
t.loc[:,"one"] # 选取列
Out[76]: 
0    5
1    6
0    5
1    6
t.iloc[0,1] # 坐标定位，使用数字
Out[72]: 7
~~~



DataFrame交换列的顺序

<https://blog.csdn.net/qq_36523839/article/details/80094541>

函数list.insert(index, obj)用于将指定对象插入列表的指定位置。



ravel()与tolist()：

前者将多维array展平为一维数组，用于降维，后者将数组转化为列表。



values与ravel的区别：values函数pd和都有，ravel适用于np与pd的Series，不适用于DataFrame。相同点在于都可用于多维数组的降维。

values属性会以二维ndarray的形式返回Series或DataFrame中的数据。

~~~python
a = np.array([[1,2,3],[4,5,6]])
a
Out[41]: 
array([[1, 2, 3],
       [4, 5, 6]])
d = pd.DataFrame(a)
d
Out[49]: 
   0  1  2
0  1  2  3
1  4  5  6
d[0]
Out[50]: 
0    1
1    4
Name: 0, dtype: int32
a
Out[51]: 
array([[1, 2, 3],
       [4, 5, 6]])
a[0]
Out[52]: array([1, 2, 3])
d[0].ravel()
Out[53]: array([1, 4])
d[0].values
Out[54]: array([1, 4])

~~~

ndim表示数组的维度，dimension

shape就是数组的行、列数

DataFrame.values.reshape(-1,1)

pandas.dataframe doesn't have a built-in reshape method, but you can use .values to access the underlying numpy array object and call reshape on it

reshape(-1,1)数据集变成了一列，reshape(1，-1)数据集变成了一行

~~~python
# 根据numpy库的官网介绍，这里的-1被理解为unspecified value，即未指定的，未给定的。如果我只需要特定的行数，列数我无所谓多少，
# 我只需要指定行数，列数用-1代替就行了，计算机帮我算应该有多少列，反之亦然。所以-1在这里应该可以理解为一个正整数通配符，它代替任何正整数。
d.reshape(-1,1)
AttributeError: 'DataFrame' object has no attribute 'reshape'
d.values.reshape(-1,1)
Out[60]: 
array([[1],
       [2],
       [3],
       [4],
       [5],
       [6]])
d.values.reshape(1,-1)
Out[61]: array([[1, 2, 3, 4, 5, 6]])
~~~



索引对象

pandas的索引对象负责管理轴标签和其他元数据（比如轴名称等）。构建Series或DataFrame时，所用到的任何数组或其他序列的标签都会被转换成一个Index。Index对象是不可变的，因此用户不能对其进行修改。与python的集合不同，pandas的Index可以包含重复的标签。

~~~python
t.index # 行标签
Out[83]: Int64Index([0, 1, 0, 1], dtype='int64')
t.index[1:]
Out[84]: Int64Index([1, 0, 1], dtype='int64')
t.index[1] = "d" # Index对象是不可变的
TypeError: Index does not support mutable operations
t.columns # 列标签
Out[86]: Index(['one', 'two', 'three'], dtype='object')
~~~

重新索引使用reindex方法，作用是创建一个新对象，它的数据符合新的索引。用该Series的reindex将会根据新索引进行重排。如果某个索引值当前不存在，就引入缺失值。也就是说，索引存在时重排，索引不存在时缺失。

~~~python
s = pd.Series([1,2,3])
s.reindex(["a","b","c"]) # 索引之前不存在
Out[100]: 
a   NaN
b   NaN
c   NaN
~~~



dataframe与ndarray的相互转换：

dataframe.values —> ndarray，没有降维，flatten()

pd.Dataframe(ndarray) —> dataframe



to_csv()与read_csv()：

to_csv()是DataFrame类的方法，read_csv()是pandas的方法。

read_csv报错：

read_csv UnicodeDecodeError: 'utf-8' codec can't decode byte 0x86 in position 0: invalid start byte

编码：编码格式，encoding="utf-8"；

路径：路径有中文；

file_path=Util.handle_file_path(video_path,"video_info")        

f=open(file_path)        data=pd.read_csv(f,encoding='utf-8',decimal=',')



函数小全

~~~python
# 切片时，loc行包含下标上限，iloc不含行列下标上限。
# read_csv()方法中的参数header = None表示原来的数据是没有列索引的，就算你的数据里面有列索引，这时就把原来的列索引当成了数据；
# 默认为数据添加行索引，即会把原来的行索引当成数据，自己再添加新的行索引。如果不想添加新的行索引，使用index_col=0。
read_df = pd.read_csv(path,sep=" ",header=0,index_col=0,dtype=str)
# to_csv的参数sep会将一行的多个数据合并为一个单元格，以sep作为分隔符；
# 不指定sep时会将一行的多个数据分别放在不同的单元格中，便于在excel中打开查看，txt中默认逗号分隔。
write_df.to_csv(filepath + filename, header=True, index=False)

>>> n = np.arange(9).reshape(3,3)
>>> n
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
>>> p = pd.DataFrame(n, columns=['a','c','b'])
>>> p
   a  c  b
0  0  1  2
1  3  4  5
2  6  7  8
>>> p['a']  # 选取列
0    0
1    3
2    6
Name: a, dtype: int32
>>> p[1]  # KeyError: 1
  File "pandas\_libs\hashtable_class_helper.pxi", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item
>>> p.loc[1]  # 选取行
a    3
c    4
b    5
Name: 1, dtype: int32
        
>>> import os
>>> os.path
<module 'ntpath' from 'F:\\Anaconda\\envs\\oil\\lib\\ntpath.py'>
>>> os.getcwd()  # 获取当前工作目录，也就是在哪个目录下运行这个程序
'F:\\engyne\\KZ\\Engine\\oil\\KZ'

>>> p.loc[1, 1]
TypeError: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [1] of <class 'int'>
>>> p.loc[1, 'a']  # df.loc[index, column] 选取指定行与列名的值
3
>>> p.iloc[1, 1]  # df.iloc[row_index, column_index] 选取指定行与列索引的值
4
>>> p.loc[[1, 2]]  # 一次选取多行
   a  c  b
1  3  4  5
2  6  7  8
>>> p.loc[1:2]  # 第2行之前的数据（包含第2行）
   a  c  b
1  3  4  5
2  6  7  8
>>> p.iloc[1:2]  # iloc可以不加第几列，则是行选择。第2行之前的数据（不包含第2行）
   a  c  b
1  3  4  5
>>> p.iloc[1:2, 1:2]  # 切片时，iloc不包括行与列下标上限
   c
1  4
>>> p[1:2]  # 第2行之前的数据（不包含第2行）
   a  c  b
1  3  4  5
>>> p[-1:]
   a  c  b
2  6  7  8
>>> p[-1]  # KeyError: -1
  File "pandas\_libs\hashtable_class_helper.pxi", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item
>>> p[-1:]  # 最后一行
   a  c  b
2  6  7  8
>>> p.loc[-1]
ValueError: -1 is not in range
>>> p[4] = [1, 2, 3]  # 默认添加一列
>>> p
   a  c  b  4
0  0  1  2  1
1  3  4  5  2
2  6  7  8  3
>>> p.loc[4] = [1, 2, 3, 4]  # 添加一行
>>> p
   a  c  b  4
0  0  1  2  1
1  3  4  5  2
2  6  7  8  3
4  1  2  3  4

>>> p.loc[p['a'] > 1]  # 选取满足条件的指定列所在行
   a  c  b  4
1  3  4  5  2
2  6  7  8  3
>>> p[p['a'] > 1]  # 同上
   a  c  b  4
1  3  4  5  2
2  6  7  8  3
>>> p[p['a'] > 1 & p['a'] < 5]
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
>>> p[(p['a'] > 1) & (p['a'] < 5)]  # # 不能使用or and 需要用 | 和 & 来代替。同时需要括号
   a  c  b  4
1  3  4  5  2

>>> pp = p.values  # dataframe --> ndarray
>>> pp
array([[0, 1, 2, 1],
       [3, 4, 5, 2],
       [6, 7, 8, 3],
       [1, 2, 3, 4]], dtype=int64)
>>> type(p.values)
<class 'numpy.ndarray'>
>>> pp = p.values.flatten()  # ndarray降维
>>> pp
array([0, 1, 2, 1, 3, 4, 5, 2, 6, 7, 8, 3, 1, 2, 3, 4], dtype=int64)
>>> type(pp)
<class 'numpy.ndarray'>
>>> pp.tolist()  # ndarray --> list
[0, 1, 2, 1, 3, 4, 5, 2, 6, 7, 8, 3, 1, 2, 3, 4]
>>> pp.shape
(16,)

>>> p['d'] = [1, 0, 1, 0]  # 添加一列
>>> p
   a  c  b  4  d
0  0  1  2  1  1
1  3  4  5  2  0
2  6  7  8  3  1
4  1  2  3  4  0

>>> p[p['a'].isin([1, 2, 3])]  # isin 操作符将返回一个布尔序列，显示系列中的每个元素是否完全包含在传递的值序列中
   a  c  b  4  d
1  3  4  5  2  0
4  1  2  3  4  0
>>> p.where(p > 5)  # where
     a    c    b   4   d
0  NaN  NaN  NaN NaN NaN
1  NaN  NaN  NaN NaN NaN
2  6.0  7.0  8.0 NaN NaN
4  NaN  NaN  NaN NaN NaN

# 将一个字符串形式的列表转换成列表：最简单的方法是使用 eval 函数
# eval() 函数用来执行一个字符串表达式，并返回表达式的值
>>> s = "[1, 2, 3, 4]"
>>> eval(s)
[1, 2, 3, 4]

>>> n  # numpy
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
>>> n.shape
(3, 3)
>>> n[1]  # 选择行
array([3, 4, 5])
>>> n[:, 1]  # 选择列
array([1, 4, 7])
>>> n[4] = [1, 2, 3]
IndexError: index 4 is out of bounds for axis 0 with size 3
>>> n = np.row_stack((n, [1, 2, 3]))  # 增加行
>>> n
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8],
       [1, 2, 3]])
>>> n = np.column_stack((n, [1, 2, 3, 4]))  # 增加列
>>> n
array([[0, 1, 2, 1],
       [3, 4, 5, 2],
       [6, 7, 8, 3],
       [1, 2, 3, 4]])
>>> nn = n[2, :]
>>> nn
array([6, 7, 8, 3])
>>> np.where((nn > 3) & (nn < 8))  # 返回索引
(array([0, 1], dtype=int64),)
>>> nn[np.where((nn > 3) & (nn < 8))]  # 返回值
array([6, 7])

~~~

#### matplotlib

Matplotlib对象简介

FigureCanvas  画布

Figure        图

Axes          坐标轴(实际画图的地方)

Title 为图像标题，Axis 为坐标轴, Label 为坐标轴标注，Tick 为刻度线，Tick Label 为刻度注释，Legend 为图例。

Figure和Subplot

matplotlib的图像都位于Figure对象中，实际上就是创建了一个空的图像窗口。可以用plt.figure创建一个新的Figure。如fig = plt.figure(e(num=None, figsize=None)。其中：

num，图像编号或名称，数字为编号 ，字符串为名称，可以通过该参数激活不同的画布；figsize，指定figure的宽和高，单位为英寸。

子图及子区域

subplot可以规划figure划分为n个子图，但每条subplot命令只会创建一个子图。add_subplot新增子图。add_subplot的参数与subplots的相似，与subplots的不同。

？因为add_subplot与subplots都是一次创建多个子图，因此需要给每个子图命名。

add_axes为新增子区域，该区域可以座落在figure内任意位置，且该区域可任意设置大小。



add_subplot 与 subplot

fig.add_subplot(111)此处，fig= plt.figure()是一个对象。其中参数111，指的是将图像分成1行1列，此子图占据从左到右从上到下的1位置。

plt.subplot(234)：将画布分成2行3列，取从左到右，从上到下第4个位置。

两者本质是一样的，不同在于add_subplot是面向对象，subplot是面向函数。

~~~python
# subplots(a two-digit number)，通过ax控制子图，也可以通过plt控制子图
>>> fig, ax = plt.subplots(2, 2) # 函数返回一个figure图像和子图ax的array列表。fig代表整个图像，ax代表坐标轴和画的图
# >>> fig = plt.figure() # 这两行与上面的一行等价
# >>> ax = fig.subplots(2,2)
>>> ax[0, 0].plot([1,2],[1,4]) # 索引的下标从0开始。一行或一列时只需要一个索引
[<matplotlib.lines.Line2D object at 0x0000016509A62550>]
>>> plt.show()
>>> fig
<Figure size 640x480 with 4 Axes>
>>> ax
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x00000165090F4C50>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016508EDDEB8>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x000001650903D4A8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016509B5AA58>]],
      dtype=object)
>>> type(ax)
<class 'numpy.ndarray'>

# add_subplot(a three-digit number)
>>> ax1 = fig.add_subplot(111) 
>>> fig # 新增一个Axes，单独窗口显示
<Figure size 640x480 with 5 Axes>

# subplot(a three-digit number)
>>> ax = fig.subplot(2,2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'Figure' object has no attribute 'subplot'

# 上面是采用面向对象，下面是使用函数
>>> ax2 = plt.subplot(221)
<matplotlib.axes._subplots.AxesSubplot object at 0x0000016509641FD0>
>>> plt.plot([1,2],[1,4])
[<matplotlib.lines.Line2D object at 0x00000165095FA518>]
>>> plt.show()
>>> fig # Axes个数未增加，新的窗口显示
<Figure size 640x480 with 6 Axes>
~~~



不能通过空Figure绘图，必须用add_subplot()创建一个或多个子sunplot绘图区才能绘图。如ax = fig.add_subplot(221)。意思是：绘制2×2两行两列共4个subplot图像，当前选中第一个。编号从1开始。

plt.imshow()与plt.show()区别

plt.imshow()函数负责对图像进行处理，并显示其格式，但是不能显示。

其后跟着plt.show()才能显示出来。

在编程的过程中发现plt.imshow()不能同时显示两张照片，如果有两条plt.imshow()语句处于一前一后的位置，那么程序运行后只会显示后面的图片。如果想让每一张图片都显示出来，需要在每一个plt.imshow()语句后面加上plt.show()语句。





油层分类代码：

~~~python
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False # 用来正常显示负号
from matplotlib import gridspec # 自定义子图布局
from matplotlib.pyplot import MultipleLocator # 手动设置坐标轴刻度间隔 

def showOilData(data):
    # 深度坐标反转
    # 将油层分类标签按类别画出
    fig = plt.figure() # 创建画布
    # 在subplot在总的图画上面加title / Add a centered title to the figure.
    fig.suptitle(filename_list[0].split(".")[0]) # 图表总标题
    # 自定义子图布局，生成不同大小的subplot，宽度比 width_ratios
    gs = gridspec.GridSpec(1, 5, width_ratios=[2, 1, 1, 1, 1])
    # print(gs) # GridSpec(1, 5, width_ratios=[2, 1, 1, 1, 1])
    oil_num = [0, 0, 0, 0] # 统计各类标签的个数
    y_series = np.dot(data[:,0],4000) + 1000

    # ax1 = fig.add_subplot(151)
    ax1 = fig.add_subplot(gs[0])
    ax1.plot(data[:,16], y_series, label='oil')
    # ax1.set(title=filename_list[0], ylabel='depth')
    ax1.set(title="OIL", ylabel='DEPTH')
    ax1.set_ylim(y_series.min(), y_series.max())

    x_major_locator = MultipleLocator(1) # 手动设置坐标轴刻度间隔
    ax1.xaxis.set_major_locator(x_major_locator)
    ax1.xaxis.set_ticks_position("top") # 坐标轴刻度在最上面
    plt.gca().invert_yaxis() # Invert the y-axis
    # ax1.legend()
    # plt.show()

    # data_0 = data[data[:,16] == 0]
    # print(data_0.shape)
    # numpy返回array中元素的index。不需要使用字典
    # itemindex_0 = np.argwhere(data[:,16] == 0)
    # 使用展平函数，ndarray --> array
    # print(itemindex_0)
    # print(type(itemindex_0))
    # print(itemindex_0.shape) # (691, 1)
    # oil_0 = itemindex_0.flatten()
    # print(oil_0)
    # print(type(oil_0)) # <class 'numpy.ndarray'>
    # print(oil_0.shape)
    # oil_num[0] = oil_0.shape[0]
    # 垂直柱状图
    # ax2.bar(oil_0, 1)
    # 水平柱状图
    # 标签0不画出
    # ax2.barh(oil_0, 1)

    # 根据油层分类标签的索引获取深度
    itemindex_1 = np.argwhere(data[:, 16] == 1)
    # oil_1 = itemindex_1.flatten()
    # print(oil_1)
    oil_1 = y_series[itemindex_1].flatten()
    oil_num[1] = oil_1.shape[0]
    # print(oil_1)
    # ax3 = fig.add_subplot(153)
    ax2 = fig.add_subplot(gs[1])
    ax2.barh(y=oil_1, width=0.2, color="red")
    ax2.set_title("1类油层") # 子图表的标题
    # 隐藏坐标轴刻度
    ax2.set_xticks([])
    # 隐藏坐标轴，xy同时隐藏
    # ax2.axis("off")
    # 隐藏坐标轴与隐藏刻度都需要将其置于 plt.show() 之前，plt.imshow() 之后
    ax2.set_ylim(y_series.min(), y_series.max())
    plt.gca().invert_yaxis()

    itemindex_2 = np.argwhere(data[:, 16] == 2)
    # oil_2 = itemindex_2.flatten()
    # print(oil_2)
    oil_2 = y_series[itemindex_2].flatten()
    oil_num[2] = oil_2.shape[0]
    # print(oil_2)
    # ax4 = fig.add_subplot(154)
    ax3 = fig.add_subplot(gs[2])
    ax3.barh(y=oil_2, width=0.2, color="blue")
    ax3.set_title("2类油层")
    ax3.set_xticks([])
    ax3.set_yticks([])
    # ax3.title("2类油层") # TypeError: 'Text' object is not callable
    ax3.set_ylim(y_series.min(), y_series.max())
    plt.gca().invert_yaxis()

    itemindex_3 = np.argwhere(data[:, 16] == 3)
    # oil_3 = itemindex_3.flatten()
    oil_3 = y_series[itemindex_3].flatten()
    oil_num[3] = oil_3.shape[0]
    # print(oil_3)
    # ax5 = fig.add_subplot(155)
    ax4 = fig.add_subplot(gs[3])
    ax4.barh(y=oil_3, width=0.2, color="gold")
    ax4.set_title("3类油层")
    ax4.set_xticks([])
    ax4.set_yticks([])
    ax4.set_ylim(y_series.min(), y_series.max())
    plt.gca().invert_yaxis()
    # print(sum(oil_num))

    # invalid = data[data[:, 0] == 4130]
    # print(invalid) # []
    itemindex_0 = np.argwhere(data[:, 16] == 0)
    oil_0 = y_series[itemindex_0].flatten()
    oil_num[0] = oil_0.shape[0]
    # ax2 = fig.add_subplot(152)
    ax5 = fig.add_subplot(gs[4])
    ax5.barh(y=oil_0, width=0.2, color="black")
    ax5.set_title("0类油层")
    ax5.set_xticks([])
    ax5.set_yticks([])
    ax5.set_ylim(y_series.min(), y_series.max())
    plt.gca().invert_yaxis()

    print(oil_num)
    # plt.tight_layout() # 使图幅紧凑
    plt.savefig("./well_data/oil-depth-J30.png") # 自动保存
    plt.show() # 显示图像
~~~





#### pywt 

时频分析

知道信号频率随时间变化的情况，各个时刻的瞬时频率及其幅值——这也就是时频分析。时频分析方法提供了时间域与频率域的联合分布信息，清楚地描述了信号频率随时间变化的关系。

时频分析的基本思想是：设计时间和频率的联合函数，用它同时描述信号在不同时间和频率的能量密度或强度。时间和频率的这种联合函数简称为时频分布。利用时频分布来分析信号，能给出各个时刻的瞬时频率及其幅值，并且能够进行时频滤波和时变信号研究。



小波就是在有限距离上有均值为零值点的波形。

相比于正旋波， 这是傅立叶变化的基， 正旋波没有有限的距离， 他们从无穷小扩展到无穷大。 正旋波是光滑的和可以预测的， 小波是不规律的和非对称的。傅立叶分析将一个信号分解为无数个不同频率的正旋信号的组合。 类似的， 小波分析将一个原始信号分解为移位过和压缩过的母小波的组合。



傅里叶分析

<https://blog.csdn.net/u013921430/article/details/79683853#commentBox>

贯穿时域与频域的方法之一，就是传中说的傅里叶分析。傅里叶分析可分为傅里叶级数（Fourier Serie）和傅里叶变换(Fourier Transformation)。任何波形都是可以用正弦波叠加起来的。

时域的基本单元就是“1秒”，如果我们将一个角频率为\omega_{0}的正弦波cos（\omega_{0}t）看作基础，那么频域的基本单元就是\omega_{0}。正弦波就是一个圆周运动在一条直线上的投影。所以频域的基本单元也可以理解为一个始终在旋转的圆。在频域图像，也就是俗称的频谱中，偶数项的振幅都是0，也就对应了图中的彩色直线。振幅为0的正弦波。实际上看似不规律的事情反而是规律的正弦波在时域上的投影，而正弦波又是一个旋转的圆在直线上的投影。

频道，就是频率的通道，不同的频道就是将不同的频率作为一个通道来进行信息传输。傅里叶变换可以用于从某条曲线中去除一些特定的频率成分，这在工程上称为滤波，是信号处理最重要的概念之一，只有在频域才能轻松的做到。

再说一个更重要，但是稍微复杂一点的用途——求解微分方程。但是求解微分方程却是一件相当麻烦的事情。因为除了要计算加减乘除，还要计算微分积分。而傅里叶变换则可以让微分和积分在频域中变为乘法和除法。

基础的正弦波A.sin(wt+θ)中，振幅，频率，相位缺一不可，不同相位决定了波的位置，所以对于频域分析，仅仅有频谱（振幅谱）是不够的，我们还需要一个相位谱。时间差并不是相位差。如果将全部周期看作2Pi或者360度的话，相位差则是时间差在一个周期中所占的比例。我们将时间差除周期再乘2Pi，就得到了相位差。

在完整的立体图中，我们将投影得到的时间差依次除以所在频率的周期，就得到了最下面的相位谱。所以，频谱是从侧面看，相位谱是从下面看。

![img](https://img-blog.csdn.net/20170305203741491)

傅里叶级数的本质是将一个周期的信号分解成无限多分开的（离散的）正弦波，傅里叶变换，可以将一个时域非周期的连续信号，转换为一个在频域非周期的连续信号。



从傅里叶变换到小波变换

<http://www.sohu.com/a/154009298_465219>

FFT（快速傅里叶变换）可以分析信号的频谱，但是傅里叶变换处理非平稳信号有天生缺陷。它只能获取一段信号总体上包含哪些频率的成分，但是对各成分出现的时刻并无所知。因此时域相差很大的两个信号，可能频谱图一样。而对于非平稳信号，只知道包含哪些频率成分是不够的，我们还想知道各个成分出现的时间。知道信号频率随时间变化的情况，各个时刻的瞬时频率及其幅值——这也就是时频分析。

短时傅里叶变换（Short-time Fourier Transform, STFT）就是把整个时域过程分解成无数个等长的小过程（加窗），每个小过程近似平稳，再傅里叶变换，就知道在哪个时间点上出现了什么频率了。使用STFT存在一个问题，我们应该用多宽的窗函数？窗太宽太窄都有问题。窄窗口时间分辨率高、频率分辨率低，宽窗口时间分辨率低、频率分辨率高。对于时变的非稳态信号，高频适合小窗口，低频适合大窗口。然而STFT的窗口是固定的，在一次STFT中宽度不会变化，所以STFT还是无法满足非稳态信号变化的频率的需求。这个道理可以用海森堡不确定性原理来解释。类似于我们不能同时获取一个粒子的动量和位置，我们也不能同时获取信号绝对精准的时刻和频率。

小波变换的出发点和STFT还是不同的。STFT是给信号加窗，分段做FFT；而小波直接把傅里叶变换的基给换了——将无限长的三角函数基换成了有限长的会衰减的小波基。这样不仅能够获取频率，还可以定位到时间了。因此做傅里叶变换只能得到一个频谱，做小波变换却可以得到一个时频谱。



pywt 

小波分析库，









#### plotly

<https://www.cnblogs.com/feffery/p/9293745.html>

plotly库中的graph_objs是plotly下的子模块，用于导入plotly中所有图形对象，在导入相应的图形对象之后，便可以根据需要呈现的数据和自定义的图形规格参数来定义一个graph对象，再输入到plotly.offline.iplot()中进行最终的呈现。

离线绘图又有plotly.offline.plot()和plotly.offline.iplot()两种方法，前者是以离线的方式在当前本地工作目录下生成html格式的图像文件，并自动打开；后者是在jupyter notebook中专用的方法，即将生成的图形嵌入到ipynb文件中。



.





#### gitlab

进入分支目录。

安装git（gitbash）：

ssh-keygen -t rsa -C "kzhang@engyne.net" 生成rsa key

设置的是git的账户信息

git config --global user.name "张凯"

git config --global user.email "kzhang@engyne.net"

点击New SSH key按钮，添加新的key

项目使用gitlab：

初始化仓库：git init

下拉项目：git clone git@115.28.93.210:engergy/oil-classification-demo

拉取远端仓库：git remote add origin 远端仓库名.git


提交信息：git commit [-a/m"自定义提交文字"]，a直接提交，m是添加注释

推送到服务器：git push --set-upstream origin master

git命令 <http://www.ruanyifeng.com/blog/2014/06/git_remote.html>

git branch 创建分支命令

git checkout -b oil-classification 本地创建并切换分支branch

git status 查看当前状态，本地与远程的区别

git branch -a 显示本地与远程的所有分支，-a包括隐藏

git pull origin oil-classification，origin代表远程

git add .-->git commit -m-->git push origin oil-classification添加

push时报错：fatal: Could not read from remote repository

解决方法：git remote add origin git@115.28.94.210:engergy/oil-classification-demo.git（账号：项目名）

![](http://www.ruanyifeng.com/blogimg/asset/2014/bg2014061202.jpg)

具体操作：

使用Git下载指定分支命令为：git clone -b 分支名 仓库地址

如：git clone -b dzhang2 git@115.28.94.210:engergy/oil-classification-demo.git

删除远程分支的部分文件：

git rm -r .DS_store —>git commit —>git push

提交部分文件到远程分支：

git add oil_lstm.py —>git commit —>git push

Git克隆部分文件：

https://www.cnblogs.com/xilifeng/p/5225666.html



git 花式报错

git@xxx password报错：

提示 Permission denied, please try again

原因是 工程的SSH key没有加入到你的gitlab账户下

git pull 失败：

提示 fatal: refusing to merge unrelated histories

git合并时冲突问题：

提示 Merging is not possible because you have unmerged files

用git diff或者git status 查看哪些文件冲突，有冲突的会提示：

++<<<<<<< HEAD

++<<<<<<< new_branch

修改冲突的文件，修改完之后，保存。

用git add xxx，把你修改的文件全部都添加进去。

最后，用git commit -a -m ” fix conflict ” 提交，完成。

git push本地分支到同名远程分支报错：

提示 Updates were rejected because the remote contains work that you do not have locally.

git pull origin master  //需要先将本地代码先更新到和github上的一样

git pull origin master --allow-unrelated-histories

后面加上 --allow-unrelated-histories ， 把两段不相干的 分支进行强行合并



<https://www.cnblogs.com/hamsterPP/p/6810831.html>



## 数学基础

### 其他

#### 范数

<https://blog.csdn.net/a493823882/article/details/80569888>



### 线性代数

#### 矩阵乘法

<http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html>

证明矩阵乘法的计算规则



#### 张量

[张量的属性——阶、形状和类型](http://book.51cto.com/art/201801/565534.htm)



#### 向量积
向量是由n个实数组成的一个n行1列或一个1行n列的有序数组。

向量的点乘（内积、数量积）

定义：对两个向量执行点乘运算，就是对这两个向量对应位一一相乘之后求和的操作，点乘的结果是一个标量。

几何意义：可以用来表征或计算两个向量之间的夹角，以及在b向量在a向量方向上的投影。

向量的叉乘（向量积、外积、叉积）

定义：叉乘的运算结果是一个向量而不是一个标量。并且两个向量的叉积与这两个向量组成的坐标平面垂直。

几何意义：在三维几何中，向量a和向量b的叉乘结果是一个向量，更为熟知的叫法是法向量，该向量垂直于a和b向量构成的平面。在3D图像学中，叉乘的概念非常有用，可以通过两个向量的叉乘，生成第三个垂直于a，b的法向量，从而构建X、Y、Z坐标系。在二维空间中，叉乘还有另外一个几何意义，aXb等于由向量a和向量b构成的平行四边形的面积。

![img](https://img-blog.csdn.net/20160902232814429)



#### 梯度下降

https://www.jianshu.com/p/c7e642877b0e

基于梯度的搜索是使用最为广泛的参数寻优方法。

实现梯度下降

https://www.cnblogs.com/tbcaaa8/p/4415429.html

在此类方法中，我们从某些初始解出发，迭代寻找最优参数值。每次迭代中，我们先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。例如由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。若误差函数在当前点的梯度为零，则已达到局部最小，更新量为零，参数的迭代更新停止。

缺陷：当误差函数具有多个局部最小，则不能保证找到的解就是全局最小。



梯度下降法的三种形式BGD、SGD以及MBGD





### 概率论

目前机器学习中最主流的一类方法是统计学习方法，将机器学习问题看作是统计推断问题，并且又可以进一步分为频率学派和贝叶斯学派。频率学派将模型参数θ 看作是固定常数；而贝叶斯学派将参数θ 看作是随机变量，并且存在某种先验分布。

#### 数学特征

数学特征如数学期望、方差、标准差、协方差。其中数学期望表征平均值，方差、标准差表征一维数据的离散程度，协方差表征多维数据的相关程度。

- 数学期望E(X) ：定义是实验中每次可能的结果的概率乘以其结果的总和。分为离散型随机量与连续型随机量两种。根据“大数定律”的描述，数学期望值可以用于预测一个随机事件的平均预期情况。E是期望值expected value的缩写；

- 方差D(X) , Var(X) 或 DX：表征随机变量的取值在均值周围的散布程度，用于计算每一个变量（观察值）与总体均数之间的差异，统计学采用平均离均差平方和来描述变量的变异程度；

- 标准差 Standard Deviation：又叫均方差，是离均差平方的算术平均数的平方根，用σ 表示。标准差是”方差”的算术平方根。标准差能反映一个数据集的离散程度（等价于数据的波动大小或变异性）。与方差相比，标准差更好的在量纲上与样本集合保持同步。这就是“标准”的意义；

- 协方差 Covariance：表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。反之为负值。 如果X 与Y 是统计独立的，那么二者之间的协方差就是0。同一变量的协方差等于其方差，即X=Y；

![ç¸å³å¾ç](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR7TQWjPHcoYg9OpbVkhvjKit0OJVP8nkbFfoKzrBHESq5VaCpVJQ)



#### 期望与平均值

期望与平均值二者都有平均的概念，但一个很根本的区别在于，期望是随机变量的**总体**的平均，而平均值是从总体中抽取出来的**样本**的平均。前者是理论上的值、理想值，后者是现实观察到的统计量。

根据大数定律，如果实验次数足够大，样本均值就会趋近于总体的期望。



#### 样本标准差与总体标准差

样本标准差的意义是用于估计总体标准差，实现无偏估计。

整个数据集的总体标准差公式中分母为n，而使用样本估计总体标准差所得的样本标准差公式中分母为n-1。那么样本标准差为什么除以n-1？

选择的样本只是总体的一部分，有可能把较为极端的数值排除在外，使得数值更有可能以更紧密的方式聚集在均值周围。也就是说，样本的标准差要小于总体标准差。

因此，为了更好的用样本估计总体的标准差，统计学家对标准差公式做出了改造：将原本标准差公式的分母n改为n-1，使得标准差略大，弥补了样本的标准差小于总体标准差的不足。

![image-20190711122433044](/Users/kai/Library/Application Support/typora-user-images/image-20190711122433044.png)



数学原理：

假定随机变量![[公式]](https://www.zhihu.com/equation?tex=X)的数学期望![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)是已知的，然而方差![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5E2)未知。在这个条件下，根据方差的定义**![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cmu%5CBig%29%5E2+)是**方差![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5E2)的一个无偏估计。

考虑随机变量![[公式]](https://www.zhihu.com/equation?tex=X)的数学期望![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)是未知的情形。这时，直接用样本均值![[公式]](https://www.zhihu.com/equation?tex=%5Cbar%7BX%7D)均值替换掉上面式子中的![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)期望，会倾向于低估方差：

![[å¬å¼]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cbar%7BX%7D%29%5E2+%3C%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28X_i-%5Cmu%29%5E2+)

因此，在不知道随机变量真实数学期望的前提下，为了“正确”的估计方差，把上式中的分母![[公式]](https://www.zhihu.com/equation?tex=n)换成![[公式]](https://www.zhihu.com/equation?tex=n-1)，通过这种方法把原来的偏小的估计“放大”一点点，我们就能获得对方差的正确估计了：
![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5CBig%5B%5Cfrac%7B1%7D%7Bn-1%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cbar%7BX%7D%5CBig%29%5E2%5CBig%5D%3D%5Cmathbb%7BE%7D%5CBig%5B%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En%5CBig%28X_i+-%5Cmu%5CBig%29%5E2+%5CBig%5D%3D%5Csigma%5E2.)



#### 偏差（bias）与方差（variance）

使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力。因此模型调参的目标就是为了达到整体模型的偏差和方差的大和谐。

广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述的是预测值作为随机变量的离散程度。模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。

![img](https://images2015.cnblogs.com/blog/927391/201607/927391-20160716124330623-527064401.jpg)

研究模型的方差有什么现实的意义呢？我们认为方差越大的模型越容易过拟合：假设有两个训练集A和B，经过A训练的模型Fa与经过B训练的模型Fb差异很大，这意味着Fa在类A的样本集合上有更好的性能，而Fb反之，这便是我们所说的过拟合现象。

#### 偏差-方差分解

为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡。拟合能力强的模型一般复杂度会比较高，容易导致过拟合。相反，如果限制模型的复杂度，降低其拟合能力，又可能会导致欠拟合。**过拟合和欠拟合都是缺乏模型泛化能力的表现。**因此，如何在模型能力和复杂度之间取得一个较好的平衡对一个机器学习算法来讲十分重要。偏差-方差分解（Bias-Variance Decomposition）为我们提供一个很好的分析和指导工具。

误差、偏差与方差：

- 误差（Error）：模型预测结果与真实结果之间的差异；

- 偏差（bias）：模型的训练误差叫做偏差；

- 方差（Variance）：训练误差和测试误差的差异大小叫方差。

![img](https://img-blog.csdn.net/20180817085501567?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIwNDEyNTk1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

以回归问题为例，采用平方损失函数时，对于单个样本x，不同训练集D得到模型和最优模型的上的期望错误为：

![1565659441950](C:\Users\KZ\AppData\Roaming\Typora\typora-user-images\1565659441950.png)

其中第一项为偏差（Bias），是指一个模型在不同训练集上的平均性能和最优模型的差异。偏差可以用来衡量一个模型的拟合能力；第二项是方差（Variance），是指一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合。**欠拟合时偏差大。过拟合时方差大。**

即期望错误可以分解为：

![1565659291568](C:\Users\KZ\AppData\Roaming\Typora\typora-user-images\1565659291568.png)

因此，**最小化期望错误等价于最小化偏差和方差之和。**

损失ε 通常是由于样本分布以及噪声引起的，无法通过优化模型来减少。

当训练数据比较少时会导致过拟合。方差一般会随着训练样本的增加而减少。当样本比较多时，方差比较少，我们可以选择能力强的模型来减少偏差。然而在很多机器学习任务上，训练集上往往都比较有限，最优的偏差和最优的方差就无法兼顾。

随着模型复杂度的增加，模型拟合能力强（偏差小），模型泛化能力差（过拟合，方差大）。以结构错误最小化为例，我们可以调整正则化系数λ 来控制模型的复杂度。当λ 变大时，模型复杂度会降低，可以有效地过拟合和方差分解给机器学习模型提供了一种分析途径，但在实际操作中难以直接衡量。一般来说，当一个模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高。这种情况可以增加数据特征、提高模型复杂度，减少正则化系数等操作来改进模型。当模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高。这种情况可以通过降低模型复杂度，加大正则化系数，引入先验等方法来缓解。此外，还有一种有效的降低方差的方法为集成模型，即通过多个高方差模型的平均来降低方差。

#### L1/L2正则化

<https://blog.csdn.net/cymy001/article/details/79118741>

为了平衡模型的拟合能力与泛化能力，通常采用L1/L2正则化方法。

所谓正则化方法,就是在优化的目标函数上加上模型参数的L1/L2范数：

- L1正则化是让参数向量中的许多元素趋向于0，让有效特征变得稀疏,对应的L1正则化模型称为Lasso。 
- L2正则化是让参数向量中的大部分元素都变得很小，压制了参数之间的差异性，对应的L2正则化模型称为Ridge。

#### 无偏估计

首先，什么叫做有偏？

如图所示，当使用样本均值估计变量的期望时，方差估计会有偏差。也就是说有偏！但“估计是无偏的“。

![img](https://img-blog.csdn.net/20180915164545791?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjbnRfMjAxMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

方差可以用来衡量均值估计量与期望之间的“接近”程度。方差越小，均值估计量的分布越接近![\mu](data:image/svg+xml;utf8,%3Csvg%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20class%3D%22mjx-svg-math%22%20width%3D%221.402ex%22%20height%3D%222.176ex%22%20style%3D%22font-size%3A14px%3Bvertical-align%3A%20-0.838ex%3B%22%20viewBox%3D%220%20-576.1%20603.5%20936.9%22%20role%3D%22img%22%20focusable%3D%22false%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-labelledby%3D%22MathJax-SVG-1-Title%22%3E%0A%3Ctitle%20id%3D%22MathJax-SVG-1-Title%22%3E%5Cmu%3C%2Ftitle%3E%0A%3Cdefs%20aria-hidden%3D%22true%22%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-3BC%22%20d%3D%22M58%20-216Q44%20-216%2034%20-208T23%20-186Q23%20-176%2096%20116T173%20414Q186%20442%20219%20442Q231%20441%20239%20435T249%20423T251%20413Q251%20401%20220%20279T187%20142Q185%20131%20185%20107V99Q185%2026%20252%2026Q261%2026%20270%2027T287%2031T302%2038T315%2045T327%2055T338%2065T348%2077T356%2088T365%20100L372%20110L408%20253Q444%20395%20448%20404Q461%20431%20491%20431Q504%20431%20512%20424T523%20412T525%20402L449%2084Q448%2079%20448%2068Q448%2043%20455%2035T476%2026Q485%2027%20496%2035Q517%2055%20537%20131Q543%20151%20547%20152Q549%20153%20557%20153H561Q580%20153%20580%20144Q580%20138%20575%20117T555%2063T523%2013Q510%200%20491%20-8Q483%20-10%20467%20-10Q446%20-10%20429%20-4T402%2011T385%2029T376%2044T374%2051L368%2045Q362%2039%20350%2030T324%2012T288%20-4T246%20-11Q199%20-11%20153%2012L129%20-85Q108%20-167%20104%20-180T92%20-202Q76%20-216%2058%20-216Z%22%3E%3C%2Fpath%3E%0A%3C%2Fdefs%3E%0A%3Cg%20stroke%3D%22currentColor%22%20fill%3D%22currentColor%22%20stroke-width%3D%220%22%20transform%3D%22matrix(1%200%200%20-1%200%200)%22%20aria-hidden%3D%22true%22%3E%0A%3Cg%20class%3D%22mjx-svg-mrow%22%3E%0A%3Cg%20class%3D%22mjx-svg-mi%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-3BC%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E)，结果越有效。使用均值对期望![\mu](data:image/svg+xml;utf8,%3Csvg%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20class%3D%22mjx-svg-math%22%20width%3D%221.402ex%22%20height%3D%222.176ex%22%20style%3D%22font-size%3A14px%3Bvertical-align%3A%20-0.838ex%3B%22%20viewBox%3D%220%20-576.1%20603.5%20936.9%22%20role%3D%22img%22%20focusable%3D%22false%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-labelledby%3D%22MathJax-SVG-1-Title%22%3E%0A%3Ctitle%20id%3D%22MathJax-SVG-1-Title%22%3E%5Cmu%3C%2Ftitle%3E%0A%3Cdefs%20aria-hidden%3D%22true%22%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-3BC%22%20d%3D%22M58%20-216Q44%20-216%2034%20-208T23%20-186Q23%20-176%2096%20116T173%20414Q186%20442%20219%20442Q231%20441%20239%20435T249%20423T251%20413Q251%20401%20220%20279T187%20142Q185%20131%20185%20107V99Q185%2026%20252%2026Q261%2026%20270%2027T287%2031T302%2038T315%2045T327%2055T338%2065T348%2077T356%2088T365%20100L372%20110L408%20253Q444%20395%20448%20404Q461%20431%20491%20431Q504%20431%20512%20424T523%20412T525%20402L449%2084Q448%2079%20448%2068Q448%2043%20455%2035T476%2026Q485%2027%20496%2035Q517%2055%20537%20131Q543%20151%20547%20152Q549%20153%20557%20153H561Q580%20153%20580%20144Q580%20138%20575%20117T555%2063T523%2013Q510%200%20491%20-8Q483%20-10%20467%20-10Q446%20-10%20429%20-4T402%2011T385%2029T376%2044T374%2051L368%2045Q362%2039%20350%2030T324%2012T288%20-4T246%20-11Q199%20-11%20153%2012L129%20-85Q108%20-167%20104%20-180T92%20-202Q76%20-216%2058%20-216Z%22%3E%3C%2Fpath%3E%0A%3C%2Fdefs%3E%0A%3Cg%20stroke%3D%22currentColor%22%20fill%3D%22currentColor%22%20stroke-width%3D%220%22%20transform%3D%22matrix(1%200%200%20-1%200%200)%22%20aria-hidden%3D%22true%22%3E%0A%3Cg%20class%3D%22mjx-svg-mrow%22%3E%0A%3Cg%20class%3D%22mjx-svg-mi%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-3BC%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E)进行估计，有效估计和无偏估计是不相关的。

而在现实中不一定非要选无偏估计量，比如在下图中，如果能接受点误差，我倒觉得选择右边这个估计量更好。

![img](https://img-blog.csdn.net/20180915164546175?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjbnRfMjAxMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

根据方差估计偏差值公式可知，随着采样个数![n](https://www.zhihu.com/equation?tex=n) 的增加，偏差值会越来越小。那么这个估计就是“一致”的。如果样本数够多，其实这种有偏但是一致的估计量也接近无偏了。



#### R(相关系数)与R^2(确定系数)

R与R^2没有关系，就如同标准差与标准误差没有关系一样。

- 相关系数 Correlation：变量之间线性相关的度量。分三种，spearman, pearson, kendall。

协方差作为描述X和Y相关程度的量，在同一物理量纲之下有一定的作用，但同样的两个量采用不同的量纲使它们的协方差在数值上表现出很大的差异。为此引入相关系数的概念，相关性的取值在[-1，+1]之间，值为零时两个随机变量不相关，或称为线性无关。因此，相关系数是一种特殊的协方差。

- 确定系数（R^2）：对模型进行线性回归后，用于评价回归模型系数拟合优度。

公式：R2=SSR/SST=1-SSE/SST，其中：

SST (total sum of squares)：总平方和；

SSR (regression sum of squares)：回归平方和；

SSE (error sum of squares) ：残差平方和。

R^2 的缺陷：当我们人为的向系统中添加过多的自变量，SSE会减少，从而R^2变大。因此采用校正R方，惩罚过多无意义的自变量。

![img](https:////upload-images.jianshu.io/upload_images/9640232-31b286319df805d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/629)



#### 最小二乘法

假设不同的回归曲线为预测值，降低与真实值的误差，实现拟合。

法国数学家，阿德里安-馬里·勒让德提出让总的误差的平方最小的y就是真值，这是基于，如果误差是随机的，应该围绕真值上下波动（关于这点可以看下“[如何理解无偏估计？](https://www.matongxue.com/madocs/808.html)”）。这就是最小二乘法。

最小二乘法，所谓“二乘”就是平方的意思，台湾直接翻译为最小平方法。算术平均数只是最小二乘法的特例，适用范围比较狭窄。而最小二乘法用途就广泛。同一组数据，选择不同的![f(x)](data:image/svg+xml;utf8,%3Csvg%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20class%3D%22mjx-svg-math%22%20width%3D%224.418ex%22%20height%3D%222.843ex%22%20style%3D%22font-size%3A14px%3Bvertical-align%3A%20-0.838ex%3B%22%20viewBox%3D%220%20-863.1%201902%201223.9%22%20role%3D%22img%22%20focusable%3D%22false%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-labelledby%3D%22MathJax-SVG-1-Title%22%3E%0A%3Ctitle%20id%3D%22MathJax-SVG-1-Title%22%3Ef(x)%3C%2Ftitle%3E%0A%3Cdefs%20aria-hidden%3D%22true%22%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-66%22%20d%3D%22M118%20-162Q120%20-162%20124%20-164T135%20-167T147%20-168Q160%20-168%20171%20-155T187%20-126Q197%20-99%20221%2027T267%20267T289%20382V385H242Q195%20385%20192%20387Q188%20390%20188%20397L195%20425Q197%20430%20203%20430T250%20431Q298%20431%20298%20432Q298%20434%20307%20482T319%20540Q356%20705%20465%20705Q502%20703%20526%20683T550%20630Q550%20594%20529%20578T487%20561Q443%20561%20443%20603Q443%20622%20454%20636T478%20657L487%20662Q471%20668%20457%20668Q445%20668%20434%20658T419%20630Q412%20601%20403%20552T387%20469T380%20433Q380%20431%20435%20431Q480%20431%20487%20430T498%20424Q499%20420%20496%20407T491%20391Q489%20386%20482%20386T428%20385H372L349%20263Q301%2015%20282%20-47Q255%20-132%20212%20-173Q175%20-205%20139%20-205Q107%20-205%2081%20-186T55%20-132Q55%20-95%2076%20-78T118%20-61Q162%20-61%20162%20-103Q162%20-122%20151%20-136T127%20-157L118%20-162Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMAIN-28%22%20d%3D%22M94%20250Q94%20319%20104%20381T127%20488T164%20576T202%20643T244%20695T277%20729T302%20750H315H319Q333%20750%20333%20741Q333%20738%20316%20720T275%20667T226%20581T184%20443T167%20250T184%2058T225%20-81T274%20-167T316%20-220T333%20-241Q333%20-250%20318%20-250H315H302L274%20-226Q180%20-141%20137%20-14T94%20250Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-78%22%20d%3D%22M52%20289Q59%20331%20106%20386T222%20442Q257%20442%20286%20424T329%20379Q371%20442%20430%20442Q467%20442%20494%20420T522%20361Q522%20332%20508%20314T481%20292T458%20288Q439%20288%20427%20299T415%20328Q415%20374%20465%20391Q454%20404%20425%20404Q412%20404%20406%20402Q368%20386%20350%20336Q290%20115%20290%2078Q290%2050%20306%2038T341%2026Q378%2026%20414%2059T463%20140Q466%20150%20469%20151T485%20153H489Q504%20153%20504%20145Q504%20144%20502%20134Q486%2077%20440%2033T333%20-11Q263%20-11%20227%2052Q186%20-10%20133%20-10H127Q78%20-10%2057%2016T35%2071Q35%20103%2054%20123T99%20143Q142%20143%20142%20101Q142%2081%20130%2066T107%2046T94%2041L91%2040Q91%2039%2097%2036T113%2029T132%2026Q168%2026%20194%2071Q203%2087%20217%20139T245%20247T261%20313Q266%20340%20266%20352Q266%20380%20251%20392T217%20404Q177%20404%20142%20372T93%20290Q91%20281%2088%20280T72%20278H58Q52%20284%2052%20289Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMAIN-29%22%20d%3D%22M60%20749L64%20750Q69%20750%2074%20750H86L114%20726Q208%20641%20251%20514T294%20250Q294%20182%20284%20119T261%2012T224%20-76T186%20-143T145%20-194T113%20-227T90%20-246Q87%20-249%2086%20-250H74Q66%20-250%2063%20-250T58%20-247T55%20-238Q56%20-237%2066%20-225Q221%20-64%20221%20250T66%20725Q56%20737%2055%20738Q55%20746%2060%20749Z%22%3E%3C%2Fpath%3E%0A%3C%2Fdefs%3E%0A%3Cg%20stroke%3D%22currentColor%22%20fill%3D%22currentColor%22%20stroke-width%3D%220%22%20transform%3D%22matrix(1%200%200%20-1%200%200)%22%20aria-hidden%3D%22true%22%3E%0A%3Cg%20class%3D%22mjx-svg-mrow%22%3E%0A%3Cg%20class%3D%22mjx-svg-mi%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-66%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3Cg%20class%3D%22mjx-svg-mo%22%20transform%3D%22translate(550%2C0)%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMAIN-28%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3Cg%20class%3D%22mjx-svg-mi%22%20transform%3D%22translate(940%2C0)%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-78%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3Cg%20class%3D%22mjx-svg-mo%22%20transform%3D%22translate(1512%2C0)%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMAIN-29%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E)，通过最小二乘法可以得到不一样的拟合曲线。

![é©¬åå­¦é«ç­æ°å­¦](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAkACQAAD/4QCARXhpZgAATU0AKgAAAAgABQESAAMAAAABAAEAAAEaAAUAAAABAAAASgEbAAUAAAABAAAAUgEoAAMAAAABAAIAAIdpAAQAAAABAAAAWgAAAAAAAACQAAAAAQAAAJAAAAABAAKgAgAEAAAAAQAAAfSgAwAEAAAAAQAAAT8AAAAA/+0AOFBob3Rvc2hvcCAzLjAAOEJJTQQEAAAAAAAAOEJJTQQlAAAAAAAQ1B2M2Y8AsgTpgAmY7PhCfv/iD2BJQ0NfUFJPRklMRQABAQAAD1BhcHBsAhAAAG1udHJSR0IgWFlaIAfiAAEAAgAIABEAEGFjc3BBUFBMAAAAAEFQUEwAAAAAAAAAAAAAAAAAAAAAAAD21gABAAAAANMtYXBwbAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEWRlc2MAAAFQAAAAYmRzY20AAAG0AAAENmNwcnQAAAXsAAAAI3d0cHQAAAYQAAAAFHJYWVoAAAYkAAAAFGdYWVoAAAY4AAAAFGJYWVoAAAZMAAAAFHJUUkMAAAZgAAAIDGFhcmcAAA5sAAAAIHZjZ3QAAA6MAAAAMG5kaW4AAA68AAAAPmNoYWQAAA78AAAALG1tb2QAAA8oAAAAKGJUUkMAAAZgAAAIDGdUUkMAAAZgAAAIDGFhYmcAAA5sAAAAIGFhZ2cAAA5sAAAAIGRlc2MAAAAAAAAACERpc3BsYXkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABtbHVjAAAAAAAAACMAAAAMaHJIUgAAABQAAAG0a29LUgAAAAwAAAHIbmJOTwAAABIAAAHUaWQAAAAAABIAAAHmaHVIVQAAABQAAAH4Y3NDWgAAABYAAAIMZGFESwAAABwAAAIidWtVQQAAABwAAAI+YXIAAAAAABQAAAJaemhUVwAAAAwAAAJucm9STwAAABIAAAJ6bmxOTAAAABYAAAKMaGVJTAAAABYAAAKiZXNFUwAAABIAAAJ6ZmlGSQAAABAAAAK4aXRJVAAAABQAAALIdmlWTgAAAA4AAALcc2tTSwAAABYAAALqemhDTgAAAAwAAAJucnVSVQAAACQAAAMAbXMAAAAAABIAAAMkZnJGUgAAABYAAAM2aGlJTgAAABIAAANMdGhUSAAAAAwAAANeY2FFUwAAABgAAANqZXNYTAAAABIAAAJ6ZGVERQAAABAAAAOCZW5VUwAAABIAAAOScHRCUgAAABgAAAOkcGxQTAAAABIAAAO8ZWxHUgAAACIAAAPOc3ZTRQAAABAAAAPwdHJUUgAAABQAAAQAamFKUAAAAAwAAAQUcHRQVAAAABYAAAQgAEwAQwBEACAAdQAgAGIAbwBqAGnO7LfsACAATABDAEQARgBhAHIAZwBlAC0ATABDAEQATABDAEQAIABXAGEAcgBuAGEAUwB6AO0AbgBlAHMAIABMAEMARABCAGEAcgBlAHYAbgD9ACAATABDAEQATABDAEQALQBmAGEAcgB2AGUAcwBrAOYAcgBtBBoEPgQ7BEwEPgRABD4EMgQ4BDkAIABMAEMARCAPAEwAQwBEACAGRQZEBkgGRgYpX2mCcgAgAEwAQwBEAEwAQwBEACAAYwBvAGwAbwByAEsAbABlAHUAcgBlAG4ALQBMAEMARCAPAEwAQwBEACAF5gXRBeIF1QXgBdkAVgDkAHIAaQAtAEwAQwBEAEwAQwBEACAAYwBvAGwAbwByAGkATABDAEQAIABNAOAAdQBGAGEAcgBlAGIAbgD9ACAATABDAEQEJgQyBDUEQgQ9BD4EOQAgBBYEGgAtBDQEOARBBD8EOwQ1BDkAVwBhAHIAbgBhACAATABDAEQATABDAEQAIABjAG8AdQBsAGUAdQByCTAJAgkXCUAJKAAgAEwAQwBEAEwAQwBEACAOKg41AEwAQwBEACAAZQBuACAAYwBvAGwAbwByAEYAYQByAGIALQBMAEMARABDAG8AbABvAHIAIABMAEMARABMAEMARAAgAEMAbwBsAG8AcgBpAGQAbwBLAG8AbABvAHIAIABMAEMARAOIA7MDxwPBA8kDvAO3ACADvwO4A8wDvQO3ACAATABDAEQARgDkAHIAZwAtAEwAQwBEAFIAZQBuAGsAbABpACAATABDAEQwqzDpMPwATABDAEQATABDAEQAIABhACAAQwBvAHIAZQBzAAB0ZXh0AAAAAENvcHlyaWdodCBBcHBsZSBJbmMuLCAyMDE4AABYWVogAAAAAAAA8xYAAQAAAAEWylhZWiAAAAAAAABxwAAAOYoAAAFnWFlaIAAAAAAAAGEjAAC55gAAE/ZYWVogAAAAAAAAI/IAAAyQAAC90GN1cnYAAAAAAAAEAAAAAAUACgAPABQAGQAeACMAKAAtADIANgA7AEAARQBKAE8AVABZAF4AYwBoAG0AcgB3AHwAgQCGAIsAkACVAJoAnwCjAKgArQCyALcAvADBAMYAywDQANUA2wDgAOUA6wDwAPYA+wEBAQcBDQETARkBHwElASsBMgE4AT4BRQFMAVIBWQFgAWcBbgF1AXwBgwGLAZIBmgGhAakBsQG5AcEByQHRAdkB4QHpAfIB+gIDAgwCFAIdAiYCLwI4AkECSwJUAl0CZwJxAnoChAKOApgCogKsArYCwQLLAtUC4ALrAvUDAAMLAxYDIQMtAzgDQwNPA1oDZgNyA34DigOWA6IDrgO6A8cD0wPgA+wD+QQGBBMEIAQtBDsESARVBGMEcQR+BIwEmgSoBLYExATTBOEE8AT+BQ0FHAUrBToFSQVYBWcFdwWGBZYFpgW1BcUF1QXlBfYGBgYWBicGNwZIBlkGagZ7BowGnQavBsAG0QbjBvUHBwcZBysHPQdPB2EHdAeGB5kHrAe/B9IH5Qf4CAsIHwgyCEYIWghuCIIIlgiqCL4I0gjnCPsJEAklCToJTwlkCXkJjwmkCboJzwnlCfsKEQonCj0KVApqCoEKmAquCsUK3ArzCwsLIgs5C1ELaQuAC5gLsAvIC+EL+QwSDCoMQwxcDHUMjgynDMAM2QzzDQ0NJg1ADVoNdA2ODakNww3eDfgOEw4uDkkOZA5/DpsOtg7SDu4PCQ8lD0EPXg96D5YPsw/PD+wQCRAmEEMQYRB+EJsQuRDXEPURExExEU8RbRGMEaoRyRHoEgcSJhJFEmQShBKjEsMS4xMDEyMTQxNjE4MTpBPFE+UUBhQnFEkUahSLFK0UzhTwFRIVNBVWFXgVmxW9FeAWAxYmFkkWbBaPFrIW1hb6Fx0XQRdlF4kXrhfSF/cYGxhAGGUYihivGNUY+hkgGUUZaxmRGbcZ3RoEGioaURp3Gp4axRrsGxQbOxtjG4obshvaHAIcKhxSHHscoxzMHPUdHh1HHXAdmR3DHeweFh5AHmoelB6+HukfEx8+H2kflB+/H+ogFSBBIGwgmCDEIPAhHCFIIXUhoSHOIfsiJyJVIoIiryLdIwojOCNmI5QjwiPwJB8kTSR8JKsk2iUJJTglaCWXJccl9yYnJlcmhya3JugnGCdJJ3onqyfcKA0oPyhxKKIo1CkGKTgpaymdKdAqAio1KmgqmyrPKwIrNitpK50r0SwFLDksbiyiLNctDC1BLXYtqy3hLhYuTC6CLrcu7i8kL1ovkS/HL/4wNTBsMKQw2zESMUoxgjG6MfIyKjJjMpsy1DMNM0YzfzO4M/E0KzRlNJ402DUTNU01hzXCNf02NzZyNq426TckN2A3nDfXOBQ4UDiMOMg5BTlCOX85vDn5OjY6dDqyOu87LTtrO6o76DwnPGU8pDzjPSI9YT2hPeA+ID5gPqA+4D8hP2E/oj/iQCNAZECmQOdBKUFqQaxB7kIwQnJCtUL3QzpDfUPARANER0SKRM5FEkVVRZpF3kYiRmdGq0bwRzVHe0fASAVIS0iRSNdJHUljSalJ8Eo3Sn1KxEsMS1NLmkviTCpMcky6TQJNSk2TTdxOJU5uTrdPAE9JT5NP3VAnUHFQu1EGUVBRm1HmUjFSfFLHUxNTX1OqU/ZUQlSPVNtVKFV1VcJWD1ZcVqlW91dEV5JX4FgvWH1Yy1kaWWlZuFoHWlZaplr1W0VblVvlXDVchlzWXSddeF3JXhpebF69Xw9fYV+zYAVgV2CqYPxhT2GiYfViSWKcYvBjQ2OXY+tkQGSUZOllPWWSZedmPWaSZuhnPWeTZ+loP2iWaOxpQ2maafFqSGqfavdrT2una/9sV2yvbQhtYG25bhJua27Ebx5veG/RcCtwhnDgcTpxlXHwcktypnMBc11zuHQUdHB0zHUodYV14XY+dpt2+HdWd7N4EXhueMx5KnmJeed6RnqlewR7Y3vCfCF8gXzhfUF9oX4BfmJ+wn8jf4R/5YBHgKiBCoFrgc2CMIKSgvSDV4O6hB2EgITjhUeFq4YOhnKG14c7h5+IBIhpiM6JM4mZif6KZIrKizCLlov8jGOMyo0xjZiN/45mjs6PNo+ekAaQbpDWkT+RqJIRknqS45NNk7aUIJSKlPSVX5XJljSWn5cKl3WX4JhMmLiZJJmQmfyaaJrVm0Kbr5wcnImc951kndKeQJ6unx2fi5/6oGmg2KFHobaiJqKWowajdqPmpFakx6U4pammGqaLpv2nbqfgqFKoxKk3qamqHKqPqwKrdavprFys0K1ErbiuLa6hrxavi7AAsHWw6rFgsdayS7LCszizrrQltJy1E7WKtgG2ebbwt2i34LhZuNG5SrnCuju6tbsuu6e8IbybvRW9j74KvoS+/796v/XAcMDswWfB48JfwtvDWMPUxFHEzsVLxcjGRsbDx0HHv8g9yLzJOsm5yjjKt8s2y7bMNcy1zTXNtc42zrbPN8+40DnQutE80b7SP9LB00TTxtRJ1MvVTtXR1lXW2Ndc1+DYZNjo2WzZ8dp22vvbgNwF3IrdEN2W3hzeot8p36/gNuC94UThzOJT4tvjY+Pr5HPk/OWE5g3mlucf56noMui86Ubp0Opb6uXrcOv77IbtEe2c7ijutO9A78zwWPDl8XLx//KM8xnzp/Q09ML1UPXe9m32+/eK+Bn4qPk4+cf6V/rn+3f8B/yY/Sn9uv5L/tz/bf//cGFyYQAAAAAAAwAAAAJmZgAA8qcAAA1ZAAAT0AAAClt2Y2d0AAAAAAAAAAEAAQAAAAAAAAABAAAAAQAAAAAAAAABAAAAAQAAAAAAAAABAABuZGluAAAAAAAAADYAAKdAAABVgAAATMAAAJ7AAAAlgAAADMAAAFAAAABUQAACMzMAAjMzAAIzMwAAAAAAAAAAc2YzMgAAAAAAAQxyAAAF+P//8x0AAAe6AAD9cv//+53///2kAAAD2QAAwHFtbW9kAAAAAAAABhAAAKApAAAAAM7MCgAAAAAAAAAAAAAAAAAAAAAA/8AAEQgBPwH0AwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/bAEMAAgICAgICAwICAwUDAwMFBgUFBQUGCAYGBgYGCAoICAgICAgKCgoKCgoKCgwMDAwMDA4ODg4ODw8PDw8PDw8PD//bAEMBAgICBAQEBwQEBxALCQsQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEP/dAAQAIP/aAAwDAQACEQMRAD8A/fyiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Q/fpjtUn0qMTZ7U6X/VmqoNAFnzR6UeaPSq+cUbhQBY80elHmj0qvupaAJ/NHpR5o9K88+JviTVvCXgfU9e0JYH1CDyUgFyGaEPNMkQLqhVio35IBBPrXOmw+PQJA1zw1x/1DLv8A+S6iU7O1jto4LmhzuaSvbW/S3ZPueyiTPal8z2rxkWPx6/6Dnhr/AMFl5/8AJdL9g+PX/Qc8Nf8AgsvP/kyl7R9mafUI/wDP2P4/5Hsvme1Hme1eNfYPj3/0HPDX/grvP/kyl+wfHv8A6Dnhr/wWXn/yZR7T+6/wD6hH/n7H8f8AI9k8z2o8z2rxv7B8e/8AoOeGv/BZef8AyZR9g+Pf/Qc8Nf8AgsvP/kyj2j/lf4B9Qj/z9j+P+R7J5ntS768a+wfHv/oOeGv/AAWXn/yZR9g+Pf8A0HPDX/gsvP8A5Mo9o/5X+AfUI/8AP2P4/wCR7Nuo3V40LD4999c8NY/7Bl5/8mV03wv8T6n4x+H+heKNajhiv9RtUlnW3DCISZIbZvJbbkcZ5pxqXdrGdbBckPaKakr20v1v3S7M7/dRupuc0VZxDt1G6m0UAO3UbqbRQA7dS5FMooAkopoNLmgBaKKKACiiigAooooAKMiivDdT8RfE/V/iNrfhLwdc6PYWOi2Wn3Be/tri5mlkvWnDAeVNEqqoiGOpJJ6YqJzsdWFwrqt2aSSu2/VL82e5ZFJkV44bD49/9Bzw3/4LLz/5MpPsHx7/AOg34b/8Fl5/8mUvaeTN/qEf+fsfx/yPY91G6vHPsHx7/wCg34b/APBZef8AyZR9g+Pf/Qb8N/8AgsvP/kyn7TyYfUI/8/Y/j/kex7qN1eOfYPj3/wBBvw3/AOCy8/8Akyj7B8e/+g34b/8ABZef/JlHtPJh9Qj/AM/Y/j/kex7qN1eOfYPj3/0G/Df/AILLz/5Mo+wfHv8A6Dfhv/wWXn/yZR7TyYfUI/8AP2P4/wCR7Huo3V459g+Pf/Qc8N/+Cy8/+TKT7B8e/wDoOeG//BZef/JlL2nkw+oR/wCfsfx/yPZN1N3+1eOGw+Pf/Qc8Nf8AgsvP/kyk+wfHv/oOeGv/AAWXn/yZR7R/yv8AAPqEf+fsfx/yPZN9Hme1eN/YPj3/ANBzw1/4LLz/AOTKT7D8ev8AoOeGv/BZef8AyXR7R/yv8P8AMPqEf+fsfx/yPZfM9qPM9q8b+wfHv/oOeGv/AAWXn/yZXPeIde+M3gxdL1PW7/QdQsbnVNPsZorexuoJdl7cJAWR3uZFBXfuGVIOMcUnVtq0y6eWc7UYVItv1/yPoXzPajzPaovX2oz61qeWSeaPSjzR6VAaKAJ/NHpR5o9KgooAn80elHmj0qDpSZoAtq27mnVDCcg/WpqACiiigD//0f34l/1bVTq3NxE1UKAJKKYDzT80AFOBGKZkUuaAPLPjaR/wrTVP+utj/wClkNafxQ+K/gP4O+Hf+Eq+IOpjTrKSdLWBVjee4ubmU4jgt4Ig0k0rn7qIpY+lZPxs/wCSa6p/11sf/SyGvnX9pj+0PDXx2+EPxY1HQtQ8T+FPCS6yl5a6VbvqF5Z3l8kUdpfCxhDSyqgEsZdFYp5noSRmvit6fqd9T/doPzl+UTutM/bS/Z71OPUsa5eWtzosSS6jaXOl30F5YiWXyY1ubZ4RLE7sQVVlBZSHHy810/gz9qX4IfEDxNZeEPCeuzXeq6gXEETWF5CGKKXb55YlQcA9TXyh8EIfFX7Q3xF+PfxR8OahqPww1G51HSfDNlc/Y7Wa+gt9Gi+0SmSG4WaHzJmuCrbgSi7VwHU19JeEvg98bfCWvQeIdZ+NmseMbWySZjpN3pWkW0F25iZY1ea2to5VAchsqw5AB4yDr0TZwPdpFbxF+2j+z54Y8QanoN/rd3cRaFd/YdU1Kz0y8vNJ0663BWiu9Qgie2hZCR5gdx5f8eK+p4po5Y1ljYMjqGVgeCpGQR7Yr8HPEifFr4ifBw+DNXk8fy+LvFesWNn4q0O20FdL8N6RDe36pfFGjtUa6iMe5fMjuJiR+9chctX6mrrnxCtP2jrL4cavc2S+Br/wxfzWVlbxs1w8lnNaQvNdTNgAkTlUjjGAAWZiSAok7f12uEnrp/Wtj0RPi9oD/F+X4PCFxfQ6JHrZu/Mi+zmKS4a3EQ+bdvBXJ4xjvWH8UP2jPhb8JNasfC3ia8vL3X9Rga6h0vSLC61W/Nqh2tcNb2ccsiQhuPMYBc8A5r5fT/gnV+z8vxqm8UP8PdCPgd9BjtE0/D7xqi3DO0/l/dwYiF3bs8YxXC/E/wAQfEaw/aA8bWGrt4z0O00yys4PCVn4N0SKRdctRbb3F3q8lvOI2W43II5JYAiqDzuBMRbcV31/N2/T5aldX8vyX/B+Z+i3w6+I/g74reEbLxx4E1Aalo9/vVJNjxOskTFJI5IpArxyRuCro6hlYEEAiqer/FXwNoHj/TvhlrWo/Ytf1fT7rU7WOWN1hltbJlW4ZZyPK3R71LJu3bTuxjJr81vh7bfFX4Qfso+Ar2PxlqHhzxLrtjqF5DoaabHc65rXi7Vrl7uOCSG6XPkpl/PQLGQu6R5YkQmpv2yfCnxU/aUtrfwV4c8Gyyy/Cm0XW9XvpUuLVNWvpLbbPoWlzDYZEuYnkSaVS0YyqctnbctH/X9f0u4RS/r+uv8An2P0o+GvxN8H/FzwvH408B3Ul/ok8s0MF00EkCT+SxRpIfNVTJESDtkXKOOVJHNYnwJ/5JD4W/681/8AQ2ro/hre6LqPw+8OX3h3SpNC0yfT7drbT5bdrSS0iMY2wtA6q0Zj+6VKgjHIrnPgT/ySHwt/15r/AOhtUTVp29f0Ouk/9lnf+aP5SPWweafmo6Ko4iSimA4p9ABRRRQAUUUUAFFFFACg4pd3rTaKAH5FGRTKKAH5FJu9KbRQAZzXjvhn/ktXjn/sF6F/6FeV7FXjvhnj41+OT/1DNC/9CvKznuv66M9DBfBW/wAP/t0TjfHv7W3wL+HHi298FeJNbnfUdHWF9Vaysbq+ttJS45ifUbi3jeK0Vh82ZmXC/McLzWY/7aH7OsVlpuoSeJJvI1a0jvrVl069cSW0xYRyApCww20kd8c45FfBniT4q63+zp8D/jv4V1TwLrGpeMfEOta7eQ6rHZNc6TqEGszmGwuHvwfKYxRvHD9nL+dlAiJjBr7M0X9nH4naV4K8G+GfAPxh1fwLpHh7QdO0xdOtNM0u7QyW0IV5WlvLeWQu56jcBxwOtaR1V/T8V+hwy0/H8P8AM9Yj/aW+CjfDTVvi/P4ljs/CWhyGG7vbqGa2CSgqBGscqK7sxZVQKpLEgLk1D8L/ANpX4V/FvxLdeDPDU+oWOv2lot//AGfrGl3mk3MtmzbPtEMd5FEZYgxAZkyFJXOMjPzV+0BoXxn+Hfww8MaBqPjLWvGenX3iSJ9e8Q22gWV7q+l6asTPEbWxtrZ42b7QqKZRbyOisTjgEcV+z7o3xB179onxx49tk8T6pHonhCw0/wANaj46ga0+2yX88k1zJtigi8qMtDFvi8pZVwCUXcKcdX/Xa/5ie39d7H6gyzQwLvmdY1zjLEAZ/GvKPg/8YvD3xj8Gv400eF9Pt0vtSsDFcvH5gbTLqW0lf5GI2M0RZT/dIJxXlPw98I6X+0b8BNM0z9oewsvGUwv78XavAYrWS5sb2eBXjiDZVFC4QEk4xkk5NeEfDP8AZHh/Zo+CXi/xd4B8FaPqHxis7bxG+kz24eYTQ3U80ljat5nl7wsBijZDgZXaGI+aobabv2/yBapep73ov7av7PXiDxBYaHpOuXU9rq19/ZtlrA0y9GiXV6WKLDDqZh+yOzONi4kwz/KCWIFfVbOFBPXAJ45PFfijo2jeKfiHr3wP8I2mo/ELxBoY8S2V94gbWNEXRdG059Nge8ito7WO0tWUfakQrJ88YxtMhZgK+wbH4nfEHwl8WtWttT8V3nj3TvD9nql94ps9L0tZLHSASH0qxszAjXEl+8ZG+AvKzLmVhEGiVrt38/wt/wAEOv8AXme1wftQ/Be78NaP4qttallt9f1iXQLK3WzuPtsupwStDNB9l2ecDEyMXJQKqDeSE5rp7j4weH7f4xWXwaeFzf3uiXGuLdiSL7OsVvcR27RN82/zCZARgYwDk9M/nf8AAL4ceNvD37Vo+NPxU8CvZW3xitdQutKtoYp518K3QEe+K62hooZ9Rt1DzS/IBKnl7mLDPrN5/wAE7PgFcfG+08Vx/D/Qx4IXQbm1uLAiTe+qyXMbxz+X93CxB13bsgnGKcVrG/nf7n+v6IUnvbyt96/T9WfUnxR/aD+GXwi1LTtC8U3V3d61q0bzWumaTYXOq38kEfDzfZ7OOSRYlPBkYBc4GckV2Xw3+Jfgz4s+E7bxr4Ev/wC0NKuXkiDNG8Mkc0LFJYpYpVWSOSNgVdHUMpGCK/PX4ta/480n9oPxHouojxloGj6RpWnw+FIPBeiR3B1tGjcyw3OqyW04gKTKEEcksCIuHZzu45v4b6N8ePhr+x1pHjHwzqOtWXiV7fW/EV/otvp1vNrFxq2pXjTxNeDUDHi1tgWFyqIksvWNlOEaY6q7/r+kVKOqSP0e1P4u+AdE8et8Nda1L7Brg0qXWwk8UkcD2ED7JpEnZRExiJHmKG3KCCQAQT8e/Eb4ya948j0Pxj8C/H2oy3PipIIvCPh6y0xY/t8ttdYvtR1E3cTSHSxHsUzJ5ShCTG8kssIr5+/ass/HX7U9ppXifwD4TbVvDPwrsoNav79YLiKTxSt3HFLd6NpQVVea1mgBMjDckjhEAYg4+9G/Z8+DnxD1PT/iwum6ro+qahpen2yG01LUdIkjsrXMtvavBBLCEWMu26MqMnIYHpVQVnd9H/n/AF+PYibTVo9V/l/X4H0wm7YN+N2BnHTPfFePfG3/AJFrR/8AsYtA/wDTjDXrygIoUdFAHr0ryD42f8izo/8A2MWgf+nGGsq3ws9HKP8Aeafqj2E/eP1NJSsfmP1NNzitGcDFoppPpTaQh+aQmm0mRQAtFJmlzQBZg6H61PVe3+6frVigAooooA//0v33m/1TVn5FX5/9S30rMzQBJkUuc1HSg4oAfSg800nFGecUAeZfGiC8ufhrrC2FrNfTRm2lENtG00zrFcxyMEjX5mbapIA5NeAfFTSvhT8VvEmkeM9Rs/Heg+I9CgubS11LRLHVdPuRa3ZVpoXMUe2RGKKcMpwRkYNfZufSly3qfzrOUG3dM9CjiqapqnUhezb3tvbyfY+YPhhrfwr+EHhKDwX4G8NeJ7bT4pZrh2n0bU7i4nuLhzJNPPNLGzySyOSzMxJJr0L/AIXX4c/6AniP/wAEF/8A/Gq9fDtjqfzpdx9T+dNqb1v+H/BGq2FX/Lp/+Bf/AGp5B/wu3w9/0BfEn/gh1D/41XMXHjr4e3fjGw8ez+G/Era3pljc6dBMNE1IKttdyRSzKYxHsJZ4UIYjIxgEAnP0NuJ7n86XLep/Olyy7/h/wQdfDf8APt/+Bf8A2p5B/wALs8O/9APxJ/4IL/8A+NUo+Nnh4dNE8Sf+CHUP/jVevbj6n86fuP8AeP50Wn3/AAD2+G/59v8A8C/+1PjL4h6V8GPif4rsPHHiXRPHEGuaZZTafbXWm22uadJDbXDK8qobXy8Fyi5YfN8o54r2Oz+MXhqxtILKLRvE7pbosatJoeoyOQgwCzNESx45J5Ne1bj6n86XLep/OkoyXX8P+CN18N/z7f8A4F/9qeP/APC7fDp/5gfiT/wQ6h/8aq/8ErS+sPhP4XtNTtZbG6SyTzILhDHLGSWO10PKsM8g9K9RyfU/nSU4xd7tkVsVTdN06cLXaervtfyXckopAfWjIrQ4BaXJpKKAFBp2RTKKAJKKjBxT80ALRSZFLQAUUUUAFFFFABRRRQAV863fiYeCfjB4pv8AVtF1e5s9U0zSFt57HTbi9hdoGuvNQtAr4Zd65BxwwxnmvoqmPIkQBdgoJAGTjk8Ac+tRON7WOzB4iNPmUo3Ulbe3VPz7H506d8Gf2d9M1mG7i0fx9Noltqr63D4fmt9Zk0SLUZJvtBmWyKbOJv3iocor/MFB5r61/wCF2+Hv+gH4j/8ABBqH/wAZr2PNJupKMkrXNXXw17+zf/gX/wBqeO/8Lt8Pf9ALxJ/4INQ/+M0f8Lt8Pf8AQC8Scf8AUA1D/wCNV7Dk+p/OjJ9T+dFpd/wD2+G/59v/AMC/+1Pnjwn4/wDAHgjRV8P+HPDniW3sUmubgI2ialIfMu5nnlO5oieZHY46DOBwBXR/8Ls8O/8AQD8R/wDgg1D/AONV7Jk+p/OjJ9T+dFpdw9vhv+fb/wDAv/tTxz/hdnh7/oCeJP8AwQ6h/wDGq8J+Hfh/4KfCvXJNe8FaN46tJJp7y6kt5INens5Li/kMtxK9rJuhaRnJYMVJXJ2kAmvtjJ9T+dG73NCUt7g6+Gtb2b/8C/8AtTxz/hdnh3/oB+JP/BBqH/xqj/hdnh3/AKAXiT/wQah/8ar2EsfU/nSZb1P50Wl3D2+G/wCfb/8AAv8A7U8fPxt8PdtD8SD/ALgOof8AxqvIPiTF8HPitf2mq+K/DvjCO/srW5sFudP0/WdPneyvCpntpJLZY2eGQopZCcZUEcivr/Lep/OjLep/Olyy7/gHt8L/AM+3/wCBf/anh+mfFnwdo2m2mkaV4c8Q2tlYxJBBDHoGoBI4olCoijyuAoAAq9/wuzw7/wBAPxJ/4INQ/wDjVew7j/eP50FzjqfzptT7/h/wQVbC/wDPt/8AgX/2p47/AMLs8O/9APxH/wCCC/8A/jVcR488d23jmz0XQNB0LXjdNrmjzk3Gj3lvEkVteRyyu8ssaooVFJJJ9hzX0tub1P50ZPqfzpShJq1/wNKONoU5qpCm7rb3v+ADHLE+5pKM4pMitTyhSaTIppNJQA/Ipmc0UmaAFzRmkzimUAX7b7p+tWaq2v3G+tWqACiiigD/0/32uP8AUtWTWrdf6h/pWRmgCQHtS5qPcKUHNADwQfavlX4GfEv4y+Mvi78XfDPxD8Mf2P4c8O6nbw6HcedC7eU0CloZFjYtudSlwrHjbLsOGQivqcnANeKfDIj/AIWP8Xf+w7Yf+mi0oA9xBxS5FR7qWgB4OaWowcU7cKAHg4p24VGDmloAeDmlpgOKXNADqdupgOaWgDnfFnjLwv4F0ZvEPjDUodJ01ZoLczzttTzbmRYYUz6vIyqo7kgV0uQelfnT+398NfH2v/Da+8XaN8QdU0vRbG80J59GhgsjZRrb6jC8t80ssRnDQKPOx5mzMYBUgkH1X4YxfGjVNEudD0XxfqetaXc3Dyf8JX4jsbW3vPKwqeVpthbwQK6ZVmWe4UDJyqyoRiHUS069jtw+AnUXNtHq3ov+C/JXfkfUd/4p8OaXrOn+HdQ1GGDVNVLfZbUt++mCDczKg+bao6sRgcDOSK364HwX8OfDHgb7RdaZHJd6pf8AN5qd7Ibm+ujkt+8mfJ2gk7Y12xoOEUDiu+pxv1MsSqSlak213el/l0+//IcD60u4UyiqOckopgJp24UAfK3w5+JHxn139pj4leBfFPhn7B4D0W005tGvPNhd2kkVy8kiq5cLcHdsBGV8o7sbgK+qq8X8JY/4XZ8Qv+wfoH/oN1XtFABRRRQAu40u6m0UAP3CjcKZRQA/cKTdTaKAFya+e/2iPF/hrwbonhbVfGGpRaRo8fiCymu7qdtsUMNoslyWc9lBjGa+g6/M3/goD8NfiB8VdM1Dw/4J1zVH/s/wrqeof2HY20E6XdyHS3iU7o2m3TrLIh2MMKuVwcmsMQ2o6Hp5TFe25n0Tf3J2/E/S2CeK6hjuIGDxyqHVh0KsMg/iDUteOfBfwH4z8B+HGs/GfjrU/G89ysDxvqdvaW72irGAYkFpDCCM9S4LZ717HXTJJPQ8uLurhRSEgUbhUjFpMimk5pKAFJzXj/x+8T+OPBnwY8Y+Jvhrp41LxNp+m3EthExQIkwQ4mfeygpF99lzkgYHJr1+vPvi1z8KvGf/AGBdR/8ASZ6AK3wc8QeMPFXwr8K+IviBpX9i+JL/AE63l1C03pII7hkBfa0ZZSrH5hg969KrmPBZA8HaF/2D7T/0SldKTQAuRSE+lNooAKKQkCuD8WfE3wR4Kmjsdf1NE1GdS0NjArXN9MB1MdrCHlbr2XFKUkldmtGhOpLlpxbfkd7kUmRXjJ8ffETWgx8JeALhIjjZca3dxacjg9xEgnnH0ZFNTj/hfFx8x/4Rmwz/AAf6bdEf8C/c5/Ko9quiOx5bNfHKK/7eX5K7PXSc0lePPefHjTxvOm+HdaA6pFc3Vg5HfG+Odc/UioT8ZrDQnEPxL0S+8Fgkj7VeKs+mnHc3tuXjjHp5vl0vbJb6DWVVZfw7S9Gm/u3/AAPZsjpRkVWguILuCO6tZFmhmUMjowZGU9CrDIIPqKmrU85q2jFJzSUm4UbhQIMijIplITigDStDlG+tW6pWRyjfWrtABRRRQB//1P30uv8Aj3f6Vi7vWtm74tpPpWGDmgCQHNOBxUWaXNAEhbg14r8Mj/xcf4u/9h2w/wDTRaV7Nng14t8Mv+SjfF3/ALDth/6aLSgD3CnA1FmgHFAEu6nVHRmgAlligjMs7rGi9WYhQPqTxUu6vz7/AOCg8/xki+Avjc+GE0EeDf7KT7dJeyXCaisnnru8nYPK24243EHrX1h8J5Pi7Jokx+LsWiRXYaL7GNEe4eMweUuTL9oAIfdnG3jGO9NK6uD0dj1YHNLUea8P+OvxC8T+BPDtkfA0+kv4ivrhY7ey1IXEsl2m0lltoLXMrydDnG1RksQBmonJJXZrQoTqSUIK7PcIZop1Elu6yqSRlCGGRwRkZryXxB8WFfU7nwl8NdO/4S7xJbEpNHFL5VhYvtyPt15hli6j92ivKQQQmOa+Cv2KPC3xs8d/CTR7PxNqVho/w+l1TxHLf22mSzwavcXcupTs8Ek4yI4UmaRWEThmAX5sZB/TvQPD+heFtJt9C8N2EGmafartigt0Ecaj2A7+p6nvUvmltoj0FToUG/ae/JdF8Pza39Iv59DzbR/hZLqeqQeKfinqX/CUaxbt5ltb7DFpVgxxj7PaZIZxj/XTF5Mk7SoO2vZajpcmqjBLY48Ti6lZpze2y2S9Fsh9Lk0wH1p1Ucw7dTutR0UASUUzJpQ1AHjfhL/ktnxC/wCwfoH/AKDdV7Purxfwl/yWz4hf9g/QP/QbqvZ6AHbjRuptFAD8ilqOigCSio6KAJKTIplFAD91eL/D9h4g+IXjrxuvzQR3FvoNq2cgx6YrNOy+n+kTSKf9yux+Ifi6PwN4M1XxOY/PntItttCCA091KRHbwrn+KSVlUe5qP4b+FH8E+CNI8N3Evn3dtDvu5sAeddzEy3EpA7vKzNWUtZJdtT06K9nhpz/m91fKzf8A7b953O40ZNJRkVqeYFFGaaT6UAOpCcU3JpKAF3V598Wjn4V+M/8AsCaj/wCkz16BXn3xZ/5JX4z/AOwJqP8A6TPQBueDP+RP0L/sH2n/AKJSulrl/Bp/4o/Qsf8AQPtP/RKV0maAHbqyNe8QaL4X0e88Q+I76HTdM0+Npbi5uHEcUSLyWZjwKTXtd0fwxot74i8QXcdhpunRPPcTynakcaDLMT/nNeP+HfDmp/EvVLX4g/EO1e2022cTaHocwwLcDlLy8Q8NdNwyI2RAMY+fJqJzs7Lc78JhYyTq1XaC+9vsvP8ALfsnV/tXx78Ubd9RsprjwF4J8vzftUkfl61fRD5mZY5VIsYSo6uDOQT8sRAJ+W/BP7YP7KHgj4vX/gbw54t8MWXhQ6Hb6g2vNfGW6u9TkneKSCa6kLNK6xIrks5b5ucDFfpA2GBVuQeDnnP1rxWD4IeGIPjRefGMCIzXmhQaIbE2sPkqsFxJOJg23dvJkKkdMAUoU7STev8AwzKxGYSlB06a5Y9l6rd9Xa+/ySPaIpo540nhYPHIoZWHQhhkEfUU/NNAAGAOBQelannHxp8ff2zfhV8GvEmh+D/+Er0P+2312y0/WrO7utk+nWFwhkkuHRcldqlCC3GGBr6b8G+OfA/xP8Mx+KPAusWniPQL1pIkurV1nt5TGxSRc8q2GBBHrXE/Fb4M6F8VJvC9xfSrYy+Gdcs9b3pbxSNctaBgIZC4J2OG5I54r1yC2tbSPybSJIIgSQkahFBPJ4AAqYXs79/0X/BKk9U12/V/8A8N1XwH4g+G80nib4Lwh7XzDNf+GXfbZ3aH77WBY7bS5/iUDEMpyrqpbzV9T8H+MtB8eeH7bxL4cnM1pcF0KupSWGaJiksM0bfNHLE4KOjAFWBBrpu1eEa5Anw0+I9p4xsgsGg+NrmKw1hAAqR6myiOyvc8YaXaLaTgliYum05xa5NVsexCp9bj7Op/EWz6y/uvu/5Xv9nVNW93pu6m59eKMitzxR2402mE0ZoA1bH7jfWr1UNP/wBW/wBf6VfoAKKKKAP/1f3zvP8Aj2k+lc/uFb97/wAesn0rm91AE2aUHFRA5pd1AEpYYNeK/DI/8XG+Lv8A2HbD/wBNFpXspbivF/hn/wAlG+Ln/YdsP/TRaUAe4A07IqHcaUH1oAlBzTgcVF0p27PSgBLiC2uoWt7qJJ4n4ZJFDqe/IIINQ3+oafpNjPqWp3MVlZ2qF5ZpnEcUaKMlmZiAAB6mvMPEnxVs7TVpfCHgmwfxZ4ojHz2lswW3tCcYa+usGO3XkHb80rDJRGwaz9P+Fl74ivYPEHxgv4/El7byCW202JDHo9kytuQpAxJnlTA/fTEncNyLHnFZup0jqepTy9RiqmIfKn0+0/RdvN2Xa5B/wnfjH4jk23wothp+jMcP4j1GFvKdQcN9gtW2vOSB8sz7YuQy+YMiu08GfDbw74Nnn1aIzarr14oW71a/cT31xjnBkwBGmSSIogka5OFFd4DgAAcDgfSnA5ojT1u9WRWzB8rp0Vyxf3v1fX00XkNt7e2tIxDaRJBHknbGoRck5JwuBknk1PUdLuNaHnEgNO3Cog3rTqAJOtFRg4p240APBIp24VEG9adQA/IpajpcmgDx3wn/AMlr+IP/AGD9A/8AQbqvZa8Y8KHHxs+IP/YP0D/0G6r2Xd6igB+TS7qZkUtADtwpcimUUAPyKNwplFAHn/xL1v4kaF4eS9+Fnhmz8VawZ0Q2l9qJ0yIQkHdJ54hn5BxhdnOeor4v/YP8dfHrxF4MW18aeFrQeGjq/iPdrL6495eJcR6nOPswtXgBMUT5iR/N5RFIRc7R+iFeN+LNU0r4UeHbbwv8NtEtY9c1+6mTStNto1hhe8uXM1xdTKuAsUbM09w/U8gbpHUNLajeTN8PQnWkqUN2/wCr+Xd9EU9SlHxF+K1roMDiXQfADpeahjBWXWJUzaW5wf8Al3ibz3UjhniI6V7luNcT4A8G2vgTwxbaDFcPfXO557y8lx5t3eTtvnnkxxl3JOBwowoAAArtKmnFrV7s6MfXjKShT+GKsvPu/m9fSy6C5NJSbhSbq0OAdRTd1ISTQA7cKTdTaKACvPvi1/ySvxn/ANgXUf8A0meu/wB1effFkk/Cvxn/ANgXUf8A0megDe8GkDwfoX/YPtP/AESldJuFcx4N/wCRQ0L/ALB9p/6JSud+KfijUfC/hCZ9BAfXdVli03S1I3D7bdt5cbleMrECZXx/ChqZSsrs2w1CVWpGnHduxxU8S/F/x/JbXCrN4L8D3aZQ5Meo65CQ4yPutDYnBHUG49DDz731965fwb4V0/wT4Y03wppO54NOiCb25kmkPzSSuerPI5Z2J5JJNZPhH4jeHfHGueLfD2hec134K1FdL1HzIyircNBHcgIf4l8uVDn1OKmnG2+71OnH4mM5KFP4I6L/AD9Xu/u2SO9yKQt6U2mlvStDgHUhIpN1NoAfuFMppPpSbqAGzPIkUjQqHkVSVUnAZgOBntk8Zr8rtX8bftSa7+08+jax8KdFvLh/CKSNokniyU6eYo78Ol3u+x7fOWRQoHlZAGd/av1SrG/sLQ/7d/4Sf+z4P7X+z/ZPtnlr5/2fdv8AK8zrs3c7c4zzSS95P1/Jr9R8zS0dnp+DT/Q1Ld55II5LlBFM6qXQHcFYjLAHvg8Z71JuFJuNN6U2IfuFNyTUe6jcaANrTf8AVv8A739K0azNM/1b/X+ladABRRRQB//W/fG//wCPSX6f1rmAea6bUP8Ajzl+n9a5XdQBLupc1DupwPegCbPHNeL/AAzOPiL8XP8AsO2H/pptK9kLcGvFvhmf+LjfFv8A7Dth/wCmi0oA9wyadkGuWvvGXhTS9bj8N6nrFpaapLaTX6W0sypK1pbsqyzhWI/dozqGboCRnqK8y/4WN4k+IZFt8HLaJ9LfAfxHfI32DaRnNlCNr3h6YcFIech2wVqJVEjsw2BqVbtaRW7eiXz/ACW76I9K8XeOvC/gazhu/El6IGun8u2gRWlubqXBIjt4EBklcgHhVPqeK8M8JeJ9c/aOstSu7TUpvCnhKwvrvSrmztJCmsz3Fo/lTR3FyhxaKGBBWAmTofMQ5WujvvgPZ3ejXaWfizWtO8V6i0Rm8UQyQPqyrGylo4TNFJBBE+MNFFEqck43HdXzp+yR+zv8Svh7e65r3jDx14qhjg8U65Omj3b2P2HU7eaaTyb2cRweYWnDCY7JEG/+EDK1MYObfPov+G/qx1yxVLDr/ZlzSv8AE/R/Cunk3d/4Wfd3hzw14e8IaVFofhjT4dNsYiWEUCBAWY5Zm7szEkszZJPJNb+41CD2pwOK0Stojy6lSU5OUndslBpciot1OHNMglyaUH1qIHFLuFAEuRS5qPrSg4oAkyadkVFupcigCXIoqOlBIoAkyaUN61HupdwoA8f8LED41/EI+mn6Cf8Ax26r4Z8TftnWHhX9qjTPtGk+M7Tw/qugXdlNos2h3HnXuo2V0Ps89jAw3SAI0od4uHBj3bvkx9yeFT/xev4g/wDYP0D/ANBuqteN/gn8PPiH4ntvGHiuylutUs9MutJgkS4ki8q2u5I5naPYQUlWSFGSVSHQqCpFRNPRo7MFWhCTVS/K1Z2/P5PXztbqen6ZfLqenWupJFLAt1EkojmQxyoHGdroeVYZwR2PFXq8Ft9e+IvwwT7D4ytLnxn4egUCLWbGLzNSiQA8X1lGAZSMD97bglieYlxk+leGPH/gvxpE0vhbWrTUjGdskcUq+bG3dZIjh0Ydwygj0ojVTdtn2Lr5bUguZe9H+Zar/gejszsMmjJrB0nxR4e1681XT9F1GC+udDufsd/FC4d7W4KLL5UoH3H2OrYPOGB6EVths9Oas4CTJoyaydZ1zRvDthJqniC/g0yzhBZ5rmVYY1A6ks5Ar83/AIm/8FAND0z4raH4T+Hd4suhXel6jJe3d9oupv5V1C8Ygmgjhi825iVS5ZY12uSmJFByYlUSdup2YfAVaq5or3e70S+f9PsffPj/AOIekeANNhnu4ptS1TUH8jTtMtAHvL+4I4ihQkDHd5GISNcu7KoJrA+HfgfXLG/vPH/xCnivPGGsII3WAlrTTLQHcljaFgCUU8yykBppPmIVQiInwz8G6LZQJ4+m1C48S67r0CSvq18nlzGCUB0hggIUWsCgjESgHjMhZ8sfWs0lB3vL7jpq4qnSpujh9b7y7+S7R/F9bbDsmim7qTca0PKH0mRTKKAH5FJupmQKNwoAdk0lN3UhOaAH5rz74sn/AItX4z/7Auo/+kz13teffFkj/hVnjL/sC6j/AOkz0Ab3g4n/AIRDQv8AsH2f/ohK8+1ADxP8bdNsH+e08Faa2oMCOPt+ploIGB9UgSb/AL7Fd74QcL4P0LJwBp1pyf8ArgleZ/BjVdL8XT+NPiDo95HqFlrmuSw2s8ZDI0GmRpZgI3dfMSQ/UmsqmrUf60PTwHuwq1eysvWWn5XPTfGHhbTfG3hm/wDCmrT3VtZ6jH5ckljcy2dyq5BzHPCyyRnjqpBr4m+BX7Hdl8P/AIoeOvGOs3viCO3HiKG90Et4ivpku7RLOFC97H5xFw/nB1zcBm2BV+6AK++ycU3Nax0d0eY9VZj89abuFNJzTScUAPLelNzjmm7qaTQA/IppOaaWpN1ADifWkJpmcU3d6UAPyaQtTCc0zdQBIT6U0tTd1NJ9aAN/SjmJ/wDe/pWrWRpBzFJ/vf0rXoAKKKKAP//X/e/Uf+PKX6f1rkN3vXXanxYTfT+tcXu5oAnDU/dUAOKXdQBNng14x8NefiJ8XMHBOuWPPp/xKLSvYs8Zrxj4an/i4vxb/wCw7Y/+mm0oA+HPHf7Pfx38aftL+FLbxV8QGvxD4e1m4i1ZdAheztMXkHkWciN+5kZgwkAkyS0WQuM1+pGmQXFnp1raXc4uZoIkjeUIIxIyqAWCLwuTzgcDoKnDcYzxS7qinBRVkdOIxdSrbnei2XRfInzSg4qAH1p4JFWcxNkUuah3U4HuKAJgfWnZFQ7jSg5oAmyadmoQSKXdQBNnHelyahBzTsmgCUN607NQbjTgc0ATZNGTUYOKXcaAJA1LkVFu9aXIoA8i8LH/AIvZ8QP+wfoH/oN1XsWa8a8L/wDJa/iBj/oH6B/6DdV7Fk0ASZI6V5F8V/hHonxL8P3lpDHaaXr8ibbTVzZxz3No+Rl0ztJJHH3u9etbjRuqZRTVma0a86cuanJp+R+Uf7Ov7L3xm0j4mfEeW8+LXiTR9J0rxSiypHaRQLrsIsrdzcebKjnv5G9CcCPGdw4/QZ/hOtwNl54y8TTx/wB3+0vKz/wKKNG/WvVtxPXnFG6p9jGyVjr/ALUxF2+f+vU8z0j4M/DTSL2LVf7GXUtQhJZLvUpZdRuEJ7rJdNIV/DFP1P4WaBqvxX8P/F2aeaPVPDml6hpMFugQWzw6jJDJIzgru3qYFC4YDBOQeMek7qN1VGCWyOeviqtV3qyb9XclyKNwqLdRuqjnJN1JuNM3GjcaAH5NJk0zcaMk0APyKTIplISBQA8t6U3NN3Um40APz715/wDFg/8AFrPGX/YF1H/0meu8rz74sEf8Kt8Zf9gXUf8A0negDmNf8N+PPFPwz8O2Hw+8Xv4Mv47S0kkuksob4yxfZQDD5c42jJIO4c8Yr5u/YX+H3xQ8NfDfR9V8QeNprvQHfVgNDk02CAJcSXshafzwPN5cO208fPjoBX2p4Pb/AIpHQsf9A+z/APRCVvoqRqEjUIo7KABzz0FQqa51Py/y/wAjqjibUJUe7T+6/wDmS5FIT6UwnFJuqzlHZxRkUwnNMLelADyc0maZuNNJxQBISKZmm7qQnNAD80wnNMLelISTQA/d70hPpUZOKTcaAHZozUROKbuNAHTaMcwyf739K2axNEOYZP8Ae/pW3QAUUUUAf//Q/ezVeNPm+n9a4gNXbaucadP9P61wgagCfcKUNUG73pQ1AFgt8prxX4bTRL8Rvi0GkUH+3LHgsAf+QTaeteyZ9a838RfBv4TeL9Ym8ReKfCGl6rqlwiRyXVxbI8zpECEDP1IUE4z0zQB6b9pg/wCeqf8Afa/4077RD/z1T/vtf8a8X/4Z4+A//Qg6L/4CJSj9nf4D9/AGi/8AgIlAHtIuIO8qf99r/jS/aYP+eqf99r/jXi//AAzx8B/+hA0X/wABEpw/Z3+A/wD0IGi/+Ai0Ae0C4g/56p/32v8AjTvtMH/PVP8Avtf8a8X/AOGdvgN/0IGi/wDgIlH/AAzr8B/+hA0X/wABEoA9pFzD/wA9U/77X/Gnfabf/nqn/fa/414t/wAM7fAb/on+i/8AgGlO/wCGdvgN/wBE/wBF/wDANKAPaPtMB/5ap/32v+NOFzB/z1T/AL7X/GvFx+zr8Bj/AM0/0X/wESl/4Z1+A3/QgaL/AOAiUAe0/aLf/nqn/fa/40ouYP8Anqn/AH2v+NeLD9nX4C/9E/0X/wABEp3/AAzp8Bev/Cv9F/8AARKAPaRcwd5U/wC+1/xrn9R8a+D9I13TfDGq63ZWerayksllazXCJNcrAVEpiUnLbN67sdMj1rzf/hnX4Df9E/0X/wABErjde/Y6/Zt8R+J9A8Val4GsRc+G3kltYYQ0Nq0smMSTQoQszJj93vB2kkjnmgD6g3e9OyaroFRQiDCqAAPQDpTw1AE271pdwqHcaXd60AeQ+GZok+NfxADSKp/s/QOCwB+5detev/aIf+eqf99r/jXnXin4RfCzxxqo1zxj4T0zWdRWJYBcXVsskvlISVTeeSqliQO2T61zv/DOfwE/6J/ov/gIlAHs/wBoh/56p/32v+NL9ph/56p/32v+NeL/APDOnwE/6J/on/gGlH/DOfwE/wCif6J/4BpQB7T9pg/56J/32v8AjR9pt/8Anqn/AH2v+NeL/wDDOfwE/wCifaJ/4BpR/wAM5/AX/on2i/8AgGlAHtP2m3/56p/32v8AjSfabf8A56p/32v+NeL/APDOnwE7fD7RP/ANaP8AhnP4Cf8ARPtF/wDANKAPaftNv/z1T/vtf8aT7Tb/APPVP++1/wAa8X/4Zz+An/RPtE/8A0o/4Zz+An/RPtE/8A0oA9o+0wf89E/77X/Gk+0w/wDPVP8Avtf8a8Y/4Zz+Av8A0T7Rf/ANKT/hnP4Cf9E/0T/wDSgD2f7RD/z1T/vtf8aPtEP/AD1T/vtf8a8Y/wCGdPgJ/wBE/wBE/wDANK57V/hB+yz4fnS21/wz4X02aRd6pdJBCzLnGQHYEjPek3bcuFOUnaKuz2fWvGvhHw5e6Zp3iDWrPTbnWpmt7GO4nSNrmZEMhji3H5nCAtgc4BPaukDA8ivirxb8BP2IPGs+kya7pvhcxaROblIIbuCGGZypQC4VJP3qDOQjcZ5xxX0nb/EX4YWdvFaWnijR4YIVVERb63CqqjAAG/oBxU+0j3N/qNb+R/cz0It6U3Jrhf8AhZvw4/6GzSP/AAPt/wD4uk/4Wd8OD/zNmkf+B8H/AMXR7SPcPqVb+R/czut3vXAfFdgPhb4yJP8AzBdR/wDSd6kPxN+G4/5mvSP/AAPt/wD4uoJ/iN8MruCS1ufFGjSwzKyOjX1uysrDBUgvggjgij2ke4fUq38j+5mz4QuYP+ER0L96n/IPs/41/wCeCe9dAbmD/nqn/fa/418vw/Dr9jm3hS2ttH8GxRRKEREa0VVVRgBQHwABwAOgp/8AwgH7H3/QK8H/APfdr/8AF0e0j3D6lW/kf3M+nPtMH/PVP++1/wAaQ3EH/PVP++1/xr5j/wCEA/Y//wCgV4O/77tf/i6b/wAIB+x//wBAvwf/AN92v/xdHtI9w+pVv5H9zPpz7TB/z1T/AL7X/GkNzB2lT/vtf8a+Yz4A/Y/7aV4P/wC+7X/4umnwB+yB/wBArwf/AN92v/xdHtI9w+o1v5H9zPpw3MH/AD1T/vtf8aQ3MHeVP++1/wAa+Yz4A/ZA/wCgV4P/AO+7X/4um/8ACAfsg/8AQK8H/wDfdr/8XR7SPcPqNb+R/cz6c+0Q/wDPVP8Avtf8ab9pgP8Ay1T/AL7X/GvnPTvhb+ynrF4mn6R4f8K311LnZDALeWRsDJwqMScCulP7O/wH/wChA0X/AMBEqk09jGpSlB2mrHspuYf+eqf99r/jTftMH/PVP++1/wAa8c/4Z3+A/wD0IGi/+AiUz/hnf4EdvAGi4/69EpmZ7KbmD/non/fa/wCNM+0QdPNT/vtf8a8b/wCGePgP/wBCDov/AICLSH9nj4Ef9CBov/gItAHo+h+MfCniefULTw5q9pqU+kzta3kdvMkj286Y3RyqCSrDI4I710Bb1rwb4Zfs2/Bb4QeJta8ZeAfDUGna3r0jvc3eS8gSTGYYsnEcI2jCKAOM9a9zLelAHW6Cc28v+9/St2uf8PHNvL/v/wBK6CgAooooA//R/enWf+QZcf7v9a8/DV3+tf8AILuP93+orzndQBYzTwaqg1JuoAn3U8NVVWp+6gCxu5qlqt82naVe6gih2tLeaYKehMaFgD9cVMGrG8Tt/wAU1rH/AF5XX/olqT2NaMU5xT7nm3hnVvjZ4i8N6T4gJ8N251O0gujFsvX8vzow+3duXdjOM4GfQVvBPjZ/z38N/wDfq9/+Lq78P7y1sfhj4bvL2ZILe30ezeSSRgqIi26EszHgADkk1wehftTfs5+JbgWmgfEbRL2Z5fJVI7tCzvtd/kH8Q2xudw4wrc8GsoQulqz08XjFGrOMacbJvp5nYbPjZ/z38N/9+r3/AOLpwT42drjw3/36vf8A4uuHt/2sv2YbqeO2tvix4YlmmYIiLqtuzMzHAAAbJJPAFbnjn9o34F/DPWv+Ec8feOdK0PVBEk7W1xcKskcMhCpJIozsRiQAzYBNV7PzZzrH/wByP3G7s+Nv/Px4b/79Xv8A8XTgnxt7XHhv/v1e/wDxdd/YajY6pZW+pabcJdWl1GssM0TB45I3GVZWHBBHINcn4++Ivh34bWOk6j4nM6QazqllpFuYYmlJur+QRQhsdFLkAt0HU0ey1tdi/tDryR+4ztnxu/57+G/+/V7/APF0bPjb3n8N/wDfq9/+Lq/8Qfil8O/hTY22p/EjxDZ+Hbe9mFvbm8lEZnmPIjiX7ztgZwoJqz4A+JXgL4paCPE/w61+z8R6UZGiNxZSiVFkX7yMR0YZ5B5oVLzY3j7fYj9xkiP43drjw3/36vf/AIul2fG7/n48N/8Afq9/+LrF+PfxI1X4YfDe51/w3DFc6/fXlhpWlxTqzxPfalcx20PmKpViil9zgEHaDzXUSfFP4d2Vr4luNQ8S2EK+C9q647zBF09zGJP3+77gKkMM9QRij2Wl7sPr7vb2cfuKWz43f8/Hhr/vze//ABdO2fG/tceGv+/V7/8AF13ulatp2uaZa6zpM63VjfRJNBMmdskbjKsuR0I5FaO6m6XmxLMP7kfuPJtI8UfEKy+ImneC/F8WkzW2p6ZfXyTaeLhJEezlgj2Ms24FWExOQQRt6HPHsAb1rx3WDn45+FD/ANS/rf8A6U2VevbqmnfVeZeYqNqc4xSvG7t6tfoTA+lKGqDdS5Fanmk+6jdUINO3GgCXIpciod1G6gCbNG73qHcaN1AE+40bjUG6jdQBPuNJn3qLdSbqAJs0VDuo3CgCXIrxSDTdM1H48a+2oWUF0U8N6TtM0SSEZu7zpuBxXs26vI9MJPx38Q9s+HNJ/wDSu8rKqvh9f0Z6mWyajWa/l/8Abonon/COeGu+j2X/AIDRf/E0Hw54a/6BFkP+3aL/AOJr88dS+Ivx1+IPg74oftB6B8QU8DeFfAtzq1voekrY29xBeJobPFcz6rJMrSsJpo3EawNFsTBJZjgcg3xz+JWi6B4TTx78Z5vDHiG+8PaVf6hYL4NuNV8u6u42kkb7TbL5RzwNgGU28/eFWkmvu/Hb8jidap/M/wDhj9O/+Eb8M9f7Isj/ANu0X/xNJ/wjfhrto9l/4Cxf/E1+emv/ABx8Tr8FtH1jT/jPDG2qeI3sLrWn8NXFtqzWqRlmtNK0p43aa6yBiRo3TZuYK2AK6L9lf4zeL9e+JnxJ8C+LPFGr+IfDfh7T9L1bT9Q8UaQuhanDHdmeOZJo/Ktw0IMO5JDEp5PJGKaS/r0uL29S3xP7/kfdP/CN+G/+gPZf+AsX/wATTT4b8N99Jsv/AAGi/wDia+Mv2l/2mJtC+FfgrXfg1qjzp8SNaTSLXWbCxk1Sa2thHLLNc2lmiObiXbERENpUkhyGUYPlHgT4gfHzxbrHi74YeFPF3iia0uNEbVdF8VeJ/DI0i4sNStHXzLG4WW1hhnt51IIYRB1G8B8gFSy/r0uP289Pef8ATsfpEfDnhr/oEWX/AIDRf/E0n/COeGv+gRZf+A0X/wATX5lfC/8Aai+Lvxw+H/xN+NVjqdr4a0P4deHbyyTTLJoL17rxBBbmae/d9pIt0KgWyggSZYsMba6ax8X/ALQ/wx8OfCD4w+O/iE3i218dajo+l67obWVrBZW662FjhlsJIoln8yGRl3+ZIyupchV+VQJK9vT8dEN1qlr3fX8LN/mfdS3fw1k8WyeBY4dNfX4bNNQks1tojLHayOY0lcBPlV3VguSN204zg10X/COeGu2kWX/gNF/8TXxH8HNfvl8H/Hv49S3un6d4hv8AX9btYr3VXIsbO18O7rGzSZ1+ZbdDG8zKP4pHI5bNeI/D/wCP3j+2+K3wqtLT4k+IfHmmeNdQfTtXXVPDI0rQyZLWW4WXSro20DsFeLCAyS5jJYk8NTUVdK3b8SXiJ2b5n1/A/Un/AIRvw31/siy/8Bov/iar3Gh+FrWCS5m0qzEcSs7EWsRwqjJOAuTx2FeeeGvjZ8NfiJq9x4P8H+IC+qSwXMlvIIJY0njt38maezllQRXKwSMocxllUkBuoz8Z/s9/GD4+/GjxxF8M7jxLaw23wi1C6tvF+u2iWsr+IpkkdLOC3jXcsCMg33TBVIkHlx4+bCUE9EN15pXcn9597eFpfh1418Paf4s8Jwabquj6pEs9rdQQQtHLG3RlO2t0+HPDn/QIsv8AwGi/+Jr5U/ZnmPh74jfHL4VWWV0fw34oW9sI/wCGBNbtIr6eNf8AZ+0SSOB23YHGAPr/AHU3FaNC9vNO3MzxzxvpGj2XjP4bT2Wn21vKNdnAeKGNGwdNu+6qDXsm7ivJ/iCceLvhv/2HZv8A023depBuKyprWX9dEdmOm3Sotv7L/wDSpEu6mk1EWpN1anmkm6mlqiLYpN1AEm6mlqiLGk3UAdn4bObab/f/AKCujrmvDBzbTf7/APQV0tABRRRQB//S/efXP+QTc/7o/mK8z3V6VrxxpFyf9kfzFeXbvegCwrU/dVUNT99AFkNTt1Vlb1PHrXzH8FPHfxj8TfFT4saF8QtFj0/w/omp28WiyJPHI0cTW6MYZFXnLoVnDZODIU/hoA+pw1YviZv+Ka1j/ryuv/RTVphqyfEKyT+H9UhhUvJJaXCqoGSWaJgAPcmlLY2w/wDEj6o+Pv2i9N1rV/2OtKsdMtLu/sTD4cfV7exV3uJdHjmt2v0VI/ncGAMHRcllyMHOK8ul8VfDP4/ftW/DjRfhTYRm0+HPh3VtTTUZtNkgtg93GlpaQoJFjMsKbnLhfl3LtDZVhXrev6h4S+IXwj8LeEb3UfF/hHVdHi0y6hvdL0jUIrm2u7ONflZXt3jkQ8q8cisjA8jpUnwqg8B/DrxJrvjzXdd8W+NvFviGKC1udU1TQbqN0tLUsYraGG1tIoY41Z2c7UyzMSSeMYUK0bav+rWPWx+U4p1ZtU3u+j7/AORPa/Bf9o+KeFp/FPw/aJGQsE8GyKxUEZw3244OOhxxXzJ8RviPcwfEH41eHLnW9P8AhxcTStbppVn4ebU/EPifFkipdwySF0dGyYwEhYIFyWGK/Qz/AIXZ4MH/AC663/4ItS/+MU0/GfwMZBMbLWTIBgMdA1HcB6Z+z5qpVIP7RzRynFL/AJdP7mfMXwX8cfETwX+zp+zvp/h/SrVtI1G20bTdYvLuVluLfzf3QggttuWkLD5nZlEYHAYnA2/2n/2T4PixqWk+J/D1zrc2qT+ItHub+BNevLazhsLaRBcS29t5vlRSrGuVaNVff8wIYk1654q8efDjxfb6Za6jBr8KaVqFrqUXk6HqK5mtH3orf6OflJ6gV1v/AAuzwX/z7a3/AOCLUv8A4xWnt4X5r9b/AJE/2NirWVKW1tn5nxp8a00j4E+Pvht4W0fVbTwtpsNrq9xH428XRXXiKayklki32MM9xKPLknU7laRzwm0KelZ/7MGp+PJrP416j4K12Fbu98YWV1LrXiLTW09BphsoBcXq2IEC8qjCEEqp4dieQftmT40eBplCzWOtOoOcNoOokZ9ebevMPjBqvw1+M3hA+C9buvFOl2L3MFzKbDRL5WnFu29YpRJaurxFgCyMMMBg5GRUe3iuv9XRX9j4p/8ALp/c+x5P4P8AHniP47+PvhPpviOVL2w0G88QeKTqVrbSWdpqNlprtp+lXBhkZ9qXH2hpozuKv5XmJ8pFfP8A8UX0f4qftG6L8ap/Dd4/wVfWtO8Na5dCSaJNbv7F5H0+9ltAn72xtrthFvJAkY5bMajP374K+IfhbwjoqaTqOp+KfEs6O5F5qPh+6W4EbHKxD7NZQxiOMcKAmcdSTzXY/wDC6vBG3Z9j1vb6f2DqOPy+z1ca8E009vz/AOG/HXyF/ZGLaa9lLXyf9bnsa7UARQAFGABwAB0x7U4N6V44Pjd4L/59dc/8EOpf/GKePjb4L/59Nc/8EOpf/GKn20O4f2Piv+fT+5k+rn/i+fhQ/wDUv63/AOlFlXrm6vnrS/FNn4z+Muh6nollqSWmnaHqsU815p11ZRrJcT2pjUNcRoGZgjHAz05r34N70qTvzNd/0LzSlKCpQmrNR2/7ekWN1O3Gq+6l3fhWp5RPuo3V8t/D/wAc/GbV/wBo/wCIvhHxTokdp4F0uz059InWaNpVeQPueRVO7FxhiB/DsGcbsV9PbvegCfdS59Kh3UbqAJ8mjcah3CjdQBPuNG41Buo3UAT7jSZNQ7qN1AEufeuA+IviHU/D2maTPpMixz3us6ZZsWUODDcXCpMAD3KZwe3Wu53CvA/jx4v8PeD4/B2q+K7yPT9HtNZF9e3EpOyG2sbaaVpGxztD7M8d6zqytFs9DK6alXjdXSu/uTf6H0EWGTg15FpzkfHXxEV6jw3pOM+v2u8r1GC4juYI7iFg0cqq6n1VhkH8Qa828RfDq41jxS/i7SPFOp+Hb2azisZhYi1ZJYoJHkjLC4hlwymRuVIyDz2oqp6WXUMtqQXPGcrKUbX17p9Lvoflj4u+CvxD+Ien658K/D3hrxv4OPjLxG11q+jPdQS+EbaKS9Et5fRXpAuHS4iVmW2Vtokf5o8c1+ytpa29jaQWNuqrDbRpEgwOFjUKB+Qryv8A4V34uxj/AIWb4h/796Z/8h0h+HXi0/8ANTfEP/frTP8A5DpqpK1uX8v8yngaTd3XX3S/+RM34y/BOw+LcnhnV7fxBf8AhTxD4OvXvtK1TTlgeWCWWJoZAYrmOWJ1dGIIZDjqK5b4e/sy+H/B3jDxR418TeJtU8d6h41sLSw1j+3fs80NytnI7xsIooo0jXDlfKRRHgZ27ixPdf8ACuvF3/RTfEP/AH70z/5DpP8AhXfi7/opviD/AL96Z/8AIdSpv+X8hvBUf+f6+6X/AMieUaV+z/YeOfgbongPxrHd+G9V0LU7vUtMu7J0ivdMuku52tp7dhvQfunA2kFSjFWGCRVW7/ZQl13S/EkfjP4o+Jtf1rxRZxaXcapK9rDLDpivvltbeGCGOCIT8rJKE80g8MDgj2D/AIV34t6f8LN8Q/8AfrTf/kOvmz9oCL9o3wRqvw9tvhZ4p1jXbfW9ehs9Va4bS4ils6sTHDm2X96wDOCcr8mDyRT55fy/kH1Gj/z+X3S/+ROg+Jv7MfhjQfD/AIs8WfCW1udM1KXwTd+GzoenhBZ6pFFAwskkiOAJoiSsbqVOGIYkdMH4G/spXGl+GvhhqnxJ8Y+INfh8FWlpeab4d1U2gtdM1AQBcsYIUlla33MsYkkcJ2yQCPoz/hXvi3G7/hZviH1/1Wmcf+Slc5oHh7xN4gn1m3h+I/iOFtGv5LB90elne0aI+8YtOAQ4wDzxSVVp35Xd+nS7/U0WX0pR0rRst9JdbL+Xyt+Z5d4e+DC/2t8Z/gL4r0+5k8AfEOSXXLC8g+VI/wC1F2ahaeYM7JUuFM67vvCXjO04vWX7KN0fEXg3xb4y+KXifxZd/D+7W70iC6FhFbRqsLwvE0FvbxLIZEbb5j5dcYVlDOG9l/4V34u/6Kb4g/79ab/8h03/AIV34t/6KZ4g/wC/Wmf/ACHTjOStaP5dNvuM3gqLveuvul13+yfLPhnwd8SfjLr2s3XxF0LxJ8Lrm40+/wBI0I6elglroWlTSp5wSVJpt99fKis0oj2wqNke1gzye5+Bf2a/hr8IfFeleNfh6LnQ4dG0AaFPYW+Hgv7W3YywyTqQXe4jYuVcHcxdg2eMdl/wrrxYOnxM8Qf9+tM/+Q6b/wAK88W9viZ4g/796b/8h0lJraOvy/z8xvB0nvXX3S/+R8jg/wBmrwl4g0/TvF/xR8a2MmleIfiZrUusS2UwCy2dnHGlrYQSDnEi28SNIMnDswBwBX0vvHrXkR+Hni3r/wALM8Qf9+9N/wDkOm/8K88W/wDRTPEH/frTf/kOq53tyv8AAX1Kjdv20ful/wDIlv4gN/xVvw3x/wBB2b/023VeoBuB3ryWz+Gl+uvaPr2u+MtY13+xJ3ube3u1s0h814nh3N5FvG5wkjYG7GecV6nuA70qad22icfKHLThCXNZb692+qXcmLUzdUZamb60POJi1NLVCWpu6gCUtTdwqItSbqAO88LHNrP/AL/9BXUVynhM5tZ/+un/ALKK6ugAooooA//T/ePxAcaNdH/ZH8xXlG+vVfEZxol2f9kfzFeRb6ALIan7veqgbmn76ALO7g1498OG/wCLhfFb/sN2P/pqta9Z38EV478OnC/ED4rMTwNbsifw0q1oA7j/AISPUI/iSvhSXYLG40g3sPy/OZorgRy/Nn7oV04x1Oa7nfX5kfEj9tb4Uaf+0b4Dt9B8T6Tf6Bp9nqUWr3KM5uIHmliiMPTGFx5rKRnEZOeMH9JbG/tdRs4NQspRNb3MayxuvR0cblYexBzWdNt3/r+up6GMpRUKc49VZ+q3/Bp/M1/Nf+8fzNL5r/3z+ZqoGpwatDzy55rj+JvzNKJX/vH8zVTdTg1AFzzX/vn8zThK/wDeP51T3U4NQBc81/7zfmaUSv8A3j+ZqpvpQ3NAF0Sv/eb8zS+a/wDfP5mqm6l3e9AFwSv/AHj+ZpfNf+835mqoal30AWfMY9WJ+ppd1Vg3vTt1AFjdS7s96rb6Xd70AeVeGG/4vP4+/wCwfoP/AKDdV6/uNeM+GG/4vN49/wCwfoP/AKDc16/uoAsbqN1V99G+gCzvo3e9V99G73oAsbqN9V93vRu96ALG73o3e9UZLu3hlihlmRJJiRGrMAzkDJCg8nA64qbd70AWN1fnN+2p8NvHHxit/F2keBdb1X7R4b8KORo9jHA8V/danKUWBvMjZ/3kcR37HVgAu0jJz+hss8UETzTuEijUs7HgKqjJJ+gryb4PibVNH1L4g3alZvGV6+oRA5ytioENkuDgjMCK5HZmNYVo8zUT1MD7lOpWfay9Zf8AATLPwg8CeJvAXh9rPxP421fxpNcrA6NqyWqPahYwDFGLWGEbc9dwJz3r1ncah3Um6umUru55UVZWJt3vRu96g3Gk3e9SVcn3Uhaod1NLUBcm3e9eQfFc51D4e/8AY02f/pPcV6vuryH4qt/xMPh9/wBjTZ/+k9xQFzm/2ivjzo/wT8GX91PFqLavd6dfTadJZ6Vdajbxz28fym5e3RkiXcyn94VBGTnAOPMv2QfjPp/xUi8STYvBqt7Hpes3TXOnz2MbveWccMrQiZQGQzQuVKkgjkEjmvrHxNoVj4s8Nar4W1RpFstYtZrOcxttfy50KNtbscHg145Z+H9P+FPjLwNp+ktINGu9JHhfdKxeTzLFfPsTI/clFmXJ6sR61lLR3fl+q/VevyPRwK51Okt3F/g4y/KL9PmfQhamb6i35rLvdZ0rTriztdQvIbabUJfJtkkdUaaUKW2Rg4LNtBOBk4BNannGxuphb3qLfTC1AXJy1ML1EWphbmgLk5amF6iLUwtQInLUzd71EXOKj30ATs1N3e9QlqbuoAlLUm73qBn5pu80Aej+Dzm0uP8Arp/7KK66uM8GHNncf9dP/ZRXZ0AFFFFAH//U/d/xMcaFeH/YH8xXjYavYvFH/IAvP9wfzFeKB+1AFoNzUm6qW6pN9AFncMV5B8OW/wCLgfFXn/mN2X/pqta9W38V5F8On/4uB8VP+w1Zf+mq1oAvax8G/C2vfFCw+JuoYkksdFvtEaxMMZt5oL+WKSR343bh5QA7YJqp4I1m78Da1D8JfFMjtGFY+H9QlJZb60jGfsruf+Xq2Xgg8yxgSKWIkCewK9c94q8L6J4z0aXQtehMtvIVdHRjHLDKh3RywyLho5EblWUgg1nKD0cd0ejg8VBJ0q3wS+9NbNemzXVdnZrrw1KGrwrTfGev/DySDw/8Vp/tVk7+VZ+I1j2QTA/cTUFQbbacjgyYEMjcjYWEY9rWVXUOhDKwyCDkEHuDThNMyxeCnRab1i9mtn6f5PVdUi7upwaqu+lD1ZxlvdTg1Vd1LuoAt7qcGqqGpQ1AFvdS7veq26l3UAWg1O3e9VQxpd1AFoNTt1VN1PDmgCzuo3VW3Uu73oA8t8MN/wAXm8e/9eGhf+g3Neu7q8a8Mt/xeXx5/wBeGhf+g3Neu7qALG6l3VW3Uu73oAsbvejd71W3Uu73oAsbvegNz1qvu96N3vQB+bP7QVt+0mf2ifg41rqPhQRPr+r/ANgb7O8LRL/Zk+77cRMBITFvA8sJ82DyAc/oZ4T/AOEqTw3pyeOJbObXlhUXr6ekkdo038RhSVmdU9AzE+5rWZIndJHRWaPJUlQSueuCeR+FY3ibxNpHhDQb3xJrsxhsbCMySFVLux6KiIuWd3YhUVQSzEADNF0omkacqklGKu3pY4L4rXk2vnTvhPpbst14qLfbXTOYNIgI+1yEgggygiBCDnc+QDtNeuW8UFrBHbWyCKGFVREUYCqowoHsBXlPw10LWE/tDx54vhMPiLxMY3kgO0mws4gfs1kCuc+WGLSHJBlZyDtwB6pu96ypq/vPqd+YTjBRw8HdR3feT3+S0S9L9Sxu96TdVffSbq1PMJy3pSbqg3Um73oAsbqQtVfdSFjQBNuryL4qt/xMPh9/2NFn/wCk9xXqpavI/io3/Ew+H/8A2NFn/wCk9xQB7Du4FcV8QfC7+MvCd5oltMLa++S4spyM+Re2ziW3k7ZCyKMjuMjvXW7uKbupSimrM1oVpU5qpDdO5y3gPxcnjTwxa628JtbzLwXts33ra8gYpPC3+64OD0IwRwRXw5+0h4P/AGidR+L/AMLb3QvF+ljTG8VSvpKHQpJm0tjp1wPNvJFuAJ0K7kAxH8zqc8YP034rJ+Fvim5+JdsAnhjV9g8RoF/495UAji1MY7IgEdzkf6sK+QIzn2uOaOaNZYnDxuAyspyGBGQQR1BHQ1nTlrZ7o7cww8VarS+CW3k+sflf5qz6mF4NtPFmneGrGz8c6pb6zrkSkXN3aWxs4JW3EgpAXkKADAxuPrXSF6iL0wtWp5pOWpm6oi9M30ATlvembqhL0zdQBOWpm6omem7qAJi1N3VXL0hegCYvTd1Qb6aWoA9P8EHNlcf9dR/6CK7euF8CHNjc/wDXX/2UV3VABRRRQB//1f3b8V8eHr0/7A/mK8MD817j4t/5Fy+/3B/6EK8E34NAFzfTg9VN4xSh+9AFwtxXkXw6b/iv/in/ANhqy/8ATXa16oWGK8j+HT/8V98Uv+w1Zf8AprtaAPaw9O3VT3CnhqAPz/8A2oPG3x6s/iF8O9F0fwZYTaFL4rSGxdtbeH+2c2FwTbXkAtysUR+Z8kyjKL8uTke/eB/CfxS8OeGbXVNEs7Lw3et5hn8KTXr3+lRBWYILO98pJbfcu0lRG0S/dCfxH23UtE0PWZ7C51ewgvZtKnF1aPNGsjW9wFZPNiLD5H2sy7hzgkdDWzuqXBNa79/uOvD42dJtR1i909U9/wDPR7rozzrTfizoiXcejeM7afwjqsjbEh1HasEzZIHkXSkwSbsZC7g+Oqg8V6ksgZQ6nKsMgjkEex71yvihox4b1N5NHPiERW8kg05REzXTKpIiQTFY9zngbyFz1IHNfml4I8e/Flfjx4u8LeD/AAP4t8LaXaWuiyR6PDPpdxbaZ9pMyyTTwzXDp5MoUFUt33DY3AJGYjzp23/r+ux0yWFqK6vB/fH/ADX/AJN6n6th6N9ePLq/xd0EbdU0Ww8UwoP9dpkxsrlv+3e5LR/lNUi/GLwtaOIPE9rqPhuUD5v7RspY4V/7eIw8B/B6PbR66EvKqr1p2l/hd/w3XzSPYQ9KGrkNE8aeEPEkQm8Pa3Y6mh6G3uY5P0Vs104L4yBxWiaexw1aM4Plmmn5lsNRvqqJPWl3j1pmZcD0u6qgenbqALQenB6qb6cGoAt7z2o3VW3Um6gDzDwy3/F5PHn/AF4aF/6Dc165vrxrwy3/ABePx3/14aF/6Dc162HoAtbjRuqtuFG6gC1vNG6q271o3CgCzuo3VW3VgeJfFfh/wdpEuu+JL1LCyiKpvc5LyOQqRxqMs8jsQqooLMSABmk2lqy6VOU5KEFdvojev9Rs9Msp9S1GdLW0tUaWWWVgkccaDLMzHgADqTXi2hW2o/FTxDZ+OtciktfCmlP5uh6fKpR7uYcDUbpDyAB/x7RH7oJkcbyojgtNB8RfFHUodd8eWr6X4WtHWXT9Ckx5t1Ip3Jdaljjg4MVsCVX78hZ9qx+4BgPastZ6vb8z1pSjhIuMGnUa1fSPku77vpstS1uo3VW3cUbhWx4xY3UFzVbfSb6ALG6k31W30bhQBY30heq+6mlqALG6vI/im3/Ew8Af9jRZ/wDpPcV6lvryb4pt/p/gD/sZ7P8A9J7igD2DfxSbqrb+KTeKAJZRHLG0Uqh0cFWVhkMCMEEdwa8BSeT4EN9muA0vw3dv3UvLNoBY/ck7mwz91v8Al36H91zH7uXqNwkiNHIodHBVlIyCDwQQeoNROF9Vud2Dxns7wmuaD3X6rs10f33TaJo545o0mhcSRyKGVlIKsp5BBHBB7Gnb68Mfwx4m+GUrXnw3h/tXw3hmm8PFgjwHru02VyFQdc28h8s8bGjwQ2pd/G34b2HhK+8Z3+qG2sdLlhgvY5InF1aTTusccc9vjzEYs6jkYwc5xzUqqtpaM0xGXtR9pRfNDv1XlJdH+D6NnrpfNN3VW80OAw6EZ/A0bhWp5pOXpm+oC1NL0ATl/Sk3cVW300sKAJ9/eml+Kr76az0AT7/Sml6g30zeKAPXPABzYXP/AF1/9lFd9Xnvw8OdPuv+u3/sor0KgAooooA//9b91/GBx4avz/sD/wBCFfPpccnivoDxoceF9QP/AEzH/oQr80P2wdX8SWvwWufD/g/VZtI1vxZqWmaFbTQAGXbqNykM4UnlT5LOdy4ZcZBHWgD6f0zW9J1iOSXSL2C+jhcxu0EqyhXHVWKk4I9K1N+a8v0Lwfp/w68DPoXguyhguLSzRFNtbRRtcTwQrEjtGvlxs5CKMEqMADIFcD4B1z41XniW0g8ZWVzDpTJIZWk06zt1DBcrmSK+ncc+iHPqOtAH0fv4xXknw8fHj74o/wDYasv/AE12tfIXj/8Aab8Wj49f8Kv8OaxHoqW3iTTPD6W7WnnGdJYBe3t1NIynaDGRDbomCH3O25T8v0x8JvEeleIvGfxcutBnF0tp4ghtWJDIBcQaZbK6HcAeG4JA+maAPofdSiTHFfMPgzX/AIy3Pxc8Q2OvaRpsWmJbaUZVj1O4lS3VxPl7dWtkV3cgbwSuMDk14D8Sf2nvFsPx4Pwv8MaxHoy2viLSfD6QNaecZ1uIReX11NIyttXySIbZEwfM3M2UOVAP0f34p4f3r5L+Pvx8tvC3wt8b33w+1BovFHh66sNMXzrSTbBd6nMkMEhWVAJYfnJLx7gdrBcspWp/2ZfjFrnxkj8c63fzltM0bW20iwiktzBOFs4UWeWbIGWln3sB/CuAQGyAAfV26ufsfCfhvTvE2qeMrGwjh1rWobeC8uhnfNFabvJVucYTe2MDua1g9OElAF0PXDfET4h2Pw40FNd1DSdV1mGSZYPI0iyk1CcFwcMYYgW2DHLYwO9ddup6yEdDikxpnwN+zH8T/Cvx/wDD8GmfETwXe6prjXmsP/ad7oH2eyEFteSJBF9qCKglSLahUHduVgeQa+tP+FO+B4mWTTFv9KK9BZald26j/gCy7f0rU8A+BfDnw08OL4U8KRSQ6clzdXYWWVpW829me4mO5jnmR2IHboOK7QSVMqUH0O2nmWIhpCo0vVnnR+HN/ECNP8b+IbQdh9phnx/3/hc/rWFY6XqM/iC+8KWPxX1G41jTIYbi5tGg0+SaGG4LCJ3X7OMK5Rtp74PpXq2qQXd5pt3aWF21hczxOkVwiq7QuwwsgVgVJU8gEYPevz68E/Bj46Wn7SfjO+uvipriwppOgs+qvpGnrHqipLclrQn7OIgIRnJi2yASfMfu1KpRvb9WaPNa3K72frGL/NH6CeH9N1XSbR4NX1qbW5Wfcs08UMLKuANoEKqpHfJGa3xJVMOfWlD1qlbQ4KlRzk5P/L8i7upQ9VN1G6gguB6dvqlvp4egDzDw03/F4vHX/Xhof/oNzXrW+vHPDTf8Xg8df9eOh/8AoNzXoGo+JNA0eZLbVtTtbKaRS6pPMkTMoOCQHIyAe9DZUIOTtFXOk30u/wB8V5nefFr4Y6f/AMfnizS4z/dF3Gzfgqkk/lXE+LviXZeKfD9/oXw71DWYtWuUxb3+laU1wYWBB3I13GLYkjj5yRz0rN1orqdsMqxMtVTdvRpfez2bSPFPh/XrzVdP0XUIb250O5+yX0cTbmtrgosnlSD+Ftjq2PQg962nmjijeaVgiIMszEBVA7kngCvyx+Bfwy/axg+JfjnWrTxtd6Hpr+I1uL9Nf0y0Z9VIsoU37LWNFI27V3wTKgKbcbg+fu8fCuy1qRbn4i6rdeLnB3C2udsOnKfQWcWI2A7GXzCPWkqraXKv0/4Jq8BCDftqiXkvef4e7+JNdfFJtemfS/hbp/8Awk9yhKSXu8w6VbkEg+ZdYPmsMH5IQ5zw23Oa+Fz+0x4O0H9oHw7/AMJlqOq+JVj0zVGvZn8Naigs7+3kijhGl2oidlhKtL5ky+aT8oaUAqD+msEUFtCltbRrDFEAqIgCqoHQADgCuE1L4d6LqnxM0P4qTz3C6roGnX2mQRKy+Q8N+8MkjOCu4uphXbhgME5B4wKn7yctfyFPMFGDp4ePKn13k/n09Elfrc7rStVtda0y01ey3/Z76JJ4/MRon2SKGG5HAZTg8ggEd60N1Vd+e9G8VszzCzuoL1V3+9JvpAWt/vTd9Vt9JuoAtbqTdVbfSF6ALO+m76rF6TdQBa3V5J8Un/4mHgH/ALGe0/8ASe4r1EvXknxRb/iYeAuf+ZmtP/RFxQB7AGoL4qrvwKaXoAs7qQvVbfTTJmgCxur4f/a8/Z1vvifo91418K3esXHia3jsbWDTrG4jitpYRdI0ruj7Q7KmX+ZzgqCozwftbdTS+eKipTUlZm+GxM6Uuem7P+vw8j508CaF8Ufg1Z3thr9/qfxO0u6uPPS9kmjbU7RCqr5P2YiNJI0wTmNtxJ+6a9e8O/EXwd4qlaz0fU4zfRgGSzmBt7uPPZ7eULIv5V1e6uZ8R+E/C3i6JYfEulW2pCM7kM8YZ0YdCj/eUjsQQaTUr6P7zsWIoVFarCz7x/WO33OJ1hbBweKaz+9fMnxDuNA+CmjQa83jTW9CsZpvIihNvNr0JlZSwUxeXNOq4HUOqj16V41+z1+1R4v+MWl2dnZ/2ZrPiCZbuaSBrTUdKQQ203lgi4eGa3ZipU7Q+eSMZVsJVHeziKWBpv8Ah1V6O8X+Pu/+TH39uphfNeUSeOPHFlhb7wBfzHHzNY3lnOn4eZJC36VXufisNNtZL3WvCut6dBFjfJLBCUXJwMssxHXjrT9tH+ri/sqturP0lF/kz10vTC1ZllfC/s4L0RSQCdFfy5l2SJuGcOuTg+1Wd+Oa0PPkmnZk5ek31W300v2oEe0fDht2nXf/AF2/9lFejV5p8MznTbz/AK7j/wBAFel0AFFFFAH/1/3T8bHHhXUT/wBMx/6EK+Jdb+Gfh7xH410nxvrk13eT6FIJ7G0ecmyguVjkiFwsOMebsldckkc5xkCvtfx1x4S1I/8ATMf+hCvmPfmgC6HyKUNg5qkHxT99AFjyrfzPP8pPMzu3bRuyBjOcZzjjNeD/AAX8PQeGPGnxdt4biS5N/wCJor52kCgh7jTbUlRtAG1cYHf1NZmkfEXxlN+0xrnwr1Ga1fQLPw5a6vbLFAyTiS5uJYiJZGdt2BFxhVHPtXTeAr21h8e/E5ZZo42/tiy4Z1U/8gy27EigD28MAcgcnrTPKtjJ5xiQvkNu2jdkcZzjOccZrN/tKwIz9qh/7+p/jSjUrEf8vUP/AH9T/GgDnPC/gt/D+seIdYvtZutbfxDdJcmO7jgCWojUIkUPlxqxRQBt3liOcHmu8RYogREqoGJY7QBknqTjufWsn+0rH/n6h/7+J/jTxqdh/wA/UP8A39T/ABoA2BJ2p2+sf+0rD/n6h/7+p/jThqdgR/x9Q/8Af1P8aANgP607fiscanYD/l6h/wC/qf40/wDtKw/5+of+/qf40AbAenBzWN/adgP+XqH/AL+p/jTxqVgf+XqH/v6n+NAGwHBrD8T+KtC8GeHtQ8U+Jr2PT9K0qCS4uJ5WASOONSzHnqcDgDk1L/aVj/z9Q/8Af1P8a+O/2yb/AEHxx4D0j4JWs9tf6p441/SdPa3WSJ5IbVZftVxMcn92Ft4ZCH6/3fmxQB9KfC/4kT/EvRG8RDw5qGgWE4hlsnv/ACQby2uIxKkyLE7lBg4ZJArq2QRXpm81x0NtoOn6EPD2jvaWdnDbm2hiRkWKNAu1QEVl+X2GK+UNG+HGh+CPHfhi01vxb4Wg1K5lNxaWq2lzBcXS2uPN8gyahIoZdy9VbGehoA+2bi+tLNVe7njgVjgGR1QE+gLEZqykodQyEMrDIIOQQfSvzT/an+Evx1+J/j3X4/DnhiLX/Dl3pGm6VYyzX8VutrFdXTPqzxRNuzcPGkaeayjYmTGSdyH2X4xTfFPxb8PPHPwo+F9kumavpEWi28DWV2Enk0+6Cm7WOSRUWKURI6IQfRgyk8AHsPxg+NGnfCvw7b63BZNrs93ctaxwwSfKJFjaQiSRFk2EhdqgjliAcDmqPjz486N4O0T+0bKyk1S9h1G3064syJIJIjMYw84LRndBD5yPLIBtEeWBJG01fgJ8NbH4b+G9SMWlTaZea/e/brpLmSB5iyxRwx70th5EWyONUCRlhgAszMWNet6PoGk6E9/JpkTI2p3D3dwXkeUvM4AZhvLbRgD5VwB2FAHEeEpppfit40kmVY5X03QiwVt6hilySA2BkA9DgZ9BXxn408C/tG6z+0/4Xg1DxL4Zvbr/AIRzWHtbifw9NNZQW4urfMUsTXRDztkFJA64Cv8AKd3H2FoF5awfGDxws08cbGw0Q4d1U423PPJFepf2jYdftUP/AH9T/Gk4ptXNIVJR+F2ZBomi2umafbRS2tmt4kaCaS2t0hjeQD5mVQCVUnkDJwO9dB5jEbSTj3rI/tOw/wCfqH/v6n+NL/adh/z9Q/8Af1P8aZMpt7u5q7+2envRvrK/tKw/5+of+/qf40v9pWH/AD9Q/wDf1P8AGgk1N9LvrJ/tOwP/AC9Q/wDf1P8AGj+07D/n6h/7+p/jQBrb8Um+sr+0rD/n6h/7+p/jS/2lYf8AP1D/AN/U/wAaANTfRvrK/tOw/wCfqH/v6n+NH9p2H/P1D/39T/GgDV3Um+sn+07D/n6h/wC/qf40n9pWH/P1D/39T/GgDW8zPek31xes+O/Bvh6702w13XLKwuNYma3s1mnRDPMqlzGhJwW2gnHoK6gSBgGU5B6Y5FAFvd60heqpkqq88zzJZ2cfnXMgJC5wFUdWY9gKANLfzXk/xQb/AE/wF/2M1p/6IuK9FvNF8QQRi4lulVO4RcAfnzXj3xKupbW+8CG/lURDxLaZckLj9xcevFAHuHmcUm+s2S/sYiAbqEhgCCJUwQe/Wojqdj/z9Q/9/U/xoA1i/YUzfWX/AGlYf8/UP/f1P8aadUsMf8fUP/f1P8aANUyUzf3rL/tOw/5+of8Av6n+NNOp2A/5eof+/if40AahfNN3+9Zf9pWH/P1D/wB/U/xpp1Ox/wCfqH/v6n+NAGjIRICp5BBHr14rgfhl8P8ASfhX4Ns/BOiXE13aWUlxIstxs80m5meZgSgUYDOQOOmM11f9pWH/AD9Q/wDf1P8AGmHU7H/n6h/7+p/jQBpl6+IP23NE+LeqfCjxI/hjWNMTwwbS2W402XS5bq+mm+0Llo5klChfu/L5TEYPPPH2UdSsP+fqH/v6n+NRnUbAjm5hP/bRD/Wk0M434aWPxM03RpIvifrem63esyNbyabYyWEccOwDY6STTFmzk7gQMcYr0UydqyzqdgB/x9Q/9/U/xqjqPiLRNJsLnVdSv4Le0s43mmkaVdqRoNzMeegAyapu5KR0BfvUZf3rA0PxHofijSrbXPDeoW+qadeIskNxbSLLFIjDIZWUkEEVql+KQz3X4XnOmXn/AF3H/oAr0+vK/hSc6Ven/puP/QBXqlABRRRQB//Q/c3x6ceENTP/AEzH/oQr8+fHnx18B/DbWbvRvFM1xBJY6Rca5PIkJkijs7d1RiSDkuzMFRAMselfoJ8QTjwbqh/6ZD/0IV+WPxW/Zb8I/FzxTqHirW9e1Wxn1CHTLdorSSJYlTS52uIsB0bO53JIbIB5AyAaAPfdb8feH/Dul2Or6v8AaooNRCmIRWc9zINyb8OkCOycddwAzx1q14b8a6F4qsLjU9IkmFvauUka5tprQqQu4nbcIhK4/iAx71R13wloPiaytbHXI5riKzOY9txNC2du3LNC6Fsj14zzU/h7wtoXhazmsdGheOC4fe6yzS3GTjb1mZyBjsOKAPOLfwt4Ft/jBc/GceMg2p3emppT2puLUWn2SJ2lQYA37ldmIbdnnHSu91v4ZfDHxdqJ17xH4V0rWL6ZEU3NzaRTSOiZ2DewJIGTj61Tu/FPwy0+4uLO/wBS0i3ns57e2mjkeBGjnum2QRODyHlY4RTyx6V3qkIAqgKBwAOAKAPOP+FG/BQf8yHof/gvh/8Aiak/4Ub8FOv/AAgWh/8AgBD/APE16OHpQ+KAPOB8Dfgn/wBCHof/AIL4f/iad/woz4KdvAWh/wDgBD/8TXpAenCQUAebj4G/BP8A6ELQ/wDwXw//ABNO/wCFF/BTOf8AhAtD/wDBfD/8TXpO+nCSgDzcfAv4JHr4C0P/AMF8P/xNKPgX8Eh18BaH/wCC+H/4mvSRJiniTNAHm3/Ci/gj/wBCDof/AIL4f/iaX/hRfwS/6EHQv/BfB/8AE16SH96cJM0Aeaj4FfBA/wDMg6F/4L4f/iajX4BfAtJmuV+HugLK4wzjTYNxHoW25NeoB/eviPxt8XPEfw9/aK8fzPcz6h4f8N/D1fEP9lNMEha4hnl8xkJB2M0cYA7Z69c0AfSg+BPwQ7+ANC/8F8P/AMTXNap+y7+z9rGv6R4jvPA2mC50QyNbxxQCK3LuVO+WFMJIybRsLg7OcYya5n4J/tB3/wAVvEt94c1Xw4ugyJoul69aFbsXRlstUDbBLhFCSqyHKqWGMHdngcn4/wDEvxA+IX7RVr8CtDubrQfC2kaL/bGr6hpt4IL2V7wyQ2qBjGxQROjPsBBc7STsBVwD7QUqqhVGABgDtgVnWui6NZanea1Z2MMOoaiEFzcIgEswjGEEjDlto4Geleb+PPiRY/DW10xL+Fbz7UGQPLfWlmxMSjJJupIw5PU7c471U8PfEW1+IfhLxBe6dH9i+xRSx7oL+2umDGEuGElpI4QjjGSD3HFAHtO80CQ1+b3w0/ac8daJ8JPhLp9xo58ZeJfGmg6pqRu7u+FqofS/nk89vLc4YMApRWOe2OR73J+05obeBfh/4psNJkuNS+Idk1/a2DTJGLa1gtzcXM9zMchIYQAhYKcu6Lj5sgA9w8SfDb4c+Mr5NU8XeF9N1q8jjESzXlrHNIIwSQoZgTgEk4rn/wDhRPwQ/wChA0L/AMF8P/xNWPhP8QD8Ufhz4f8AiJ/Zsmjx+IbVLyG2lcSSLDLzGSy4HzLhuOma4v4n/HLSvh9relaD9j1Ka4nv7OK4eHSby6gNtcBywimijMbS/L9wEsO68igDrT8CPgif+ZA0L/wXwf8AxNIPgR8EP+hA0L/wXw//ABNZfi/406J4Z8J6B4ktLC8v7rxZd29jpWnyQvZ3U89xk4ljuArwrFGrSy713Kin5ScCrfwb+KafF3wb/wAJnDpx0y1kvby0t8zLMtwlpK0JnjcBcxyMpKccrg96ALX/AAoj4H/9CBoX/gvh/wDiaP8AhRHwP/6EDQv/AAXw/wDxNcP8RvjtqngP4laP8L7Pwhc63qfijT7q60V4bmOKO7uLPb51vIZABDtVg3mHIxwAWwD7/aTzzWsMt1F9nmdFaSPcH2ORll3Dg4PGe9AHnP8Awoj4Hn/mQNC/8F8P/wATSf8ACiPgd/0IGhf+C+H/AOJr0/fSb6APMv8AhRHwP/6EDQv/AAXw/wDxNJ/woj4H/wDQgaF/4L4f/ia9O30u+gDzD/hRHwP/AOhA0L/wXw//ABNJ/wAKI+CH/QgaF/4L4P8A4mvT99JvoA8x/wCFEfA//oQNC/8ABfD/APE01/gX8DY0Lv4C0FVXkk6fDj/0GvTy9c/4i1OPTbWC5lmW3UTx/vH4jVgcrvJ4Ck8ZPSgDyHXf2Zf2dfF97p66r4C0wNpMpuEgS1FvHKxQr++jUKJkAOQr5XPOMgV9BQJDbQx21sixQwqERFGFVVGAAOwArOvtSXS8+MfFsq2FhZDzXmkZQH4wscQBJdnPCgZzmotJv5L/AEy2vpozC9wgkKHqu7nH4UAbRkwCSam8F3cEt1e3FwQryPwT/dThR9B1+tZFw7GCQL1KnH5Vyvh/U42tkl3YWQkZ9HQ7XU+4NAHtOt6lFJH9mibOetea+JfDPhvxbpb6L4p0y21jT3ZHa3u4lmiLxncjbXBGVIyD2NaS3URXIcfnVK71KONSEO5vagDzSf4JfA6HRpbh/AGhf6JcRKT/AGfB92Y7cfd9cU7/AIUT8Ecf8iDoX/gvh/8Aia0PiBrI0Pw5ovh4v/xNPFWoxSJH/GLS0O+SQj+6W2rn1Nd0khCKD1wKAPNP+FFfBH/oQdC/8F8P/wATSf8ACi/gl/0IOh/+C+H/AOJr03fTDJQB5mfgZ8Ef+hB0L/wXw/8AxNRSfA/4HxIXl8CaCijubCAD9Vr0O/v47C0kupOduAB6sTgD8TSxRLGifalWe6cBmLDKx56BR0oA8rHwj/Z/dtieDfDxb/rxg/8AiavD4HfBJl3L4D0Ig9CLCHH/AKDXozafayA74lOfYVlyWz6c3nWBwveMn5W/wNAHGH4GfBPt4C0P/wAF8P8A8TTf+FGfBMc/8IFof/gvh/8Aia9HhuUnhSeM5RxkZ6+hB9waeZM0AeZn4G/BTP8AyIWh/wDgBD/8TTT8DfgoP+ZD0P8A8AIf/ia9M34pm/vmgDzP/hRvwU/6EPQ//ACH/wCJqhqf7P8A8ENT0+602XwNpESXUbxM8NnFFKocYJR1XKsOxHIPNesmTFN3+poA4z4dfDXwJ8JfDcPhP4e6NBoumRYJjhB3SOFC+ZK7ZaSQgDLsST3NdyZOearGSm78d6APoP4SnOk33/XwP/QBXrNeQ/B87tIvz/08D/0AV69QAUUUUAf/0f3I+Ihx4L1U/wDTIf8AoQr5H38mvrX4jnHgjVz/ANMh/wChCvj3zOaAL/me9OD1niTFPEmaAPliz+D/AMR7j482Xxq1u38PkXdg+lahZr50ohht52ktLqBmQeZcMhKvuChMgKzBTu+uRJWeJKfvoAviT3p4fNZ4k7U7figDQ8zFPD1n+ZmnB8UAXxJT99Z/mA04SYoA0BJ2p2+qHmU4SUAXxJTt5qhvpRJigDQEleNeJvgV4A8XeJ9d8X6yL19R8SaM/h+8Md3JHG2myHJhVF+VTkkhx843HB5r1jfxSiQ0AeSeEPgr4U+Gl7ceI/BEdzLq66PBo8C3t7JJC1rYg/ZYWJDEBCT8+C2CScmqnwk+HniXQ/E/jD4nePjap4m8aS2ge2spXuLexs7KERxW6TOkZk+cySFti/fxjjJ9oElLvI70ASXVlYX237faw3OzO3zY1kxnrjcDiqk+jaZLpt3pVvCtjDextHIbVVhfDrtJBUcNg8GrPm0ofPegDwaz/Zh+Fun2PhnT7AalBD4Ps73T9M2X0m6C21Hi4jyclgwAA3ZKgDGMVU139lH4N+JPDvh3wrqtpfvp/hbTbrSLJY9QuIWFjdhRJBK0bKZU+VcB89B1r6F8wjvS+bQB4/4d+HM3grx54aXwxaMnhXRfDtxpJeW8eWYSedC8AKSbjJ8iuDIWyCQMEE49c1TStP1r7CdQRn/s66jvIMMV2zxZ2Nx1xuPHSpvNpd+e9AHBfEv4T+C/i1BpNt4ziuJV0S6a8tWtrqW1kWVonhbLwsrEFHYEZ5zXkOqfAfTPCfhvwD4D+Gnh4XejeGdXivzcXWqyx3NlHE/mnyZHDs/mNhHT5V8ncg4OD9N+YRS+ZQB4drH7NXwu1/XbbxPq66pd6xY6pLqtteyapdm4t5J8iSCCTzN0Vq4ODboRGRxivflYKoVeABgD6VT82l8ygC5vNLvxyKpeZ70eYPWgC5vNHmVT8yk8ygC7vNHme9UvNpPMoAu7/eqd/aWmp2kthfxLNBMpV0YZBB7U3fSb6APLNP8Agl4DsNTi1NbeWc2zb4YppXkiiPbYjEhcewFevAhQFXgDsKpmSk800AXS9eF+K73X/h3q0/iHTNPbWtCviGvbJW2yK4486BjwHA6g8N3x1r2bf71FKscyGOVQ6nqD0oA8m0v41fBHVIRJJ40g0KT+K21aN7SZCOo+YbWx6qSKoa5+0d8E/DEf/FO30vj/AFc/6m106NltS/bzblgFC56hctjoDW3rnwj8C69KZ7zT03nkkDGam0H4V+CfD0gmsdPQSL0JGaAOH+HWleM/G3iq5+K3xJZf7RulEdtboNsNpbr9yGJT0VfXqTkn2+it+KoqUjUJGNoHQCjfQBcMlJv96p78U0yZoAxfFc+zTUfsk0bH3ANbVlepdATq27eBWVrNiuq6bPYFzGZF+Vh1VhyCPoa8a0rxqNG1L/hHPEzrpmoqdqeafLguPRoZD8oJ7oxBB6ZFAH0b5me9Zl9cxxRkseewrmY9SvZEBSOZwe6IXB+hXIrl/E3ibSvDlm2oeJ7+PSrcdDKczSH+7FCMu7HsMY9TQB0+mamsc8Nix+a6luXQf7MYTJ+m411BfjNeHfDu61PxXq9x43vbV9P08RC10y2k5dIAdzPIRwXkb5mxx0A6V7GXoAt+ZSGT3qoZKZ5lAFveaY0nvVQyZ70hfAoAt7+9Rlzmqu/PemmUCgD6T+DRzo1//wBfP/si17FXi/wUbdomoH/p5H/ota9ooAKKKKAP/9L90fHGm3ur+FNS03To/NuZ4wqLkLk7gep4r5n/AOFW+O88aaP+/wBH/jX2HRgUAfHo+F/jv/oHD/v9H/jTh8L/AB3/ANA4f9/o/wDGvsDAowKAPkH/AIVf45/6Bw/7/R/40o+GPjr/AKBw/wC/sf8AjX17gUYFAHyJ/wAKx8c/9A4f9/o/8acPhl45H/MOH/f6P/GvrnAowKAPkf8A4Vl44/6B4/7/AEf+NOHwz8b/APQPH/f2P/GvrbAowKAPkr/hWnjcf8w8f9/Y/wD4qn/8K18bf9A8f9/U/wAa+ssCjAoA+Tf+Fa+Nv+geP+/qf40//hW3jX/oHj/v6n+NfV+BRgUAfKI+G/jYf8w8f9/U/wAad/wrjxr/ANA8f9/U/wAa+rMCjAoA+U/+FceNf+geP+/qf404fDnxp308f9/U/wAa+qsCjAoA+Vv+Fc+NP+fAf9/U/wAacPh14072A/7+p/jX1PgUYFAHyz/wrrxn/wA+A/7+p/jSj4deM/8AnwH/AH9T/GvqXAowKAPlv/hXfjPvYD/v6n+NL/wrvxl/z4D/AL+p/jX1HgUYFAHy5/wrvxl/z4D/AL+p/jS/8K78Zf8APiP+/qf419RYFGBQB8uj4d+Mu9iP+/qf40v/AAr3xl/z4D/v6n+NfUOBRgUAfL3/AAr3xj/z4D/v6n+NL/wr3xh/z4/+RU/xr6gwKMCgD5f/AOFeeMf+fEf9/U/xo/4V74x/58B/39T/ABr6gwKMCgD5f/4V54x/58B/39T/ABo/4V74w/58B/39T/GvqDAowKAPl7/hXnjH/nwH/f1P8aX/AIV54x/58B/39T/GvqDAowKAPl3/AIV54z/58R/39T/Gk/4V34y/58B/39T/ABr6jwKMCgD5d/4V14y7WA/7+p/jSf8ACu/GX/PgP+/qf419R4FGBQB8tn4d+Mj/AMuA/wC/qf403/hXXjP/AJ8B/wB/U/xr6mwKMCgD5Z/4V14yH/LgP+/qf403/hXXjT/nwH/f1P8AGvqjAowKAPlb/hXPjT/nwH/f1P8AGkPw58adtPH/AH9T/GvqrAowKAPlP/hXHjT/AKB4/wC/qf40f8K48a4/5B4/7+x/419WYFGBQB8of8K28an/AJh4/wC/qf41z/iH4Haz4os2sdc0OO6iYY+aSMkfQ5r7PwKMCgD8zZ/2M9bjJXRbnUNMhP8AyygvyiD6BXArR0D9jr+xb0andaY+p3Y58y5uEkJ+pLZP4mv0hwKMCgD5Hi+F/jOGNYotNVEUYAEsYAH504/DPxx/0Dx/3+j/AMa+tsCjAoA+R/8AhWXjf/oHD/v9H/jTT8MvHP8A0Dh/3+j/AMa+usCjAoA+RP8AhWPjn/oHD/v7H/jTP+FYeOj/AMw4f9/Y/wDGvr7AowKAPkE/DDxzj/kHDP8A12j/AMaZ/wAKu8dn/mHD/v8AR/419g4FGBQB5b8LPDus+HNLvbbWYPIklnDqNytldgGcqT3FepUUUAFFFFAH/9k=)

#### 最小二乘法与正态分布

高斯换了一个思考框架，通过概率统计那一套来思考，进而证明了最小二乘法。

计算测量值与真值的误差，这些误差最终会形成一个概率分布，只是现在不知道误差的概率分布是什么。假设概率密度函数，可以求出所有测量值同时出现的联合概率密度函数。根据极大似然估计的思想，概率最大的最应该出现在极值点，即导数为零。如果最小二乘法是对的，那么![x=\overline{x}](data:image/svg+xml;utf8,%3Csvg%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20class%3D%22mjx-svg-math%22%20width%3D%225.873ex%22%20height%3D%222.343ex%22%20style%3D%22font-size%3A14px%3Bvertical-align%3A%20-0.338ex%3B%22%20viewBox%3D%220%20-863.1%202528.6%201008.6%22%20role%3D%22img%22%20focusable%3D%22false%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-labelledby%3D%22MathJax-SVG-1-Title%22%3E%0A%3Ctitle%20id%3D%22MathJax-SVG-1-Title%22%3Ex%3D%5Coverline%7Bx%7D%3C%2Ftitle%3E%0A%3Cdefs%20aria-hidden%3D%22true%22%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMATHI-78%22%20d%3D%22M52%20289Q59%20331%20106%20386T222%20442Q257%20442%20286%20424T329%20379Q371%20442%20430%20442Q467%20442%20494%20420T522%20361Q522%20332%20508%20314T481%20292T458%20288Q439%20288%20427%20299T415%20328Q415%20374%20465%20391Q454%20404%20425%20404Q412%20404%20406%20402Q368%20386%20350%20336Q290%20115%20290%2078Q290%2050%20306%2038T341%2026Q378%2026%20414%2059T463%20140Q466%20150%20469%20151T485%20153H489Q504%20153%20504%20145Q504%20144%20502%20134Q486%2077%20440%2033T333%20-11Q263%20-11%20227%2052Q186%20-10%20133%20-10H127Q78%20-10%2057%2016T35%2071Q35%20103%2054%20123T99%20143Q142%20143%20142%20101Q142%2081%20130%2066T107%2046T94%2041L91%2040Q91%2039%2097%2036T113%2029T132%2026Q168%2026%20194%2071Q203%2087%20217%20139T245%20247T261%20313Q266%20340%20266%20352Q266%20380%20251%20392T217%20404Q177%20404%20142%20372T93%20290Q91%20281%2088%20280T72%20278H58Q52%20284%2052%20289Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMAIN-3D%22%20d%3D%22M56%20347Q56%20360%2070%20367H707Q722%20359%20722%20347Q722%20336%20708%20328L390%20327H72Q56%20332%2056%20347ZM56%20153Q56%20168%2072%20173H708Q722%20163%20722%20153Q722%20140%20707%20133H70Q56%20140%2056%20153Z%22%3E%3C%2Fpath%3E%0A%3Cpath%20stroke-width%3D%221%22%20id%3D%22E1-MJMAIN-AF%22%20d%3D%22M69%20544V590H430V544H69Z%22%3E%3C%2Fpath%3E%0A%3C%2Fdefs%3E%0A%3Cg%20stroke%3D%22currentColor%22%20fill%3D%22currentColor%22%20stroke-width%3D%220%22%20transform%3D%22matrix(1%200%200%20-1%200%200)%22%20aria-hidden%3D%22true%22%3E%0A%3Cg%20class%3D%22mjx-svg-mrow%22%3E%0A%3Cg%20class%3D%22mjx-svg-mi%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-78%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3Cg%20class%3D%22mjx-svg-mo%22%20transform%3D%22translate(850%2C0)%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMAIN-3D%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3Cg%20class%3D%22mjx-svg-munderover%22%20transform%3D%22translate(1906%2C0)%22%3E%0A%3Cg%20class%3D%22mjx-svg-mi%22%20transform%3D%22translate(24%2C0)%22%3E%0A%20%3Cuse%20xlink%3Ahref%3D%22%23E1-MJMATHI-78%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3Cg%20class%3D%22mjx-svg-mo%22%20transform%3D%22translate(0%2C259)%22%3E%0A%20%3Cuse%20transform%3D%22scale(0.707)%22%20xlink%3Ahref%3D%22%23E1-MJMAIN-AF%22%20x%3D%22-70%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%3Cg%20transform%3D%22translate(243.33524654895245%2C0)%20scale(0.2427625054254761%2C1)%22%3E%0A%20%3Cuse%20transform%3D%22scale(0.707)%22%20xlink%3Ahref%3D%22%23E1-MJMAIN-AF%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%20%3Cuse%20transform%3D%22scale(0.707)%22%20xlink%3Ahref%3D%22%23E1-MJMAIN-AF%22%20x%3D%22379%22%20y%3D%220%22%3E%3C%2Fuse%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E)时应该取得最大值，求解微分方程可得概率密度函数服从正态分布。

也就是说，**如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值**。那么误差的分布是正态分布吗？我们相信，误差是由于随机的、无数的、独立的、多个因素造成的，那么根据中心极限定理（参考“[为什么正态分布如此常见？](https://www.matongxue.com/madocs/589.html)”），误差的分布就应该是正态分布。



#### 常见分布

概率分布（probability distribution）或简称分布，是指随机变量的概率分布函数。

离散型随机变量分布：

- 两点分布 / 伯努利分布：二项分布在n=1时的特例。一次随机试验，成功概率为p，失败概率为q=1-p。任何一个只有两种结果的随机现象都服从0-1分布；

- 二项分布（Binomial distribution）：是n重伯努利试验成功次数的离散概率分布。二项分布的典型例子是扔硬币，硬币正面朝上概率为p, 重复扔n次硬币，k次为正面的概率即为一个二项分布概率；

- 超几何分布：对N件产品（其中M件次品）进行不放回抽样，在n次抽样种抽到次品数X，服从超几何分布；

- 泊松分布（Poisson distribution）：某一区间内发生随机事件次数的概率分布。如馒头店问题

  应该准备多少个馒头才能既不浪费又能充分供应？需要比服从最小二乘法的均值更合理。

  计算在 ![T](https://www.zhihu.com/equation?tex=T) 时间内卖出 ![k](https://www.zhihu.com/equation?tex=k) 个馒头的概率时求取T时间细分为二项分布的概率密度函数的极限值，可得每日卖出的馒头数![X](https://www.zhihu.com/equation?tex=X) 服从泊松分布。这样如果每天准备8个馒头的话，那么足够卖的概率就是把前8个的概率加起来，这样 ![93\%](https://www.zhihu.com/equation?tex=93%5C%25) 的情况够用；

![img](https://img-blog.csdn.net/20180720091309679?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjbnRfMjAxMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- 几何分布：几何分布由n次伯努利分布构成，用于计算第一次成功所进行试验的次数。

连续型随机变量分布：

- 均匀分布（Uniform Distribution）

- 指数分布：两次随机事件发生时间间隔的概率分布。如馒头店问题

  每日卖出馒头的数目![X](https://www.zhihu.com/equation?tex=X) 服从泊松分布，卖出馒头的时间间隔![Y](https://www.zhihu.com/equation?tex=Y) 服从指数分布。

  泊松分布只告诉了我们每天卖出的馒头数，对泊松分布对概率密度函数进行扩展后得到的函数称为 泊松过程 ，这样可知不同的时间段内卖出的馒头数的分布。两次卖出馒头之间的时间间隔大于![t](https://www.zhihu.com/equation?tex=t) 的概率，等同于![t](https://www.zhihu.com/equation?tex=t) 时间内没有卖出一个馒头的概率，而后者的概率可以由泊松过程给出。求出两次卖出馒头之间的时间间隔![Y](https://www.zhihu.com/equation?tex=Y)的累积分布函数就是指数分布；

![img](https://img-blog.csdnimg.cn/20190506102904416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjbnRfMjAxMg==,size_16,color_FFFFFF,t_70)

- 正态分布（Normal Distribution 高斯分布）：反映误差的最重要的分布，确定了均值和标准差就能确定一种正态分布。



#### 中心极限定理

大量相互独立的随机变量，其均值（或者和）的分布以正态分布为极限（意思就是当满足某些条件的时候，比如Sample Size比较大，采样次数区域无穷大的时候，就越接近正态分布）。无论是什么分布的随机变量，都满足这个定理。

在适当的条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布，其中有三个要素：

- 独立
- 随机
- 相加

每次采样受到各种随机性的支配，就好像钉板中的钉子，对采样结果进行或者正面、或者负面的影响，最终让结果形成了正态分布。





#### 条件概率

条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P（A|B），读作“在B条件下A的概率”。条件概率可以用决策树进行计算。

条件概率的谬论是假设 P(A|B) 大致等于 P(B|A)。比如在疾病检测中，P*(*A|B) 与 *P*(*B*|A)的差距可能令人惊讶，同时也相当明显。可以看出P(positive|disease)=99%与P(disease|positive)=50%的差距：前者是你得了病，而被检出为阳性的条件概率；后者是你被检出为阳性，而你实际上真得了病的条件概率。由我们在本例中所选的数字，最终结果可能令人难以接受：被测定为阳性者，其中的半数实际上是假阳性。

需要注意的是，在这些定义中A与B之间不一定有因果或者时间顺序关系。A可能会先于B发生，也可能相反，也可能二者同时发生。A可能会导致B的发生，也可能相反，也可能二者之间根本就没有因果关系。

基本上很多机器学习的算法的目标函数都是条件概率分布。条件概率分布的数学表达公式为：P(Y|X)。这里X 和 Y 都是随机变量。 如何理解该公式呢？ 该公式可以理解为：寻找一种观察结果的最可能原因。说白了， 就是从结果反推出原因。 

那么哪个是原因？ 哪个是结果？在条件概率分布中，  X代表结果， Y代表原因， 这一点非常重要。由于结果是可观察的， 所以X是已知变量， Y是要求解的未知变量。所以条件概率分布可以理解在不确定环境下的因果推导模型。 比较常见的例子就是医疗诊断，医生会根据患者的症状（观察的结果），推断出具体病因（原因）。



#### 贝叶斯定理（分类器）

[手写贝叶斯定理](https://blog.csdn.net/jiangjiang_jian/article/details/81346797)

贝叶斯定理是关于随机事件 A 和 B 的条件概率：

![1.png](http://www.zhoulujun.cn/zhoulujun/uploadfile/images/2017/0914/20170914143111653306243.png)

其中：

- P(A)是 A 的**先验概率**，之所以称为“先验”是因为它不考虑任何 B 方面的因素。**先验概率与条件概率无关。**
- P(A|B)是已知 B 发生后 A 的条件概率，也由于得自 B 的取值而被称作 A 的**后验概率**。后验概率都是条件概率。
- P(B|A)是已知 A 发生后 B 的条件概率，也由于得自 A 的取值而被称作 B 的后验概率。
- P(B)是 B 的先验概率，也作标淮化常量（normalizing constant）。根据条件概率公式可知：

![img](http://www.zhoulujun.cn/zhoulujun/uploadfile/images/2017/0914/20170914143129866153424.png)

按这些术语，贝叶斯定理可表述为：后验概率 = (相似度 * 先验概率) / 标淮化常量

也就是说，后验概率与先验概率和相似度的乘积成正比。

另外，比例P(B|A)/P(B)也有时被称作标淮相似度（standardised likelihood），Bayes定理可表述为：后验概率 = 标淮相似度 * 先验概率

从条件概率的定义以及概率乘法定理可以推导出贝叶斯定理，如下公式所示。概率乘法定理(multiplication theorem of probability)，亦称概率乘法规则，即两事件积的概率，概率论的重要定理之一，等于其中一事件的概率与另一事件在前一事件已发生时的条件概率的乘积。

![è´å¶æ¯å®ç](http://upload.wikimedia.org/math/e/f/a/efaf8fda8a92eeb2d8cf70468c20ed5a.png)

贝叶斯定理用于计算逆向概率问题。这个定理需要大规模的数据计算推理才能凸显效果，它在很多计算机应用领域中都大有作为，如自然语言处理，机器学习，推荐系统，图像识别，博弈论等等。

通常，事件 A 在事件 B 发生的条件下的概率，与事件 B 在事件 A 发生的条件下的概率是不一样的；然而，这两者是有确定关系的，贝叶斯定理就是这种关系的陈述。

贝叶斯公式的用途在于通过己知三个概率来推测第四个概率。它的内容是：在 B 出现的前提下，A 出现的概率等于 A 出现的前提下 B 出现的概率乘以 A 出现的概率再除以 B 出现的概率。通过联系 A 与 B，计算从一个事件发生的情况下另一事件发生的概率，即从结果上溯到源头（也即逆向概率）。

因此贝叶斯公式实际上阐述了这么一个事情：

新信息出现后的A概率 = A概率 * 新信息带来的调整

先验概率：是指根据以往经验和分析得到的概率；

后验概率：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小；

类条件概率：类条件概率密度函数 ![img](https://gss1.bdstatic.com/9vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D64/sign=6dccedceafc27d1ea12638c01ad5787c/622762d0f703918f191f62a05a3d269758eec48a.jpg) 是指在已知某类别的特征空间中，出现特征值X的概率密度。

也就是说，贝叶斯分类器依据类条件概率密度 ![img](https://gss1.bdstatic.com/9vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D59/sign=785220646c09c93d03f20efe9e3d6cd5/060828381f30e92409dcf65b47086e061d95f764.jpg) 和先验概率![img](https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D39/sign=37e82daa89025aafd73278c2faed5a0f/1b4c510fd9f9d72a86a27b77df2a2834359bbb92.jpg) 来判别样本工的类别属性，因此在构建分类器时需要估计出每个类别的先验概率，并且确定类条件概率密度。



贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。

这时贝叶斯公式可以转换为P(类别|特征)=P(特征|类别)P(类别)/P(特征)。而我们最后要求解的就是P(类别|特征)。



#### 极大似然估计

原理如图所示：

![img](https://img-blog.csdn.net/20170528002827749)



<https://blog.csdn.net/baidu_15238925/article/details/81291281>

<https://blog.csdn.net/u012284960/article/details/72859171>

<https://blog.csdn.net/expleeve/article/details/50466602>

<https://blog.csdn.net/appleyuchi/article/details/80930416>

<https://blog.csdn.net/u012421852/article/details/79614417>

<https://blog.csdn.net/u010692239/article/details/52345754?utm_source=app>

<https://blog.csdn.net/jk123vip/article/details/80591619>

<https://blog.csdn.net/jiangjiang_jian/article/details/81346797>

<https://blog.csdn.net/jiangjiang_jian/article/details/81346797>

<https://mp.weixin.qq.com/s/83RoweQ3xn4ATzLZ8SPe1g>



#### 假设检验

其中假设检验部分为核心，其他辅助更好的理解该部分内容，比如区间估计可以理解为正向的推断统计，假设检验可以理解为反证的推断统计，关于假设检验本身，你可能还需要知道小概率事件、t分布、z分布、卡方分布、p值、alpha错误、belta错误等内容。





## 机器学习

传统的计算机可以执行指令返回结果，但不能自行决定。根据经验给予计算机思考能力的智能算法就是机器学习算法。机器学习是一个从定义数据开始，最终获得一定准确度的模型的过程。



相似度极高的程序代码

<https://www.jianshu.com/p/204e3a7184dc>

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier,Support Vector Machine



https://www.jianshu.com/p/ed9ae5385b89

机器学习的目的主要是通过训练数据样本得到输入数据与输出数据之间存在的某种意义下的依赖关系，根据这种关系建立模型，从而可以对未知的输入数据进行分类和作出预测，使期望分险达到最小。一般机器学习问题类型分为模式识别、函数逼近和概率密度估计三种。

经验分险最小化的主要作用就是使训练数据样本的误差率达到最小。在遇到函数逼近与概率密度估计两个问题时，经验风险最小化是为了使前者的训练数据样本平均误差最小，而后者的最大似然估计法等同于经验风险最小化。

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

对于一个机器学习算法，可以按照如下方法进行思考：

- 它的优化目标是什么
- 它的目标函数/损失函数是什么
- 它用什么方法去优化目标函数的函数值
- 它是怎么对数据进行分类的



训练集、验证集、测试集

当模型复杂度增加时，训练误差减小。但由于过拟合，泛化误差将增大。



### 流程

#### 选取9个参数

井深、oil

#### 去线性化与时频分析

得到RT和RXO之间的差值

对SP曲线去线性化，去掉趋势，先调用线性回归模型，拟合一条直线，然后减去该直线

对SP进行时频分析，提取最大波峰值

提取同一时间，能量最大值



#### 标准化连续值特征

我们要对连续值属性做一些处理，最基本的当然是标准化，让连续值属性处理过后均值为0，方差为1。 这样的数据放到模型里，对模型训练的收敛和模型的准确性都有好处。

使用均值方差标准化模块sklearn.preprocessing.StandardScaler类，使用该类的好处在于可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据。

fit：Compute the mean and std to be used for later scaling 根据训练数据集获取均值和方差，scikit-learn中返回一个Scalar对象。

transform：对训练数据集、测试数据集进行标准化，分别传入对应的数据集。标准化并没有改变训练数据集，而是又生成一个新的矩阵，除非将新生成的数据集赋给原数据集，一般不改变原数据。



#### 皮尔森相关系数与特征两两对比显示

常用的特征选择(排序)方法如皮尔森相关系数和基于学习模型的特征排序 。

Pearson相关系数 Pearson Correlation coefficient

皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]。

基于学习模型的特征排序 (Model based ranking)

基于学习模型的特征排序的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。

皮尔森相关系数为了确定每个特征之间是否紧密相关，如果很相关就属于重复特征，可以去除。我们输入机器学习模型中的每个特征都独一无二，这才是最佳。算法底层原理是根据协方差/（X数组的标准差*Y数组的标准差)判断两个数组的相关性强弱，最终计算出了-1 到 1 的取值结果。其中1和-1 他们就是线性相关，一个是正相关，一个负相关，而0就是没有任何关系的求得了两个数组之间的关系程度，排除强相关特征，降低机器学习数据维度，从而得到更好的模型。

使用Pearson Correlation Heatmap(Pearson相关性热力图)将皮尔森系数值画成图表形式。生成一个 pairplots图来观察每个特征与其它特征之间的关系。之后选取了三个特征参数，并确定了x_train、y_train、x_test、y_test。

接下来开始训练，首先进行多个简单分类器进行训练，最后以ensamble集成方式进行最后的预测。通过训练发现ensamble方式能提高训练集的得分，但提高有限。最重要的是针对测试集的得分反而降低了，不知道是不是储层分类标准的问题导致。

```python
# 数据进行矩阵化为模型的输入做准备
# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models
y_train = train_9T[u'oil'].ravel()
train_9T = train_9T.drop([u'oil',u'DEPTH'], axis=1)
# x_train = train_9T.values # Creates an array of the train data
# 选取三个特征参数
x_train = train_9T[['SP_cwt','DEN','RT_RXO']].values # Creates an array of the train data
```

#### 定义模型类 SklearnHelper

准备开始训练，首先进行多个简单分类器进行训练，最后以ensamble集成方式进行最后的预测。

定义模型类，包括模型__init__，拟合train、预测predict/fit、以及特征重要性函数feature_importances，方便后续调用。

feature_importances

其中sklearn.tree.DicisionTreeClassifier类中的feature_importances_属性返回的是特征的重要性。feature_importances_越高代表特征越重要。树的构建中，每个特征我们都会计算一个它的criterion（gini(基尼不纯度)或者entropy(信息增益)），特征重要性就是这个criterion减少量的归一化值。

我发现用SelectKBest中的特征评分score方法，出来的特征排序结果，与模型训练完毕出来的feature_importances_对特征权重的排序，会有较大出入。。

 SelectKBest中评分默认使用 f_classif （ANOVA F-value between label/feature for classification tasks），也可以使用chi2等，你的模型feature_importances_使用了什么方法？应该是检验方法不同导致的排序不同。

可以输出两种feature_importance，分别是Variable importance和Gini importance，两者都是feature_importance，只是计算方法不同。

Variable importance选定一个feature M，在所有OOB样本的feature M上人为添加噪声，再测试模型在OOB上的判断精确率，精确率相比没有噪声时下降了多少，就表示该特征有多重要。假如一个feature对数据分类很重要，那么一旦这个特征的数据不再准确，对测试结果会造成较大的影响，而那些不重要的feature，即使受到噪声干扰，对测试结果也没什么影响。这就是 Variable importance 方法的朴素思想。

Gini importance选定一个feature M，统计RF的每一棵树中，由M形成的分支节点的Gini指数下降程度（或不纯度下降程度）之和，这就是M的importance。两者对比来看，前者比后者计算量更大，后者只需要一边构建DT，一边做统计就可以。从sklearn的官方文档对feature_importances_参数的描述来看，sklearn应当是使用了Gini importance对feature进行排序，同时sklearn把所有的Gini importance以sum的方式做了归一化，得到了最终的feature_importances_输出参数。

criterion: ”gini” or “entropy”(default=”gini”)是计算属性的gini(基尼不纯度)还是entropy(信息增益)，来选择最合适的节点。



选取几种不同的分类算法：from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier,Support Vector Machine

#### stacking 模型层叠第一层

使用KFold进行k折交叉验证，定义get_oof 方法。

stacking 就是将一系列模型（也称基模型）的输出结果作为新特征输入到其他模型，这种方法由于实现了模型的层叠，即第一层的模型输出作为第二层模型的输入，第二层模型的输出作为第三层模型的输入，依次类推，最后一层模型输出的结果作为最终结果。

5-Fold stacking两层原理图：

![img](https://pic3.zhimg.com/80/v2-69e70cbe2417f707535b458ab200181a_hd.jpg)

KFold是用于k折交叉验证的包。KFold通过提供index索引来确定不同组的训练集以及测试的index，来构造交叉验证数据集。定义进行K-fold训练的get_oof方法，最终返回训练集和测试集在基模型上的预测分类结果，也就是两个一维向量，长度分别是训练集和测试集的样本数。其中预测函数学习时使用k-1个折叠中的数据，最后一个剩下的折叠会用于测试。kf中的train_index为k-1折（k=NFOLDS）的索引，赋值给训练集，test_index为训练集抽出的其中一折数据的索引。

![img](https://pic3.zhimg.com/80/v2-106ffb6ffbf04b5641bcd08f990cf512_hd.png)

从不同的分类模型中可以得到不同的特征比重，而下一个步骤就是将这几个基模型的输出作为第二层模型的输入。首先需要将各个（5个）基模型的输出 reshape 和 concatenate 联结成合适的大小，形成新的（5个）特征作为第二轮的预测的特征，然后用第二层模型进行训练和预测。



#### stacking 模型层叠第二层

使用Cross-validation进行交叉验证。

~~~python
# 分别针对10棵树，100棵树，500棵树进行调参
tune_parameters = {'n_estimators':[10,100,500],'max_depth':[3,6,9]}
clf = GridSearchCV(RandomForestClassifier(warm_start = True,min_samples_leaf= 2, max_feagtures = 'sqrt'),
                   tune_parameters,verbose=0,scoring='accuracy', cv=StratifiedShuffleSplit(n_splits=10,
                   test_size=0.2,train_size=None, random_state=seed)).fit(X_train_second, Y_train_second)
~~~

train_test_split函数用于将矩阵随机划分为训练子集和测试子集，并返回划分好的训练集测试集样本和训练集测试集标签。

GridSearchCV 网格搜索

https://blog.csdn.net/u012969412/article/details/72973055

通常算法不够好，需要调试参数时必不可少。Python超参数自动搜索模块GridSearchCV用于系统地遍历多种参数的不同模型组合，通过交叉验证确定最佳效果参数。其中的estimator是选择使用的分类器，并且传入除需要确定最佳的参数之外的其他参数。cv是交叉验证参数，默认None，指定fold数量，整数代表k折交叉验证，默认为3，也可以是yield训练/测试数据的生成器（交叉验证生成器或一个可迭代器）。



数据集划分函数 StratifiedShuffleSplit，交叉验证的概念同样适用于这个划分函数。

函数作用描述：

1.其产生指定数量的独立的train/test数据集划分数据集划分成n组。

2.首先将样本随机打乱，然后根据设置参数划分出train/test对











#### 杂记

plotly、sklearn、seaborn、 preprocessing、pandas、



设置断点，执行到这一句的上一句后截至



全角模式：输入一个字符占用2个字符，  半角模式：输入一个字符占用1个字符。

shift加上 空格键可以实现键盘模式切换。



UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbe in position 0: invalid start byte

~~~python
# UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbe in position 0: invalid start byte
# csv中的编码格式是‘ANSI’，而Python 3默认读取格式为‘utf-8’，导致decode异常
test_label = pd.read_csv(test_label, sep='\s+',header = Nrows_header,encoding="ANSI") # 不报错，但是存在NaN
# test_label = pd.read_csv(test_label, sep='\s+',header = Nrows_header,na_values= NAN_values) # 依旧报错
test_label = pd.read_csv(test_label, sep='\s+',header = Nrows_header)
~~~



mac快捷键

https://jingyan.baidu.com/article/08b6a591aac09614a909224f.html

快捷键之文本处理：

Command-右箭头 将光标移至当前行的行尾　　

Command-B 切换所选文字粗体（Bold）显示　　

fn-Delete 相当于PC全尺寸键盘上的Delete，也就是向后删除　　

fn-上箭头 向上滚动一页（Page Up）　　

fn-下箭头 向下滚动一页（Page Down）　　

fn-左箭头 滚动至文稿开头（Home）　　

fn-右箭头 滚动至文稿末尾（End）　　

Command-右箭头 将光标移至当前行的行尾　　

Command-左箭头 将光标移至当前行的行首　　

Command-下箭头 将光标移至文稿末尾　　

Command-上箭头 将光标移至文稿开头　　

Option-右箭头 将光标移至下一个单词的末尾　　

Option-左箭头 将光标移至上一个单词的开头　　

Control-A 移至行或段落的开头



权重

脏数据数据



特征曲线计算：泥质含量 孔隙度 含油饱和度 



CSV

逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号），其文件以纯文本形式存储表格数据（数字和文本）。纯文本意味着该文件是一个，不含必须像二进制数字那样被解读的数据。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。

CSV文件有个突出的优点，可以用excel等软件打开，比起记事本和matlab、python等编程语言界面，便于查看、制作报告、后期整理等。



导入当前文件夹内的模块需要注意以下三点：

- 模块内函数执行语句前添加if __name__ == "__main__":；

- 新建文件，模块不会自动执行；

- 模块名前不需要添加.。

from Oil_Classification_random_2870 import train_label,change_data,select_feature,split_data,random_clf,score







### 入门

#### 机器学习项目流程

1. 了解需求，获取数据。与产品和运营开会，了解需求，然后提取公司积累大量的数据和自己网上下载、爬取的数据。

2. 数据预处理。数据处理大概会占到整个50%-70%的工作量，通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。

3. 特征工程。做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。把中文分词的字符串转换成数字，有两种常用的表示模型分别是词袋模型和词向量。

4. 特征选择。构造好的特征向量，是要选择合适的、表达能力强的特征。特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。

5. 模型训练。对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型，如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。

6. 评价指标。训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。

7. 模型上线应用。模型线上应用，线下训练模型，然后将模型做线上部署，发布成接口服务以供业务系统使用。

#### 监督学习与非监督学习

要区分监督学习与非监督学习，也就是判断是否有监督（supervised），就看输入数据是否有标签（label）。输入数据有标签，则为有监督学习，没标签则为无监督学习，这是二者最根本的区别。监督学习的数据既有特征又有标签，而非监督学习的数据中只有特征而没有标签。

首先看什么是学习（learning）/训练？一个成语就可概括：举一反三。  最简单也最普遍的一类机器学习算法就是分类（classification）。对于分类，输入的训练 数据有特征（feature），有标签（label）。所谓的学习，其本质就是找到特征和标签间的关系（mapping）。这样当有特征而无标签的未知数据输入时，我们就可以通过已有的关系得到未知数据标签。

在上述的分类过程中，如果所有训练数据都有标签，则为有监督学习（supervised learning）。监督学习可以分为两大类：回归分析和分类，二者之间的区别在于回归分析针对的是连续数据，而分类针对的是离散数据。如果数据没有标签，显然就是无监督学习（unsupervised learning）了，也即聚类（clustering）。因此聚类算法的效果低于监督学习，但是由于在实际应用中，标签的获取常常需要极大的人工工作量，有时甚至非常困难。例如在自然语言处理（NLP）中，Penn Chinese Treebank在2年里只完成了4000句话的标签。

监督学习在于学习两个数据集的联系：观察数据 X 和我们正在尝试预测的额外变量 y (通常称“目标”或“标签”)， 而且通常是长度为 n_samples 的一维数组。scikit-learn 中所有监督的 估计量 都有一个用来拟合模型的 fit(X, y) 方法，和根据给定的没有标签观察值 X 返回预测的带标签的 y 的 predict(X) 方法。当在 scikit-learn 中进行分类时，y 是一个整数或字符型的向量。

然而在监督学习与非监督学习之间还存在着一种半监督学习。半监督学习的训练数据中有一部分是有标签的，另一部分是没有标签的，而没标签的数据量远远大于有标签的数据量。隐藏在半监督学习下的基本规律在于数据的分布必然不是完全随机的，通过一些有标签数据的局部特征，以及更多没标签数据的整体分布，就可以得到可以接受甚至是非常好的分类结果。



捕获拟合参数噪声使得模型不能归纳新的数据称为 过拟合。

维度惩罚，是机器学习领域的核心问题。？？



#### 特征工程

[特征工程：特征选择、特征表达、特征预处理](https://blog.csdn.net/songyunli1111/article/details/82937931)

特征工程是一种工程活动，目的是从原始数据中最大限度的提取出能表征原始数据信息的特征。**数据和特征决定了机器学习的上限，算法和模型不过是逼近这个上限。**与机器学习不同的是，深度学习不用像传统机器学习那样人为合成高级复杂特征，只需利用人类的先验知识处理一阶特征，后面深度学习会自己学习到相关的复杂特征。

从数据中对特征进行工程化（如创建、转换等）对过程就是特征工程。在数据科学中，有效的属性（或字段）对形式称为特征。

1. 特征提取与特征选择

共同点是都是用于减少训练数据的维度。

- 特征提取（Feature Extraction）是生成较少的新特征来表示所有的特征，如现有特征的变换；
- 特征选择是从数据中选择有用的特征，基本上是一个搜索问题，如使用皮尔森相关系数获得特征分数。

2. 处理数值特征

两种常用的数值特征标准化技术包括：

- 最大最小标准化（Min-Max Normalization），使得特征取值范围在[0,1]内，可以通过sklearn.preprocessing预处理中的MinMaxScaler完成；
- Z-分数标准化（Z-Score Normalization），在存在离群值时，随着数据范围的增加，最大最小标准化将使数值不断接近零。Z-Score遵循统计学原理，使数据标准差为1，均值为0，可以通过sklearn.preprocessing预处理中的scaler完成；
- 有时将数值特征转化成类别或者将它们组合在一起是有益的。这种方法有助于减少离群效应，并产生准确的预测。

3. 处理分类特征

分类特征是包含预定义集合值的特征。处理分类特征是将数据放入预分类的类别中。

- 将分类变量转化为数值，可以通过sklearn.preprocessing预处理中的labelEncoder完成，从零开始向每个类别分配唯一的一个整数，还可以执行反向变换，返回类别；

- 创建虚拟变量，为每个类别创建一个新的特征，每个特征包含0和1。在同一时间，只有一个特征可以取值1。可以使用pandas中的get_dummies(dataframe.categories)完成。类似于 one-hot encoding。



#### 数据预处理

sklearn.preprocessing

https://blog.csdn.net/u012609509/article/details/78554709

标准化数据的作用是，保证每个维度的特征数据方差为1，均值为0。使得预测结果不会被某些维度过大的特征值而主导。

训练的时候需要在标准化时记下训练集的标准化参数，如果是z-score标准化，则需记下训练集的均值和标准差。在测试时，**测试集使用训练集的参数进行标准化。**如果是z-score标准化，则测试集样本需要减去训练集均值并除以训练集标准差，从而标准化。

算法比赛的目的是为了在给定的数据集（绝大多数数据已知）上达到最大准确度，因此可以直接将训练集和测试集都进行标准化，然后就进行预测。而生产项目是为了在给定的 + 海量未知的数据集上达到最大准确度。因此生产项目的算法泛化能力（对未知数据的预测能力）一定要强。也就是尽量不要过于依赖于已知数据集。所以我们一般只拿训练集的均值方差做z-score标准化。

均值方差标准化模块sklearn.preprocessing.StandardScaler类的作用：去均值和方差归一化。且是针对每一个特征维度来做的。使用该类的好处在于可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据。

- fit：Compute the mean and std to be used for later scaling 根据训练数据集获取均值scaler.mean_和方差scaler.var_，scikit-learn中返回一个Scalar对象。scaler没有标准差scaler.std_属性。这里的均值是每个特征即每列的均值。

- transform：对训练数据集、测试数据集进行标准化，分别传入对应的数据集。标准化并没有改变训练数据集，而是又生成一个新的矩阵，除非将新生成的数据集赋给原数据集，一般不改变原数据。

- fit_transform：fit和transform的组合。

需要注意的是这里的fit(x) / fit_transform(x)的括号中只有一个参数，而常见的模型传入两个参数。原因在于fit(x,y)传两个参数的是有监督学习的算法，fit(x)传一个参数的是无监督学习的算法，比如降维、特征提取、标准化。

~~~python
def test_algorithm():
    np.random.seed(123)
    print('use sklearn')
    # 注：shape of data: [n_samples, n_features]
    data = np.random.randn(10, 4)
    scaler = StandardScaler()
    scaler.fit(data)
    trans_data = scaler.transform(data)
    print('original data: ')
    print data
    print('transformed data: ')
    print trans_data
    print('scaler info: scaler.mean_: {}, scaler.var_: {}'.format(scaler.mean_, scaler.var_))
    print('\n')

    print('use numpy by self')
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    var = std * std
    print('mean: {}, std: {}, var: {}'.format(mean, std, var))
    # numpy 的广播功能
    another_trans_data = data - mean
    # 注：是除以标准差
    another_trans_data = another_trans_data / std
    print('another_trans_data: ')
    print another_trans_data
~~~

#### validation与cross-validation

https://blog.csdn.net/u010167269/article/details/51340070

从训练集中抽取验证集。验证与交叉验证。

- validation

在机器学习任务中，拿到数据后，如果给定的样本充足，在调参的时候需要将数据集分为三部分，分别是：训练集（training set）、验证集（validation set）以及测试集（testing set）。 训练集用于训练模型，验证集用于模型的选择，选择对验证集有最小预测误差的模型。验证集用于防止过拟合。测试集对于模型来说是未知数据，用于评估模型的泛化能力（generalization）。如果给定的样本有限，可以采用交叉验证。也就是说，从验证集转换到了交叉验证集。

![img](https:////upload-images.jianshu.io/upload_images/1667471-9db53006d07c7d20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/688)

- cross-validation

We want to use all of the data. We would do 5-fold cross validation by training the model 5 times on 80% of the data and testing on 20%. We ensure that each data point ends up in the 20% test set exactly once. We've therefore used every data point we have to contribute to an understanding of how well our model performs the task of learning from some data and predicting some new data. The purpose of cross-validation is model checking, not model building.



#### 抽样技术



**Cross-validation is a tool to help you estimate the skill of models. We calculate these estimates so we can compare models and configs.**

After we have chosen a model and it’s config, we throw away all of the CV models. We’re done estimating.

We can now fit the “final model” on all available data and use it to make predictions.



The problem with applied machine learning is that we are trying to model the unknown.

On a given predictive modeling problem, the ideal model is one that performs the best when making predictions on new data.

We don’t have new data, so we have to pretend with statistical tricks.

The train-test split and k-fold cross validation are called resampling methods. Resampling methods are statistical procedures for sampling a dataset and estimating an unknown quantity.

Machine learning algorithms are stochastic and this behavior of different performance on the same data is to be expected.

Resampling methods like repeated train/test or repeated k-fold cross-validation will help to get a handle on how much **variance** there is in the method.



train_test_split + fit + score == cross_val_score



sklearn.model_selection

https://www.cnblogs.com/feffery/p/8823405.html

https://blog.csdn.net/luanpeng825485697/article/details/79836262

怎样对数据集进行合理的抽样-训练-验证至关重要，机器学习中常见的抽样技术包括：

- 留出法（hold-out）：常用sklearn.model_selection中的train_test_split()实现分割；

- 交叉验证法（cross validation）：常用klearn.model_selection中的cross_val_score()与cross_validate()，两者输出不同；

- 基于生成器的交叉验证法：常用sklearn.model_selection中的KFold。KFold()以生成器的方式产出每一次交叉验证所需的训练集与验证集。在sklearn.model_selection中使用TimeSeriesSplit()来分割时间序列类型的数据。

具体如下：

1. 留出法 （holdout cross validation）二划分验证

留出法（hold-out）的基本思想是将数据集D（即我们获得的所有样本数据）划分为两个互斥的集合，将其中一个作为训练集S，另一个作为验证集T。

需要注意的是，训练集/验证集的划分要尽可能保持数据分布的一致性，尽量减少因数据划分过程引入额外的偏差而对最终结果产生的影响，例如在分类任务中，要尽量保持S与T内的样本各个类别的比例大抵一致，这可以通过分层抽样（stratified sampling）来实现。

- train_test_split

在分类问题中，我们通常通过对训练集进行train_test_split，划分成train 和test 两部分，其中train用来训练模型，test用来评估模型，模型通过fit方法从train数据集中学习，然后调用score方法在test集上进行评估，打分；从分数上我们可以知道 模型当前的训练水平如何。然而，这种方式存：只进行了一次划分，数据结果具有偶然性，如果在某次划分中，训练集里全是容易学习的数据，测试集里全是复杂的数据，这样就会导致最终的结果不尽如意；反之，亦是如此。

2. 交叉验证法

交叉验证法（cross validation）先将数据集D（dataset）划分为k个大小相似的互斥子集，即D=D1UD2U...UDk，Di∩Dj=Φ(i≠j)，每个子集Di都尽可能保持数据分布的一致性，即从*D*中通过分层采样得到。然后每次用k-1个子集的并集作为训练集，剩下的那一个子集作为验证集；这样就可获得k组训练+验证集，从而可以进行k次训练与测试，最终返回的是这k个测试结果的均值。显然，交叉验证法的稳定性和保真性在很大程度上取决与k的取值，因此交叉验证法又称作“k折交叉验证”（k-fold cross validation）。

K值的选取会影响bias和viriance。K越大，每次投入的训练集的数据越多，模型的Bias越小。但是K越大，又意味着每一次选取的训练集之前的相关性越大，而这种大相关性会导致最终的test error具有更大的Variance。一般来说，根据经验我们一般选择k=5或10。

- Standard Cross Validation

针对上面通过train_test_split划分，从而进行模型评估方式存在的弊端，提出Cross Validation 交叉验证。

Cross Validation：简言之，就是进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练、测试评估，从而得出一个评价结果；如果是5折交叉验证，意思就是在原始数据集上，进行5次划分，每次划分进行一次训练、评估，最后得到5次划分后的评估结果，一般在这几次评估结果上取平均得到最后的 评分。

- cross_val_score

使用交叉验证最简单的方法是在估计器和数据集上调用 cross_val_score 辅助函数，作为参数cv传入。

cv代表不同的cross validation的方法。如果cv是一个int值，并且如果提供了rawtarget参数，那么就代表使用StratifiedKFold分类方式；如果cv是一个int值，并且没有提供rawtarget参数，那么就代表使用KFold分类方式；也可以给定它一个CV迭代策略生成器，指定不同的CV方法。

对于分类问题，这个函数的返回值就是对于每次不同的的划分raw_data时，在test_data上得到的分类的**准确率**。至于准确率的算法可以通过score_func参数指定，如果不指定的话，是用clf默认自带的准确率算法。

3. 基于生成器的交叉验证法

基于生成器的方法的好处是利用Python中生成器（generator）的方式，产出每一次交叉验证所需的训练集与验证集，以非常节省内存的方式完成每一次的交叉验证。KFold本质是将将索引分离。

~~~python
# https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation
>>> def custom_cv_2folds(X):
...     n = X.shape[0]
...     i = 1
...     while i <= 2:
...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)
...         yield idx, idx
...         i += 1
...
>>> custom_cv = custom_cv_2folds(iris.data)
>>> cross_val_score(clf, iris.data, iris.target, cv=custom_cv)
array([1.        , 0.973...])
~~~

交叉验证法包括：基于生成器的K折交叉验证法KFold、分层K折交叉验证StratifiedKFold、分层随机交叉验证StratifiedShuffleSplit等。

- KFold交叉验证（k-fold cross validation）

KFold 将所有的样例划分为 k 个组，称为折叠 (fold) （如果 k = n， 这等价于 Leave One Out（留一） 策略），都具有相同的大小（如果可能）。预测函数学习时使用 k - 1 个折叠中的数据，最后一个剩下的折叠会用于测试。通过设置伪随机数生成器random_state, 控制随机性使结果可重复。可以循环遍历出索引。

- StratifiedKFold与StratifiedShuffleSplit

如何解决样本不平衡问题？ 使用基于类标签、具有分层的交叉验证迭代器StratifiedKFold和StratifiedShuffleSplit 分层抽样。一些分类问题在目标类别的分布上可能表现出很大的不平衡性：例如，可能会出现比正样本多数倍的负样本。在这种情况下，建议采用如 StratifiedKFold 和 StratifiedShuffleSplit 中实现的分层抽样方法，确保相对的类别频率在每个训练和验证 折叠 中大致保留。

StratifiedKFold 是 k-fold 的变种，会返回 stratified（分层） 的折叠：每个小集合中， 各个类别的样例比例大致和完整数据集中相同。通常情况下，可以设置cv参数来控制几折，但是我们希望对其划分等加以控制，所以出现了KFold，KFold控制划分折，可以控制划分折的数目，是否打乱顺序等，可以赋值给cv，用来控制划分。

StratifiedShuffleSplit 是 ShuffleSplit 的一个变种，会返回直接的划分，比如： 创建一个划分，但是划分中每个类的比例和完整数据集中的相同。



交叉验证迭代器

i.i.d.数据交叉验证迭代器

假设数据是独立同分布的(Independent and Identically Distributed, i.i.d.), 即，所有样本相互独立且来自相同的分布。注意到，虽然i.i.d.数据是机器学习理论普遍的假设，但实际情况通常不满足。

随机置换CV

ShuffleSplit迭代器将产生一个用户定义的独立训练/检验集分割数。首先shuffle样本，然后分到训练集和检验集。通过设置伪随机数生成器random_state, 控制随机性使结果可重复。



选择模型的参数时可以手动设置，也可以通过使用 网格搜索 及 交叉验证 等工具，可以自动找到参数的良好值。



shuffle：bool型，控制是否在采样前打乱原数据顺序，后期用于测试！！



#### 不平衡样本的处理

[从“数据层面“和”算法层面“两个方面介绍不平衡样本问题。](https://blog.csdn.net/siyue0211/article/details/80318999)

数据层面采用数据重采样处理解决样本不平衡问题，其操作简单，不过该类方法会改变数据原始分布，有可能产生过拟合；

算法层面采用代价敏感法处理样本不平衡问题，通过指定代价敏感矩阵或代价敏感向量的错分权重，可缓解样本不平衡带来的影响。

机器学习中经典假设中往往假定训练样本各类别是同等数量即各类样本数目是均衡的，但是真实场景中遇到的实际问题却常常不符合这个假设。做分类算法训练时，如果训练集里的各个类别的样本数量不是大约相同的比例，就需要处理样本不平衡问题。

如何解决这个问题呢？一般是两种方法：权重法或者采样法。

权重法是比较简单的方法，我们可以对训练集里的每个类别加一个权重class weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，我们还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。

如果权重法做了以后发现预测效果还不好，可以考虑采样法。

采样法包括简单的数据重采样与其他方法。简单的数据重采样包括上采样和下采样。由于样本较少类，可使用上采样，即复制该图像直至与样本最多类的样本数一致。当然也可以由数据扩充方式替代简单复制。对于样本较多的类别，可采用下采样，注意，对深度学习而言，下采样并不是直接随机丢弃一部分图像，因为那样做会降低训练数据多样性而影响模型泛化能力。正确的下采样方式为，在批处理训练时对每批随机抽取的图像严格控制其样本较多类别的图像数量。以二分为例，原数据的分布情况下每次批处理训练正负样本平均数量比例为5：1，如仅使用下采样，可在每批随机挑选啊训练样本时每5个正例只取1个放入该批训练集的正例，负例选取按照原来的规则，这样可使每批选取的数据中正负比例均等。此外，仅仅将数据上采样有可能会引起模型过拟合。更保险有效的方式是上采样和下采样结合使用。

类别均衡采样是把样本按类别分组，每个类别生成一个样本列表，训练过程中先随机选择1个或几个类别，然后从各个类别所对应的样本列表里选择随机样本。这样可以保证每个类别参与训练的机会比较均等。 上述方法需要对于样本类别较多任务首先定义与类别相等数量的列表，对于海量类别任务如ImageNet数据集等此举极其繁琐。海康威视研究院提出类别重组的平衡方法。 类别重组法只需要原始图像列表即可完成同样的均匀采样任务，步骤如下： 

首先按照类别顺序对原始样本进行排序，之后计算每个类别的样本数目，并记录样本最多那个类的样本数目。之后，根据这个最多样本数对每类样本产生一个随机排列的列表， 然后用此列表中的随机数对各自类别的样本数取余，得到对应的索引值。接着，根据索引从该类的图像中提取图像，生成该类的图像随机列表。之后将所有类的随机列表连在一起随机打乱次序，即可得到最终的图像列表，可以发现最终列表中每类样本数目均等。根据此列表训练模型，在训练时列表遍历完毕，则重头再做一遍上述操作即可进行第二轮训练，如此往复。 类别重组法的优点在于，只需要原始图像列表，且所有操作均在内存中在线完成，易于实现。

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180515094552661?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpeXVlMDIxMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上述几种常用的采样法很简单，但是都有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差。所以有的算法就通过其他方法来避免这个问题，比如SMOTE算法通过人工合成的方法来生成少类别的样本。方法也很简单，对于某一个缺少样本的类别，它会随机找出几个该类别的样本，再找出最靠近这些样本的若干个该类别样本，组成一个候选合成集合，然后在这个集合中不停的选择距离较近的两个样本，在这两个样本之间，比如中点，构造一个新的该类别样本。举个例子，比如该类别的候选合成集合有两个样本(x1,y),(x2,y),那么SMOTE采样后，可以得到一个新的训练样本((x1+x2)/22,y),通过这种方法，我们可以得到不改变训练集分布的新样本，让训练集中各个类别的样本数趋于平衡。我们可以用imbalance-learn这个Python库中的SMOTEENN类来做SMOTE采样。

#### 参数寻优 GridSearchCV

参数调整  Parametr Tuning

选择模型的参数时可以手动设置，也可以通过使用 网格搜索 及 交叉验证 等工具，可以自动找到参数的良好值。

如 GridSearch（网格搜索）是优化模型参数时采用的一种暴力搜索策略。GridSearchCV（网络搜索交叉验证）用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。也就是说GridSearchCV根据你给定的模型自动进行交叉验证，通过调节每一个参数来跟踪评分结果，实际上，该过程代替了进行参数搜索时的for循环过程。这里GridSearchCV 的参数形式和cross_val_score的形式差不多。

GridSearchCV 存在的意义就是自动调参，只要把参数输进去，就能给出最优化的结果和参数。但是这个方法适合于小数据集，一旦数据的量级上去了，很难得出结果。这个时候就是需要动脑筋了。

数据量比较大的时候可以使用一个快速调优的方法——坐标下降。它其实是一种贪心算法：拿当前对模型影响最大的参数调优，直到最优化；再拿下一个影响最大的参数调优，如此下去，直到所有的参数调整完毕。这个方法的缺点就是可能会调到局部最优而不是全局最优，但是省时间省力。

~~~python
# clf.best_score_等于cv_results_列表中的最大值 [0.84  0.848 0.846]，可以进行画图了
# https://blog.csdn.net/weixin_40283816/article/details/83346098
# print(clf.cv_results_)
print(clf.cv_results_["mean_test_score"])
print("OK")
# grid_scores_：给出不同参数情况下的评价结果
# cv_results_ ： 具体用法模型不同参数下交叉验证的结果
# 格式：[mean: , std: , params: {'n_neighbors': }]

# print(clf.grid_scores_)
# AttributeError: 'GridSearchCV' object has no attribute 'grid_scores_'
# grid_scores_在sklearn0.20版本中已被删除，取而代之的是cv_results_
# print(clf.cv_results_) # <class 'dict'>
# print("best_estimator_: %s"%clf.best_estimator_) # 获取最佳模型
print("best_score_: %s"%clf.best_score_) # 查看优化过程期间观察到的最佳分数(此处为f1_score)
# best_params_: {'n_estimators': 200}
print("best_params_: %s"%clf.best_params_) # 查看最佳参数

# 特征之间的关联
# 学习曲线与ROC曲线

# predict 进行预测
y_pred = clf.best_estimator_.predict(x_test)
print(confusion_matrix(y_test,y_pred)) # 混淆矩阵
# scikitplot.metrics.plot_confusion_matrix(y_test,y_pred,normalize=True)

# accuracy_score(y_test,y_pred) == score(x_test,y_test)
print("accuracy_score: %s"%accuracy_score(y_test,y_pred))
print("score: %s"%clf.best_estimator_.score(x_test,y_test))
~~~

sklearn中GridSearchCV设置嵌套参数：

集成学习模型有两种参数，包括框架参数与基模型参数。以adaboost为例，adaboost有自己的参数，它的base_estimator指向一个弱学习器，这个弱学习器也包含自己的参数，为了使用GridSearchCV，我们需要使用嵌套参数。在sklearn中我们使用双下划线表示”__”。使用方法见程序。

~~~python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.grid_search import GridSearchCV
iris = datasets.load_iris()
param_grid = {"base_estimator__criterion": ["gini", "entropy"],
          "base_estimator__splitter":   ["best", "random"],
          "n_estimators": [1, 2]}
dtc = DecisionTreeClassifier()
ada = AdaBoostClassifier(base_estimator=dtc)
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
grid_search_ada = GridSearchCV(ada, param_grid=param_grid, cv=10)
grid_search_ada.fit(X, y)}
~~~



#### 模型评价

https://blog.csdn.net/heyongluoyao8/article/details/49408319 指标

https://blog.csdn.net/yueguizhilin/article/details/77711789 可视化

对于分类模型，模型性能指标（evaluation metric）：准确率，混淆矩阵，ROC，ROC曲线和AUC（ROC曲线下面积 Area under the ROC curve）。

在回归模型中，模型性能指标：均方根误差（rMSE）与R平方估计（R-squared）。



准确率（accuracy）是对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率。由准确率，我们的确可以在一些场合，从某种意义上得到一个分类器是否有效，但它并不总是能有效的评价一个分类器的工作。如果是对于平衡样本来说，这个没问题。但是对于样本不平衡的情况，这就不行。因此出现了precision，recall和f1-measure。

精确率和召回率：

- 精确率（precision）是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。
- 召回率（recall）是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。

精确率和召回率都是针对某一类计算。其实二者就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。通俗的说，精确率就是找得对，召回率就是找得全。

F1值（f1-measure）就是精确率和召回率的调和平均值。默认认为精确率和召回率的权重是一样的，但有些场景下，可能认为精确率会更加重要，这时需要对参数进行调整。

精确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下精确率高、召回率就低，召回率低、精确率高，当然如果两者都低，那是什么地方出问题了。如果是做搜索，那就是保证召回的情况下提升精确率；如果做疾病监测、反垃圾，则是保精确率的条件下，提升召回。所以，在两者都要求高的情况下，可以用F1来衡量。

<https://blog.csdn.net/asasasaababab/article/details/79994920>

<https://blog.csdn.net/u012879957/article/details/80564148>







#### 交叉验证的用处

<https://blog.csdn.net/dinkwad/article/details/78647273>

交叉验证可以用来确定模型的参数、检视过拟合、用于模型的选择、用于特征选择。

确定模型的参数，

检视过拟合，

用于模型的选择，

一般我们在模型训练过程中，会采用K折交叉验证的方法来验证模型的表现，从而进行调参。

用于特征选择，



#### 降维 PCA与SVD

PCA与SVD都可以将N个特征降维到K个，所以可以用来进行数据压缩，例如100维的向量最后可以用10维来表示，那么压缩率为90%。

PCA是主成分分析的简写，实际上就是将N维数据降维到最能代表数据集真实结构的K个正交维度上去（N>K），且这K个维度是重新构造而成的，而不是从原有N维中选取，这K维特征被称为主成分。其价值在于四点：使数据集更容易使用、降低很多算法的计算开销、去除噪声、使结果易懂。

PCA对数据进行降维，我们要把高维度的数据去除噪声，只保留最重要的N个维度，那怎么确定需要保留的维度呢？第一个维度选择的是原始数据中方差最大的方向，通俗的讲，就是给定大量数据点，我们画出一条直线，尽可能的经过这些点，这条直线的方向就是方差最大的方向。接下来要找第二个维度，对第二个维度有这样的要求，它需要与第一个维度正交，我们要在与第一个坐标轴正交的所有坐标轴里找到方差最大的方向作为第二个坐标轴，第三个坐标轴同理，它需要与前两个坐标轴均正交。

PCA方法中特征值最大的那个特征向量的方向（投影到该坐标轴后数据集总方差最大的方向），实际上也就是我们用最小二乘法对数据集进行拟合所得到的线性回归直线的方向，这个是可以证明的，最小二乘法目标函数的优化目标实际上就是找到数据集协方差矩阵（X'X）的最大特征值，而最大特征值对应的特征向量也就是最小二乘法的回归直线。

SVD是奇异值分解的简称，SVD同样是将高维数据降低到低维，或者理解成在噪声数据中抽取相关特征。

SVD的应用非常广，其中一个就是隐性语义索引，SVD可以抽取文档和词的概念，也就是可以识别拥有同一主题的文章以及同义词。举个例子，如果我们要根据某个关键词找出包含它的文章，如果文档中的该词包含了错别字，或者使用的是该词的同义词，只基于词语存在与否的搜索是无法找到这样的文章的，但是使用SVD就可以做到。

SVD的另一个应用就是推荐系统，简单的推荐系统直接用余弦距离等计算项或者人之间的相似度，更先进的方法则先利用SVD从数据中构建出一个主题空间，然后再在该空间下计算相似度。对于原始数据矩阵进行SVD处理就能将数据压缩到若干概念中去。也可以用于自然语言处理。



#### k-近邻（kNN）算法

一句话概要，将标注好类别的训练样本映射到X（选取的特征数）维的坐标系之中，同样将测试样本映射到X维的坐标系之中，选取距离该测试样本欧氏距离（两点间距离公式）最近的k个训练样本，其中哪个训练样本类别占比最大，我们就认为它是该测试样本所属的类别。kNN是监督学习算法。

kNN的缺点在于计算的时间空间复杂度都太高，首先内存中需要存储所有的训练样本点，而且每个新测试样本需要通过kNN算法分类，都要计算这个测试样本与所有训练样本点的欧式距离。

决策树算法较之于kNN算法一个很显著的不同在于，kNN算法需要持续使用全部的训练数据，而决策树算法经过训练构建出决策树之后，就不再需要将所有的训练样本保持在内存中了，这颗决策树就是在训练样本中抽象出来的模型。



### 决策树

https://shuwoom.com/?p=1452

#### decision tree

决策树（decision tree）：是一种基本的分类与回归方法，此处主要讨论分类的决策树。

决策树可以捕获数据中的非线性关系，使用贪心算法来创建树。决策树的学习、创建与预测的速度都很快。优点是不需要对数据进行标准化或其他清理，不需要基于假设。缺点是决策树需要停止条件。决策树的停止条件是节点中的样本个数小于预定阈值，或样本集的基尼指数小于（信息增益或信息增益比大于）预定值，或者没有更多特征。

决策树主要有两种类型：

- 分类树的因变量是离散的，节点处采用少数服从多数的原则生成预测；
- 回归树的因变量是连续的，节点处采用预测值的均值生成预测。

在分类问题中，表示基于特征对实例进行分类的过程，可以认为是if-then的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一颗熵值下降最快的树，到叶子节点处，熵值为0。其具有可读性、分类速度快的优点，是一种有监督学习。最早提及决策树思想的是Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法。

决策树算法的核心在于决策树的构建，每次选择让整体数据香农熵（描述数据的混乱程度）减小最多的特征，使用其特征值对数据进行划分，每次消耗一个特征，不断迭代分类，直到所有特征消耗完（选择剩下数据中出现次数最多的类别作为这堆数据的类别），或剩下的数据全为同一类别，不必继续划分，至此决策树构建完成，之后我们依照这颗决策树对新进数据进行分类。

决策树通常有三个步骤：

- 特征选择：使用某特征对数据集划分之后，各数据子集的纯度要比划分前的数据集D的纯度高（也就是不确定性要比划分前数据集D的不确定性低）；
- 决策树的生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。这个过程实际上就是使用满足划分准则的特征不断的将数据集划分成纯度更高，不确定行更小的子集的过程。对于当前数据集的每一次划分，都希望根据某个特征划分之后的各个子集的纯度更高，不确定性更小；
- 决策树的修剪：决策树容易过拟合，一般需要剪枝来缩小树结构规模、缓解过拟合。使用CART构建树时会涉及到树剪枝的问题，一棵树如果节点过多，表示该模型可能对数据进行了过拟合，我们可以通过交叉验证来判断是否发生了过拟合，通过降低决策树复杂度来避免过拟合的过程称为剪枝，提前终止条件实际上就是预剪枝，另一种形式的剪枝需要使用训练集和测试集，称作后剪枝。如果使用预剪枝，我们需要不断的修改停止条件，而后剪枝则不需要用户指定参数，但相对的，后剪枝不如预剪枝有效。

如下图是一棵结构简单的决策树，用于预测贷款用户是否具有偿还贷款的能力。贷款用户主要具备三个属性：是否拥有房产，是否结婚，平均月收入。每一个内部结节都表示一个属性条件判断，叶子结节表示贷款用户是否具有偿还能力。这里说的属性，也就是算法中的特征，对应于数据表就是字段。这里说的可以偿还/不能偿还，就是一种分类问题。

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20160525193701618)

#### 特征选择

常用的特征选择(排序)方法如皮尔森相关系数和基于学习模型的特征排序 。

Pearson相关系数 Pearson Correlation coefficient

皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]。

基于学习模型的特征排序 (Model based ranking)

基于学习模型的特征排序的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。

皮尔森相关系数为了确定每个特征之间是否紧密相关，如果很相关就属于重复特征，可以去除。我们输入机器学习模型中的每个特征都独一无二，这才是最佳。算法底层原理是根据协方差/（X数组的标准差*Y数组的标准差)判断两个数组的相关性强弱，最终计算出了-1 到 1 的取值结果。其中1和-1 他们就是线性相关，一个是正相关，一个负相关，而0就是没有任何关系的求得了两个数组之间的关系程度，排除强相关特征，降低机器学习数据维度，从而得到更好的模型。



#### 特征选取评估方法

特征选择表示从众多的特征中选择一个特征作为当前节点分裂的标准，如何选择特征有不同的量化评估方法，从而衍生出不同的决策树，如ID3（通过信息增益选择特征）、C4.5（通过信息增益比选择特征）、CART（通过Gini指数选择特征）等。其中：

- ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征递归地构建决策树；
- C4.5算法与ID3算法很相似，C4.5算法是对ID3算法做了改进，在生成决策树过程中采用信息增益比来选择特征。由于信息增益会偏向取值较多的特征，使用信息增益比可以对这一问题进行校正；
- 基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点跟熵相似。

CART(classification and regression tree分类与回归树)假设决策树为二叉树。

- 回归树 → 平方误差最小化准则

- 分类树 → 基尼指数(Gini index)最小化准则

基尼指数是一种数据的不纯度的度量方法，系数越小，代表数据纯度越高。当数据集中数据混合的程度越高，基尼指数也就越高。当数据集 D 只有一种数据类型，那么基尼指数的值为最低 0。 对于特征选取，需要选择最小的分裂后的基尼指数。也可以用基尼指数增益值作为决策树选择特征的依据。在决策树选择特征时，应选择基尼指数增益值最大的特征，作为该结节分裂条件。

基于信息熵，通过信息增益的计算公式，可以看出和基尼系数大体类似。

sklearn.tree.DicisionTreeClassifier类中的feature_importances_属性返回的是特征的重要性，feature_importances_越高代表特征越重要。树的构建中，每个特征我们都会计算一个它的criterion（基尼指数或者信息熵），特征重要性就是这个criterion减少量的归一化值。在scikit-learn中的默认实现也就是我们常说的Gini importance。

我发现用SelectKBest中的特征评分score方法，出来的特征排序结果，与模型训练完毕出来的feature_importances_对特征权重的排序，会有较大出入。。这个该如何是好？SelectKBest中评分默认使用 f_classif （ANOVA F-value between label/feature for classification tasks），也可以使用chi2等，你的模型feature_importances_使用了什么方法？应该是检验方法不同导致的排序不同。

可以输出两种feature_importance，分别是Variable importance和Gini importance，两者都是feature_importance，只是计算方法不同。



#### 量化纯度、信息增益度、停止条件、评估

https://www.jianshu.com/p/ea12c6a46198



#### 信息增益

划分数据集的大原则是：将无序数据变得更加有序，但是各种方法都有各自的优缺点。信息论是量化处理信息的分支科学，在划分数据集前后信息发生的变化称为信息增益，获得信息增益最高的特征就是最好的选择，所以必须先学习如何计算信息增益，集合信息的度量方式称为香农熵，或者简称熵。

熵定义为信息的期望值，如果待分类的事物可能划分在多个类之中，需要计算所有类别所有可能值所包含的信息期望值。熵越大，随机变量的不确定性就越大。当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。浅显的解释就是，这概率是我们根据数据数出来的。

信息增益表示得知特征X的信息而使得类Y的信息不确定性减少的程度。条件熵H(Y∣X) H(Y|X)H(Y∣X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy) H(Y|X)。

#### 集成学习

集成学习ensemble learning通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。个体学习器可以选择决策树或神经网络。集成时可以所有个体学习器属于同一类算法，即全是决策树，或全是神经网络；也可以来自不同的算法。

Ensemble Learning 是指将多个不同的 Base Model 组合成一个 Ensemble Model 的方法。它可以同时降低最终模型的 Bias 和 Variance。

集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。而个体学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。

常见的 Ensemble 方法有这么几种：

- Bagging：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。个体之间不存在强依赖关系，可并行生成；
- Boosting：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting 的原理。比 Bagging 效果好，但更容易 Overfit。个体学习器间存在强依赖关系，必须串行生成；
- Blending：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了；
- Stacking：相比 Blending，Stacking 能更好地利用训练数据。整个过程很像 Cross Validation。



#### 集成学习的调参

https://www.cnblogs.com/jasonfreak/p/5657196.html

目前，有三种常见的集成学习框架：bagging，boosting和stacking。sklearn提供了sklearn.ensemble库，支持众多集成学习算法和模型。恐怕大多数人使用这些工具时，要么使用默认参数，要么根据模型在测试集上的性能试探性地进行调参（当然，完全不懂的参数还是不动算了），要么将调参的工作丢给调参算法（网格搜索等）。这样并不能真正地称为“会”用sklearn进行集成学习。

学会调参是进行集成学习工作的前提。参数可分为两种，一种是影响模型在训练集上的准确度或影响防止过拟合能力的参数；另一种不影响这两者的其他参数。模型在样本总体上的准确度（后简称准确度）由其在训练集上的准确度及其防止过拟合的能力所共同决定，所以在调参时，我们主要对第一种参数进行调整，最终达到的效果是：模型在训练集上的准确度和防止过拟合能力的大和谐！

使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力。因此调参的目标就是为了达到整体模型的偏差和方差的大和谐。

我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。

整体模型的偏差和方差与基模型的偏差和方差息息相关。Random Forest（bagging）的子模型都拥有较低的偏差，整体模型的训练过程旨在降低方差，故其需要较少的子模型（n_estimators默认值为10）且子模型不为弱模型（max_depth的默认值为None），同时，降低子模型间的相关度可以起到减少整体模型的方差的效果（max_features的默认值为auto）。另一方面，Gradient Tree Boosting（boosting）的子模型都拥有较低的方差，整体模型的训练过程旨在降低偏差，故其需要较多的子模型（n_estimators默认值为100）且子模型为弱模型（max_depth的默认值为3），但是降低子模型间的相关度不能显著减少整体模型的方差（max_features的默认值为None）。

参数进一步可以粗略地分为两类了：控制整体训练过程的参数和基模型的参数，这两类参数都在影响着模型在训练集上的准确度以及防止过拟合的能力。

常见参数对模型性能的影响：

![img](https:////images2015.cnblogs.com/blog/927391/201607/927391-20160731184748841-69767136.jpg)



残差其实是最小均方损失函数的关于预测值的反向梯度。



由于bagging的训练过程旨在降低方差，而boosting的训练过程旨在降低偏差，过程影响类的参数能够引起整体模型性能的大幅度变化。一般来说，在此前提下，我们继续微调子模型影响类的参数，从而进一步提高模型的性能。

到此为止，我们终于知道需要调整哪些参数，对于单个参数，我们也知道怎么调整才能提升性能。然而，表示模型的函数F并不是一元函数，这些参数需要共同调整才能得到全局最优解。也就是说，把这些参数丢给调参算法（诸如Grid Search）咯？对于小数据集，我们还能这么任性，但是参数组合爆炸，在大数据集上，或许我的子子孙孙能够看到训练结果吧。实际上网格搜索也不一定能得到全局最优解，而另一些研究者从解优化问题的角度尝试解决调参问题。如坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。

无法对整体模型性能进行量化，也就谈不上去比较参数影响整体模型性能的程度。是的，我们还没有一个准确的方法来量化整体模型性能，只能通过交叉验证来近似计算整体模型性能。然而交叉验证也存在随机性，假设我们以验证集上的平均准确度作为整体模型的准确度，我们还得关心在各个验证集上准确度的变异系数，如果变异系数过大，则平均值作为整体模型的准确度也是不合适的。





#### 决策树的应用

https://www.cnblogs.com/pinard/p/6056319.html



### 随机森林

#### Random Forest

总的来说，我们应该生成尽量多的 Feature，相信 Model 能够挑出最有用的 Feature。但有时先做一遍 Feature Selection 也能带来一些好处，Feature Selection 最实用的方法也就是看 Random Forest 训练完以后得到的 Feature Importance 了。使用随机森林来得到每个特征的重要程度。

在训练时，我们主要希望通过调整参数来得到一个性能不错的模型。通常我们会通过一个叫做 Grid Search 的过程来确定一组最佳的参数。其实这个过程说白了就是根据给定的参数候选对所有的组合进行暴力搜索。

交叉验证 Cross Validation 是非常重要的一个环节。它让你知道你的 Model 有没有 Overfit，是不是真的能够 Generalize 到测试集上。

如果读者接触过决策树（Decision Tree）的话，那么会很容易理解什么是随机森林。随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习方法。随机森林的名称中有两个关键词，一个是“随机”，一个就是“森林”。“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这样的比喻还是很贴切的，其实这也是随机森林的主要思想--集成思想的体现。“随机”的含义我们会在下边部分讲到。

其实从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。

随机森林分类效果（错误率）与两个因素有关：

- 森林中任意两棵树的相关性：相关性越大，错误率越大；
- 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。

随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。



常见参数：

n_jobs=1：并行job个数。这个在ensemble算法中非常重要，尤其是bagging（而非boosting，因为boosting的每次迭代之间有影响，所以很难进行并行化），因为可以并行从而提高性能。1=不并行；n：n个并行；-1：CPU有多少core，就启动多少job

进行预测可以有几种形式：

predict_proba(x)：给出带有概率值的结果。每个点在所有label的概率和为1.  

predict(x)：直接给出预测结果。内部还是调用的predict_proba()，根据概率的结果看哪个类型的预测值最高就是哪个类型。  

predict_log_proba(x)：和predict_proba基本上一样，只是把结果给做了log()处理。  



### SKlearn

十分钟上手sklearn：特征提取，常用模型，交叉验证

https://www.jianshu.com/p/731610dca805

sklearn是一个Python第三方提供的非常强力的机器学习库，它包含了从数据预处理到训练模型的各个方面。

sklearn拥有可以用于监督和无监督学习的方法，一般来说监督学习使用的更多。sklearn中的大部分函数可以归为估计器(Estimator)和转化器(Transformer)两类。

- 估计器(Estimator)其实就是模型，它用于对数据的预测或回归；
- 转化器(Transformer)用于对数据的处理，例如标准化、降维以及特征选择等等。与估计器的使用方法类似。



#### Sklearn

scikit-learn非常简单，只需实例化一个算法对象，然后调用fit()函数就可以了，fit之后，就可以使用predict()函数来预测了，然后可以使用score(）函数来评估预测值和真实值的差异，函数返回一个得分。

Scikit-learn可以从一个或者多个数据集中学习信息，这些数据集合可表示为2维阵列，也可认为是一个列表。列表的第一个维度代表 样本 ，第二个维度代表 特征 （每一行代表一个样本，每一列代表一种特征）。

使用sklearn常见报错ValueError: Expected 2D array, got 1D array instead。这是由于在新版的sklearn中，所有的数据都应该是二维矩阵，哪怕它只是单独一行或一列（比如前面做预测时，仅仅只用了一个样本数据），所以需要使用.reshape(1,-1)进行转换。也就是说.reshape(1,-1)是二维数组。

sklearn中所有的模型都有四个固定且常用的方法：

~~~python
# 拟合模型
model.fit(X_train, y_train)
# 模型预测
model.predict(X_test)
# 获得这个模型的参数
model.get_params()
# 为模型进行打分
model.score(data_X, data_y) # 回归问题：以R2参数为标准 分类问题：以准确率为标准
~~~

如：最小二乘法线性回归 Linear Regression on Pandas DataFrame using Sci-kit Learn

sklearn.linear_model.LinearRegression 主要方法：

- fit(X, y, sample_weight=None) 训练模型，sample_weight是每条测试数据的权重，三个参数均以array格式传入；
- predict(X) 预测模型，将返回预测值y_pred；
- score(X, y, sample_weight=None) 评价模型，将返回一个小于1的得分，可能会小于0。



#### Sklearn.metrics

Sklearn.metrics模型评估方法中的classification_report函数用于显示主要分类指标的文本报告。在报告中显示每个类的精确度，召回率，F1值虚警率和精确度等。而这些指标都是基于混淆矩阵 (confusion matrix) 进行计算的。 混淆矩阵用来评价监督式学习模型的精确性，矩阵的每一列代表一个类的实例预测，而每一行表示一个实际的类的实例。

输入参数：

y_true：1维数组，或标签指示器数组/稀疏矩阵，目标值。

y_pred：1维数组，或标签指示器数组/稀疏矩阵，分类器返回的估计值。

输出参数：

列表左边的一列为分类的标签名，有4个标签代表四种层位，因此共4行。

右边包括：precision  /  recall / f1-score  / support 1.0 2.0 3.0 4.0

有张图片可以找到

准确率（precision）也就是被分类器检测到的数据中 分类正确的部分

召回率（recall）就是 正类中被分类正确的部分

而F1值虚警率就是 准确率和召回率的调和平均数，负样本被错误分为正样本的概率

support列为每个标签的出现次数

在实际应用中，如果是做搜索类的问题，那就是在保证召回率的情况下提升准确率

在做垃圾邮件检测之类的问题，就是要保证准确率的情况下提升召回率

具体也就是遇到具体问题看两者的权衡

如果两者都要求高，那就需要保证较高的F1 score

报错：F-score is ill-defined and being set to 0.0 in labels with no true samples.

原因：存在一些样本 label 为 y_true，但是你的 y_pred 并没有预测到。所以F-score没有计算这项 label， 因此这种情况下 F-score 就被当作为 0.0 了。但是又因为，要计算所有分类结果的平均得分就必须将这项得分为 0 的情况考虑进去，所以，scikit-learn出来提醒你，warning警告一下。



#### 特征选择

当特征特别多的时候，且有冗余的情况下，对特征进行选择不仅能使训练速度加快，还可以排除一些负面特征的干扰。sklearn的feature_seletion提供了它许多特征选取函数，目前包括单变量选择方法和递归特征消除算法。它们均为转化器，故在此不举例说明如何使用。

除了使用feature_seletion的方法选取特征外，我们也可以选择那些带有特征选择的模型进行选择特征，例如随机森林会根据特征的重要程度对特征打分。



#### Pineline

使用pineline可以按顺序构建从数据处理到和训练模型的整个过程。pineline中间的步骤必须转化器（对数据进行处理）。使用pineline的好处就是可以封装一个学习的过程，使得重新调用这个过程变得更加方便。中间的过程用多个二元组组成的列表表示。

上面的封装的估计器，会先用PCA将数据降至两维，在用逻辑回归去拟合。



#### 学习曲线与验证曲线

相同点是都是用于判别过拟合和欠拟合，y坐标都是训练集和交叉验证的准确率，x坐标不同。

- 学习曲线Learning Curve：x坐标是训练集大小，评估样本量和指标的关系，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。调用sklearn.learning_curve的learning_curve；

欠拟合情况：随着训练样本数增大，训练集得分和验证集得分收敛，并且两者的收敛值很接近，同时得分都不高。增加训练集，模型没有提升，也就表明模型的学习能力较弱，没有充分拟合数据。

过拟合情况：随着训练样本数增大，训练集得分和验证集得分相差还是很大。

![img](https://img-blog.csdn.net/20180706151327850?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwMzc0NTQ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- 验证曲线validation Curve：x坐标是模型参数取值，评估不同的模型参数与指标的关系，可用于判定过拟合与欠拟合。调用sklearn.learning_curve的validation_curve。





#### cross_validation

Cross-validation is used to evaluate estimator performance. The best parameters can be determined by grid search techniques.

三种，包括：

- cross_val_score

- cross_validate

- ross_val_predict

cross_validate function 与 cross_val_score：

The cross_validate function differs from cross_val_score in two ways:

- It allows specifying multiple metrics for evaluation.
- It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.

cross_val_predict 与 cross_val_score：

The function cross_val_predict has a similar interface to cross_val_score, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).



#### cross_val_score

主要参数包括：estimator、dataset、cv、scoring。

cross_val_score 的作用：

Using sklearn cross_val_score and kfolds to fit and help predict model

Q: So if I have a dataset with training and testing data, and I use the cross_val_score function with kfolds to determine the accuracy of the algorithm on my training data for each fold, is the model now fitted and ready for prediction on the testing data?

A: cross_val_score clones the estimator before fitting the fold training data to it. cross_val_score will give you output an array of scores which you can analyse to know how the estimator performs for different folds of the data to check if it overfits the data or not. 

You need to fit the whole training data to the estimator once you are satisfied with the results of cross_val_score, before you can use it to predict on test data.

scoring参数的默认值：

By default, the score computed at each CV iteration is the **score method** of the estimator. It is possible to change this by using the scoring parameter.  When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.

All scorer objects follow the convention that higher return values are better than lower return values. Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as neg_mean_squared_error which return the negated value of the metric.

采用K折交叉验证的方法来验证模型的表现，从而进行调参时，通常会使用sklearn.model_selection的cross_val_score方法，其中的scoring参数默认为 score方法的值，可以进行设置。值得注意的是，无论是对于分类问题还是回归问题，统一要求scoring参数越高越好。因此在回归问题中，neg_mean_squared 返回值为负值。

![img](https://img-blog.csdn.net/20180924201007560?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNTkwNjMx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)





### Boosting

提升(Boosting)是集成学习方法里的一个重要方法，其主要思想是将弱分类器组装成一个强分类器。在 PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

Boosting方法其实本质上采用的是加法模型（基函数的线性组合）与前向分布算法。以决策树为基函数的Boosting方法被称为提升树（Boosting tree）。前向分步算法(forward stagewise algorithm)是每一步都只学习一个基函数及其系数。

Boosting算法的两个核心问题：

- 在每一轮如何改变训练数据的权值或概率分布

通过提高那些在前一轮被弱分类器分错样本的权值，减小前一轮分对样本的权值，来使得分类器对误分的数据有较好的效果。

- 如何将弱分类器组合成一个强分类器

#### Boosting tree

Friedman 等在2000 年提出 Boosting Tree 算法，其构建并结合多个决策树学习器完成分类任务以提高分类精度，以获得比单个决策树学习器显著优越的泛化性能 。不再是基于单一学习器的方 法，一次学习完成后，不能对错误样本进行再学习。Boosting Tree 算法的基学习器为决策树 。决策树算法是一种具有树状结构的符号学习方法，其由结点和有向边组成，结点分为代表属性测试的内部结点和代表决策结果的叶结点。样本属性测试的规则是决策树算法的关键。CART算法是目前应用广泛的决策树方法，其使用基尼指数来选择划分属性。 

Boosting Tree 算法的基本思想是通过改变训练样本分布，学习多个决策树学习器 (基学习器 )，并将这些学习器进 行线性组合，以提高分类性能。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。不同问题的提升树学习算法，其主要区别在于损失函数不同。平方损失函数常用于回归问题，用指数损失函数用于分类问题，以及绝对损失函数用于决策问题。

通过加法模型将弱分类器进行线性组合，比如 AdaBoost 通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。



#### AdaBoost Tree

自己实现一遍

http://www.17bigdata.com/adaboost-算法/

https://blog.csdn.net/weixin_28750267/article/details/81056866#commentBox



AdaBoost算法是损失函数为指数函数时的Boosting算法，也就是说AdaBoost是Boosting算法的一种实现。AdaBoost是英文”Adaptive Boosting“（自适应增强）的缩写。使用指数损失函数而不用均方误差损失函数的原因是均方误差损失函数对分类问题的效果不好。指数损失可以作为分类任务 0 1 损失的替代函数，因为它连续可微，就用它来替代 0 1 损失函数作为优化目标。AdaBoost算法是前向分步加法算法的特例。

在基本的AdaBoost算法中，每个弱分类器都有权重，弱分类器预测结果的加权和形成了最终的预测结果。训练时，训练样本也有权重，在训练过程中动态调整，被前面的弱分类器错分的样本会加大权重，因此算法会关注难分的样本。

当我们对Adaboost调参时，主要要对两部分内容进行调参，第一部分是对我们的Adaboost的框架进行调参， 第二部分是对我们选择的弱分类器进行调参。两者相辅相成。

AdaBoostClassifier和AdaBoostRegressor的主要框架参数：

- base_estimator：AdaBoostClassifier和AdaBoostRegressor都有，即我们的弱分类学习器或者弱回归学习器。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。默认是决策树，即AdaBoostClassifier默认使用CART分类树DecisionTreeClassifier，而AdaBoostRegressor默认使用CART回归树DecisionTreeRegressor。另外有一个要注意的点是，如果我们选择的AdaBoostClassifier算法是SAMME.R，则我们的弱分类学习器还需要支持概率预测，也就是在scikit-learn中弱分类学习器对应的预测方法除了predict还需要有predict_proba；
- n_estimators：AdaBoostClassifier和AdaBoostRegressor都有，就是我们的弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑；
- learning_rate：AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重缩减系数νν，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为fk(x)=fk−1(x)+ναkGk(x)fk(x)=fk−1(x)+ναkGk(x)。νν的取值范围为0<ν≤10<ν≤1。对于同样的训练集拟合效果，较小的νν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的νν开始调参，默认是1。

AdaBoostClassifier和AdaBoostRegressor弱学习器主要参数：

讨论默认的决策树弱学习器的参数。即CART分类树DecisionTreeClassifier和CART回归树DecisionTreeRegressor。

- max_features: 划分时考虑的最大特征数。可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑log2Nlog2N个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间；

- max_depth: 决策树最大深。默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。



#### 梯度提升树（GBDT）



#### XGBoost

极端梯度提升(XGBoost，eXtreme Gradient Boosting)算法是对梯度提升算法的优化，具有更高的模型精度与泛化能力，并加入了防止过拟合的正则项，且支持并行计算。



## 深度学习

深度学习是机器学习的一个大分支，深度学习的概念源于对人工神经网络的研究，深度学习的基本结构（也即模型）是深度神经网络。

深度学习指的是一类广泛的机器学习技术和架构，其特点是采用多层的非线性信息处理方法，这种方法在本质上是分层的。

一般将深度机器学习技术分为：

- （有监督学习的深度网络）深度判别式模型（深度神经网络DNN、递归神经网络RNN、卷积神经网络CNN等），目标类别标签总是以直接或间接形式给出；
- （无监督的或生成式学习的深度网络）生成式/无监督模型（受限玻尔兹曼机RBM、深度信念网络DBN、深度玻尔兹曼机DBM、正则化的自编码器等）。



BP神经网络最好的一篇科普文：https://blog.csdn.net/weixin_40432828/article/details/82192709

BP神经网络是后续复杂网络的基础。



### 入门

#### 深度学习特征工程

使用深度学习的话，特征工程就没那么重要了，特征只需要做些预处理就可以了，因为它可以自动完成传统机器学习算法中需要特征工程才能实现的任务，特别是在图像和声音数据的处理中更是如此，但模型结构会比较复杂，训练较为麻烦。另一个方面，虽然深度学习让我们可以省去特征工程这一较为繁琐的过程，但也让我们失去了对特征的认识，如特征的重要性等。因为各自的特点和要求，因此会有相对优势的偏向。如各自的优势领域：

- 深度学习：图像处理，自然语言处理等，因为图像、语言、文本都较难进行特征工程，交给深度学习是一个很好的选择。

- 机器学习：金融风控，量化分析，推荐系统，广告预测等，因为需要较好的可解释性，会更多的采用传统机器学习方法。

与机器学习不同的是，深度学习不用像传统机器学习那样人为合成高级复杂特征，只需利用人类的先验知识处理一阶特征，后面深度学习会自己学习到相关的复杂特征。不同的数据有不同的数据类型，而不同数据类型的数据处理方式也不同。

离散型与连续型特征最大的区别在于，连续型处理后结果还只是一个特征，而离散型处理后会生成多个特征，这一列有多少个key(字段可能的值)就会抽取出多少个特征，单变量离散化后的每个变量有各自的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。连续和离散可以互相转化。不同的特征处理方式适用于不同的模型。如海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。LR是线性模型， 我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。

1. 离散型数据

比如 性别：男，女；比如学历：高中，大学，硕士，博士。一般来说，对离散型的数据有以下几种处理方式： 

- One-Hot Encoding(若特征种类很多，高稀疏化)
- Hash Encoding(低稀疏，高压缩)
- embedding(注意比较与Hash Encoding 的区别,NLP常用)
- 基于计数的encoding
- 特殊情况：当输入是0,1的二值信号，而且0是对所模拟的模型是有作用的，那么这时候采用flatten的战术，即0变成0,1  ，1变成1,0 。比如原来64个输入特征，flatten后变成128个特征，实例参考建模攻击PUF的相关项目。注意flatten和one-hot有本质区别，一个是扩展特征的长度，一个是扩展特征的维度。

2. 连续型数据

 除了分类这样的离散数据，我们也会碰到诸如身高，学习成绩，资金等连续型的数据。对于连续型数据，有以下的处理方式： 

- 缺失数据处理：在收集来的数据中，往往会出现某处数据为空不存在的情况。一般处理方式有填0处理，填NAN处理，平均值或中位值处理等。没有特殊情况的话，一般不推荐填0处理，0和空相差的意义较大，0是有意义的。
- 归一化：**归一化与标准化的区别**：标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。在深度学习中，也会有batch norm的操作。其实就是把每一层的输出都进行归一化处理后再交给下一层计算。
- 离散化：将连续值分区，某个分区内的数据均为某个分类值。例如个人资产为连续值，处理后 个人资产小于100W为普通阶级，个人资产100W-1000W为中产阶级，个人资产1000W以上为富人阶级等。某些情况下若取值跨度太大或者太小，可以取对数或者开方平方等处理后再离散化。

3. 时间数据

时间数据本质上也还是一种连续数据，但是有一些特殊的地方，比如时区，比如周期性。因此处理时尤其要注意特殊节假日，时区等问题。

4. 空间数据

例如经纬度，地址，邮编等，比较不规范，根据具体情况进行处理



#### 感知机

感知机也被用来特指单层人工神经网络，以区别于多层感知机。

神经网络是模仿大脑的神经元，当外界刺激达到一定的阈值时，神经元才会受刺激，影响到下一个神经元。

神经网络中常见参数：

- 𝒙𝒊 来自第𝑖个神经元的输入；
- 𝒘𝒊 第𝑖个神经元的连接权重（突触）；
- 𝜽 阈值(threshold)或称为偏置（bias）；
- 𝑓 为激活函数（细胞体），常用：sigmoid，relu，tanh等等。

其中权重值和偏置值构成了这个网络的参数。对于一个神经元来说，有i个输入，每一个输入都对应一个权重（w），神经元具有一个偏置（阈值），将所有的i*w求和后减去阈值得到一个值，这个值就是激活函数的参数，激活函数将根据这个参数来判定这个神经元是否被激活。在人工神经网络中，规定神经元函数只能对输入变量（指向它的节点的值）线性组合后的结果进行一次非线性变换（激活函数）。

Note that the weights ![\left( w_1, w_2, \cdots, w_n \right)](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-0e4fbd3474985a8ad61a56ccf99e10e0_l3.png) and the bias ![b](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-173b5c22c5ff048f406b49d642c938f4_l3.png) transform the input signal linearly. The activation, on the other hand, transforms the signal non-linearly and it is this non-linearity that allows us to learn arbitrarily complex transformations between the input and the output.

![neuron](https://www.learnopencv.com/wp-content/uploads/2017/10/neuron-diagram.jpg)感知机无法处理线性不可分问题。什么叫线性不可分？举个例子，二维平面上有两类点，无法用一条一维直线将其完全分开；N维空间中有两类点，用N-1维的分隔超平面无法将两类点完全分开，就叫做线性不可分。

线性不可分如异或问题(XOR）。异或是一种基于二进制的位运算，用符号XOR 表示(Python中的异或操作符为 ^ )，其运算法则是对运算符两侧数的每一个二进制位，同值取0，异值取1。XOR gate如图所示，线性不可分。

![img](https://www.learnopencv.com/wp-content/uploads/2017/10/xor.png)



#### BP

BP神经网络是一种按照误差反向传播算法训练的多层前馈神经网络，是目前应用最广泛的神经网络。首先从名称中可以看出，BP神经网络可以分为两个部分，BP和神经网络。

一个神经网络包括输入层、隐含层（中间层）和输出层。输入层神经元个数与输入数据的维数相同，输出层神经元个数与需要拟合的数据个数相同，隐含层神经元个数与层数就需要设计者自己根据一些规则和目标来设定。在深度学习出现之前，隐含层的层数通常为一层，即通常使用的神经网络是3层网络。

普通的网络中权重固定，没有反向传播信息，因此它不是神经网络。神经网络，就是一类相对复杂的计算网络。[神经网络是一个信息可以反向传播的网络](https://blog.csdn.net/weixin_40432828/article/details/82192709)，最早的BP网络就是这一思想的体现。

反向传播是什么意思？

举一个通俗的例子，猜数字：当我提前设定一个数值 50，让你来猜，我会告诉你猜的数字是高了还是低了。你每次猜的数字相当于一次信息正向传播给我的结果，而我给你的提示就是反向传播的信息，往复多次，你就可以猜到我设定的数值 50 。 这就是典型的反向传播，即根据输出的结果来反向的调整模型，只是在实际应用中的BP网络更为复杂和数学，但是思想很类似。在BP神经网络中，反向传播的数据是误差，也被称为损失Loss。更新的数据是权值与阈值。

反向传播算法（Back-Propagation，BP）是一种有监督的学习算法，是基于梯度下降策略实现的，以目标函数的负梯度方向对网络的权值和阈值进行调整，使BP神经网络的误差平方和最小。通过链式法则，梯度是一层一层向后传递。

**BP算法包括两个过程：输入信号正向传播与误差反向传播**，在正向传播的过程中，输入信号从输入层经过隐含层逐层处理后传到输出层，计算实际输出与期望输出的误差，一旦在输出层不能得到期望的结果，则将误差从输出层开始逐层向前反向传播，并将误差分配给每一层的所有神经元，得到各层神经元的误差信号，并将此误差作为更新网络权值和阈值的依据。这两个过程循环进行，通过不断地调整网络权值和阈值，直到BP网络在所有训练样本的实际输出与期望输出之间的误差在一个很小的范围之内。BP神经网络的权值和阈值的初值一般是随机产生的。

缺点：对训练样本的依赖性大、训练参数的选择不准确、局部最优问题。



正向学习+反向传播

实现神经网络  https://blog.csdn.net/weixin_40432828/article/details/82192709

梯度矩阵 https://blog.csdn.net/ppp8300885/article/details/78492166



![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20171109182246852?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHBwODMwMDg4NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)





#### CNN



如果人们选择图像中的连续范围作为池化区域，并且只是池化相同(重复)的隐藏单元产生的特征，那么，这些池化单元就具有平移不变性 (translation invariant)。这就意味着即使图像经历了一个小的平移之后，依然会产生相同的 (池化的) 特征。

在很多任务中 (例如物体检测、声音识别)，我们都更希望得到具有平移不变性的特征，因为即使图像经过了平移，样例(图像)的标记仍然保持不变。例如，如果你处理一个MNIST数据集的数字，把它向左侧或右侧平移，那么不论最终的位置在哪里，你都会期望你的分类器仍然能够精确地将其分类为相同的数字。

个人理解：

牛逼的球队不管抽签到什么小组，最后都能进决赛圈。



卷积层与全连接层

major problem with a fully connected classifier

In our training dataset, all images are centered. If the images in the test set are off-center, then the MLP（Multilayer Perceptron） approach fails miserably. We want the network to be **Translation-Invariant**.

Another major problem with a fully connected classifier is that the number of parameters increases very fast since each node in layer L is connected to a node in layer L-1. So it is not feasible to design very deep networks using an MLP structure alone.

Both the above problems are solved to a great extent by using Convolutional Neural Networks.

与只用全连接层相比，卷积层的主要优点是参数共享和稀疏连接，这使得卷积操作所需要学习的参数数量大大减少。



卷积神经网络（Convolutional Neural Network，CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks）。卷积神经网络由卷积层（Convolutions Layer）、池化层（Pooling Layer）和全连接层构成。全连接层在最后，前面是若干卷积层和池化层，每个卷积层后面跟一个池化层，如此重复。池化层又叫子采样层，也即Subsample Layer（feature extractor）。

卷积神经网络是一个专门针对图像识别问题设计的神经网络。它模仿人类识别图像的多层过程：瞳孔摄入像素；大脑皮层某些细胞初步处理，发现形状边缘、方向；抽象判定形状（如圆形、方形）；进一步抽象判定（如判断物体是气球）。

[keras实现mnist数据集手写数字识别](https://www.cnblogs.com/ncuhwxiong/p/9774515.html)

[卷积](https://www.jianshu.com/p/e15bda2e4ebf)在数学上被定义为作用于两个函数f 和g 上的操作来生成一个新的函数z。这个新的函数是原有两个函数的其中一个（比如f）在另一个（比如g）的值域上的积分（连续）或者加权平均（离散）。

卷积之所以叫“卷”，是因为卷起来后那点的函数值正好x+y为定值的一条直线上函数值的积分。

具体过程是先将g函数取值反转，并在此基础上加一个时间抵消项，这样在新的值域上的函数g 就是在这个轴上移动的窗口。从负无穷大的时间开始，一直移动到正无穷大。在两个函数取值有交接的地方，找出其积分，换句话说，就是对函数f 计算其在一个平滑移动窗口的加权平均值，而这个权重就是反转后的函数g 在同样值域中的相应取值。这样得到的波形就是这两个函数的卷积。

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Comparison_convolution_correlation.svg/400px-Comparison_convolution_correlation.svg.png)

卷积应用的场景，如两枚骰子点数加起来和为4的概率为：

![[公式]](https://www.zhihu.com/equation?tex=f%281%29g%283%29%2Bf%282%29g%282%29%2Bf%283%29g%281%29%5C%5C)

![img](https://pic3.zhimg.com/80/v2-0cdabcc04398ea723aa6e47e05072e5c_hd.jpg)

符合卷积的定义，把它写成标准的形式就是：

![[公式]](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%28f%2Ag%29%284%29%3D%5Csum+_%7Bm%3D1%7D%5E%7B3%7Df%284-m%29g%28m%29%5C%5C)





Convolution can be thought of as a weighted sum between two signals ( in terms of signal processing jargon ) or functions ( in terms of mathematics ). In image processing, to calculate convolution at a particular location ![(x, y)](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-d5d613a288ee226f2b09be76bd01fb61_l3.png), we extract ![k](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-9dc53f8ecc1bcf15020c6df4c12f1c27_l3.png) x ![k](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-9dc53f8ecc1bcf15020c6df4c12f1c27_l3.png) sized chunk from the image centered at location ![(x,y)](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-722d11727d8db5dd1cf6f211fd791bc4_l3.png). We then multiply the values in this chunk element-by-element with the convolution filter (also sized ![k](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-9dc53f8ecc1bcf15020c6df4c12f1c27_l3.png) x ![k](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-9dc53f8ecc1bcf15020c6df4c12f1c27_l3.png)) and then add them all to obtain a single output. That’s it! Note that ![k](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-9dc53f8ecc1bcf15020c6df4c12f1c27_l3.png) is termed as the kernel size. 

马同学万岁！

卷积可用于平滑算法，操作动图如下。比如我要平滑 ![[公式]](https://www.zhihu.com/equation?tex=a_%7B1%2C1%7D) 点，就在矩阵中，取出 ![[公式]](https://www.zhihu.com/equation?tex=a_%7B1%2C1%7D) 点附近的点组成矩阵 ![[公式]](https://www.zhihu.com/equation?tex=f) ，和 ![[公式]](https://www.zhihu.com/equation?tex=g) 进行卷积计算后，再填回去。运用卷积时， ![[公式]](https://www.zhihu.com/equation?tex=g) 虽然和 ![[公式]](https://www.zhihu.com/equation?tex=f) 同维度，但下标有点不一样。图片处理时通常g取正态分布矩阵。

![img](https://pic3.zhimg.com/50/v2-c658110eafe027eded16864fb6a28f46_hd.gif)

写成卷积公式就是：

![image-20190718115556812](/Users/kai/Library/Application Support/typora-user-images/image-20190718115556812.png)

这样相当于实现了 ![[公式]](https://www.zhihu.com/equation?tex=g) 这个矩阵在原来图像上的划动（准确来说，下面这幅图把 ![[公式]](https://www.zhihu.com/equation?tex=g) 矩阵旋转了 ![[公式]](https://www.zhihu.com/equation?tex=180%5E%5Ccirc) ）：

旋转的目的是将卷积运算原本不是对应位置乘积之和，变成对应位置乘积之和，方便观察。

![img](https://pic1.zhimg.com/50/v2-15fea61b768f7561648dbea164fcb75f_hd.gif)

三通道卷积

![7c226c1fdb52fcdf643374901021e8b0](C:\Users\KZ\Documents\WeChat Files\longzhu4206\FileStorage\File\2019-08\7c226c1fdb52fcdf643374901021e8b0.gif)



The convolution kernel is slid over the entire matrix to obtain an **activation map**. The same filters are slid over the entire image to find the relevant features. This makes the CNNs Translation Invariant. The activation map of one layer serves as the input to the next layer. Unlike traditional neural nets where weights and biases of neurons are independent of each other, in case of CNNs the neurons corresponding to one filter in a layer share the same weights and biases.



[卷积是如何工作的？](https://www.zhihu.com/question/52668301)每个过滤器可以被看成是特征标识符（ feature identifiers）。这里的特征指的是例如直边缘、原色、曲线之类的东西。过滤器滑过所有位置后将得到一个数组，我们称之为激活映射（activation map）或特征映射（feature map）。也就是说，卷积层使用过滤器对特征（如曲线边界）进行检测/筛选，返回激活映射作为下一层的输入。在新的卷积层中进一步实现更高级特征的激活映射（低级特征如边缘与曲线，高级特征如手与耳朵）。过滤器越多，激活映射的深度越大，我们对输入内容的了解也就越多。越深入网络，过滤器的感受野（[Receptive field](https://iphysresearch.github.io/posts/receptive_field.html)）越大，意味着它们能够处理更大范围的原始输入内容（或者说它们可以对更大区域的像素空间产生反应）。

检测高级特征之后，网络最后的完全连接层就更是锦上添花了。简单地说，这一层处理输入内容（该输入可能是卷积层、ReLU 层或是池化层的输出）后会输出一个 N 维向量，N 是该程序必须选择的分类数量。例如，如果你想得到一个数字分类程序，如果有 10  个数字，N 就等于 10。这个 N 维向量中的每一数字都代表某一特定类别的概率（softmax）。完全连接层观察上一层的输出（其表示了更高级特征的激活映射）和哪一分类最为吻合和拥有怎样的特定权重，因此当计算出权重与上一层之间的点积后，将得到不同分类的正确概率。







[卷积操作分为一维、二维和三维](https://www.cnblogs.com/szxspark/p/8445406.html)，对应的方法分别是Conv1D、Conv2D 和Conv3D，这些方法有同样的选项，只是作用于不同维度的数据上，因此适用于不同的业务场景。

一维卷积通常被称为时域卷积，因为其主要应用在以时间排列的序列数据上，常用于自然语言处理领域，其使用卷积核对一维数据的邻近信号进行卷积操作来生成一个张量。二维卷积通常被称为空域卷积，一般应用在与图像相关的输入数据上，也是使用卷积核对输入数据进行卷积操作的。三维卷积也执行同样的操作，常用于医学领域（CT影响），视频处理领域（检测动作及人物行为）。

卷积层常见参数包括：

- filters：卷积滤镜（核）输出的维度，要求整数。过滤器/滤镜的个数。

- kernel_size：卷积核的空域或时域窗长度。要求是整数或整数的列表，或者是元组。如果是单一整数，则应用于所有适用的维度。过滤器的尺寸，感受野的大小。在卷积操作中，过滤器（又称核）的大小通常为奇数，如3x3，5x5。

- strides：卷积在宽或者高维度的步长。要求是整数或整数的列表，或者是元组。过滤器每次移动的距离，默认为1。
- padding：填充，补齐策略，取值为valid、same 或causal。causal 将产生因果（膨胀的）卷积，即output[t] 不依赖于input[t+1:]，在不能违反时间顺序的时序信号建模时有用。请参考WaveNet: A Generative Model for Raw Audio, section 2.1.。valid代表只进行有效的卷积，即对边界数据不处理。same 代表保留边界处的卷积结果，通常会导致输出shape 与输入shape 相同。
- data_format：数据格式，取值为channels_last 或者channels_first。这个选项决定了数据维度次序，其中channels_last 对应的数据维度次序是（批量数，高，宽，频道数），而channels_first 对应的数据维度次序为（批量数，频道数，高，宽）。



padding

在没有padding的情况下，经过卷积操作，输出的数据维度会减少。以二维卷积为例，输入大小 n×n，过滤器大小f×f，卷积后输出的大小为(n−f+1)×(n−f+1)。而在网络的早期层中，我们想要尽可能多地保留原始输入内容的信息，这样我们就能提取出那些低层的特征。为做到这点，我们可以对这个层应用大小为 2 的零填充（zero padding）。零填充在输入内容的边界周围补充零。如对于大小为32 x 32 x 3 的输入层，如果我们用两个零填充，就会得到一个 36 x 36 x 3 的输入层。

计算任意给定卷积层的输出的大小的公式是：

![img](https://pic4.zhimg.com/50/v2-47d83d1bb16072a57d5c1fbb49991d68_hd.jpg)

其中 O 是输出尺寸，W 是输入尺寸，K 是过滤器尺寸，P 是填充，S 是步幅。计算结果向下取整。

在每个卷积层之后，通常会立即应用一个非线性层（或激活层）。其目的是给一个在卷积层中刚经过线性计算操作（只是数组元素依次（element wise）相乘与求和）的系统引入非线性特征。

在几个 ReLU 层之后，程序员也许会选择用一个池化层（pooling layer）。它同时也被叫做下采样（downsampling）层。在这个类别中，也有几种可供选择的层，最受欢迎的就是最大池化（ max-pooling）。它基本上采用了一个过滤器（通常是 2x2 的）和一个同样长度的步幅。然后把它应用到输入内容上，输出过滤器卷积计算的每个子区域中的最大数字。池化操作不需要对参数进行学习，只是神经网络中的静态属性。

Pooling layer is mostly used immediately after the convolutional layer to reduce the spatial size (only width and height, not depth). This reduces the number of parameters, hence computation is reduced. Using fewer parameters avoids overfitting. The most common form of pooling is Max pooling where we take a filter of size p and apply the maximum operation over the sized part of the image. The most common pooling operation is done with the filter of size 2×2 with a stride of 2. It essentially reduces the size of input by half.

![illustration of max pooling](https://www.learnopencv.com/wp-content/uploads/2017/11/max-pooling-demo.jpg)

池化层还有其他选择，比如平均池化（average pooling）和 L2-norm 池化 。这一层背后的直观推理是：一旦我们知道了原始输入（这里会有一个高激活值）中一个特定的特征，它与其它特征的相对位置就比它的绝对位置更重要。可想而知，这一层大幅减小了输入卷的空间维度（长度和宽度改变了，但深度没变）。这到达了两个主要目的。第一个是权重参数的数目减少到了75%，因此降低了计算成本。第二是它可以控制过拟合（overfitting）。



池化(Pooling)是在卷积神经网络中对图像特征的一种处理，通常在卷积操作之后进行。池化的目的是为了计算特征在局部的充分统计量，从而降低总体的特征数量，防止过度拟合和减少计算量。池化技术就是对卷积出来的特征分块（比如分成新的m  n 个较大区块）求充分统计量，比如本块内所有特征的平均值或者最大值等，然后用得到的充分统计量作为新的特征。

当然，这个操作依赖于一个假设，就是卷积之后的新特征在局部是平稳的，即在相邻空间内的充分统计量相差不大。对于大多数应用，特别是与图像相关的应用，这个假设可以认为是成立的。



Dropout 层将丢弃（drop out）该层中一个随机的激活参数集，即在前向通过（forward pass）中将这些激活参数集设置为 0。简单如斯。既然如此，这些简单而且似乎不必要且有些反常的过程的好处是什么？在某种程度上，这种机制强制网络变得更加冗余。这里的意思是：该网络将能够为特定的样本提供合适的分类或输出，即使一些激活参数被丢弃。此机制将保证神经网络不会对训练样本「过于匹配」，这将帮助缓解过拟合问题。另外，Dropout 层只能在训练中使用，而不能用于测试过程，这是很重要的一点。



图像在计算机中是一堆按顺序排列的数字，数值从0到255，0表示最暗，255表示最亮。为保留结构特征，通常选用矩阵的表示方式：28×28的矩阵。这是只有黑白色的灰度图。

而更普遍的图片表达方式是RGB颜色模型，即红绿蓝三原色的色光以不同的比例相加，以产生各种各样的色光。在电脑中，一张图片构成一个“长方体”。可用宽高深来描述，如图所示。输入的是shape为（width,heigtht,depth）的三维张量。

![img](https://pic4.zhimg.com/80/v2-0d24890b2e0d73f4ce4ad17ebfb2d0c4_hd.jpg)



卷积操作中过滤器的channel数量必须与输入数据的channel数量相同。在卷积的过程中，过滤器与数据在channel方向分别卷积，之后将卷积后的数值相加。当输入数据维度为14×14×3， 过滤器维度为5×5×3时，执行10×10次3个数值相加的操作，最终输出的数据维度为10×10。

以上都是在过滤器数量为1的情况下所进行的讨论。如果将过滤器的数量增加至16，即16个大小为10×10×3的过滤器，最终输出的数据维度就变为10×10×16。可以理解为分别执行每个过滤器的卷积操作，最后将每个卷积的输出在第三个维度（channel 维度）上进行拼接。



flatten layer

The final droupout layer has an output of 2x2x64. This has to be converted to a single array. This is done by the flatten layer which converts the 3D array into a 1D array of size 2x2x64 = 256. The final layer has 10 nodes since there are 10 classes.



一维卷积



假设输入数据为 1 条语句，共 8 个词。每个词对应于 16 维向量。利用的 filter 也一定是对应于 16 维的，filter 数量为 1，如果选择长度为 5 的 filter，则经过卷积输出长度为 (8−5+1)=4。由于只有 1 条输入，则输出的 shape 为 1×4×1。







sigmod有个缺点，sigmoid函数反向传播时，很容易就会出现梯度消失,在接近饱和区的时候，导数趋向0，会变得非常缓慢。因此，在优化器选择时选用Adam优化器。

Adam 也是基于梯度下降的方法，但是每次迭代参数的学习步长都有一个确定的范围，不会因为很大的梯度导致很大的学习步长，参数的值比较稳定。有利于降低模型收敛到局部最优的风险，而SGD容易收敛到局部最优。

修正线性单元(Rectified linear unit,ReLU)解决了sigmoid梯度下降慢，深层网络的信息丢失的问题。









#### 神经网络常见参数

1. 神经网络的batchsize、iteration、epoch：

- batchsize：中文翻译为批大小（批尺寸）。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练。每经过batchsize个sample更新一次权重，defult 32；
- iteration：中文翻译为迭代，1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过；
- epoch：迭代次数，1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递。

举个例子，训练集有1000个样本，batchsize=10，那么：训练完整个样本集需要：100次iteration，1次epoch。

2. 神经网络的loss、learning_rate、Regularization：

损失函数(loss)：预测值y与已知答案y_的差距。神经网络优化目标即找到适合的w以减小loss, 有三种减小loss的方法：

- 均方误差mse(Mean Squared Error)
- 自定义损失函数
- 交叉熵ce(Cross Entropy)，表示两个概率分布之间的距离。

学习率(learning_rate)：参数每次更新幅度。学习速率，即在梯度下降中的步伐大小。

W_(n+1)	= W_(n) - learning_rate ▽，其中▽表示损失函数的梯度(导数)。

正则化（Regularization）：正则化是在损失函数中引入模型复杂度指标, 利用给W加权值, 弱化了训练数据的噪声(一般不正则化b)，用于缓解过拟合。

loss = loss(y与y_) + 超参数REGULARIZER * loss(w)

其中：loss()为参数的损失函数

超参数REGULARIZER给出参数w在总loss中的比例(正则化权重)

w为需要正则化的参数



#### 过拟合

https://blog.csdn.net/xjcvip007/article/details/52801216

关于过拟合问题的讨论：

解决方法大致只有两种，第一种就是添加dropout层，dropout可以放在很多类层的后面，用来抑制过拟合现象，常见的可以直接放在Dense层后面，对于在Convolutional和Maxpooling层中dropout应该放置在Convolutional和Maxpooling之间，还是Maxpooling后面的说法，我的建议是试！这两种放置方法我都见过，但是孰优孰劣我也不好说，但是大部分见到的都是放在Convolutional和Maxpooling之间。关于Dropout参数的选择，这也是只能不断去试，但是我发现一个问题，在Dropout设置0.5以上时，会有验证集精度普遍高于训练集精度的现象发生，但是对验证集精度并没有太大影响，相反结果却不错，我的解释是Dropout相当于Ensemble，dropout过大相当于多个模型的结合，一些差模型会拉低训练集的精度。当然，这也只是我的猜测，大家有好的解释，不妨留言讨论一下。 

当然还有第二种就是使用参数正则化，也就是在一些层的声明中加入L1或L2正则化系数。



解决方法：

1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。

2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。

3）采用正则化方法。

4）采用dropout方法。



https://blog.csdn.net/df19900725/article/details/82973049#commentBox

解决过拟合问题有两个方向：降低参数空间的维度或者降低每个维度上的有效规模（effective size）。降低参数数量的方法包括greedy constructive learning、剪枝和权重共享等。降低每个参数维度的有效规模的方法主要是正则化，如权重衰变（weight decay）和早停法（early stopping）等。



#### 正则化

正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0。

In machine learning, regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.

https://blog.csdn.net/jinping_shi/article/details/52433975



正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对余的范数。但是在机器学习中一般使用L2正则，下面看具体的原因。

L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。L2范数是指向量各元素的平方和然后求平方根。

两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似， 而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。

L2范数可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。还有就是看到有人说L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题（具体这儿我也不是太理解）。



Different Regularization techniques in Deep Learning

- L2 and L1 regularization
- Dropout 随机失活

Dropout是指对于神经网络单元按照一定的概率将其暂时从网络中丢弃，从而解决过拟合问题。在学习过程中通过将隐含层的部分权重或输出随机归零，降低节点间的相互依赖性（co-dependence ）从而实现神经网络的正则化（regularization），降低其结构风险（structural risk）。

- Data augmentation 数据增强：https://blog.csdn.net/u010801994/article/details/81914716
- Early stopping



#### 前馈神经网络

https://www.learnopencv.com/understanding-feedforward-neural-networks/

Feedforward Neural Networks, also known as Deep feedforward Networks or Multi-layer Perceptrons. They form the basis of many important Neural Networks being used in the recent times, such as Convolutional Neural Networks ( used extensively in computer vision applications ), Recurrent Neural Networks ( widely used in Natural language understanding and sequence learning) and so on. 

1. Feedforward Neural Networks

- Input Layer：This is the first layer of a neural network. It is used to provide the input data or features to the network；

- Output Layer：This is the layer which gives out the predictions；

- Hidden Layer：A feedforward network applies a series of functions to the input. By having multiple hidden layers, we can compute complex functions by cascading simpler functions. The type of hidden layer distinguishes the different types of Neural Networks like CNNs, RNNs etc. The number of hidden layers is termed as the depth of the neural network. 

2. How does the network learn?

The training samples are passed through the network and the output obtained from the network is compared with the actual output. This error is used to change the weights of the neurons such that the error decreases gradually. This is done using the **Backpropagation algorithm**, also called backprop. Iteratively passing batches of data through the network and updating the weights, so that the error is decreased, is known as **Stochastic Gradient Descent ( SGD )**. The amount by which the weights are changed is determined by a parameter called Learning rate. 

A Neural Network with a single hidden layer with nonlinear activation functions is considered to be a **Universal Function Approximator** ( i.e. capable of learning any function )（泛逼近器）. 

3. Regularization

A multilayer network can learn nonlinear decision boundaries. But the network will be fragile in the presence of noise. This phenomenon is called **Overfitting**. In such cases, the training loss is decreasing but the test loss is increasing. Also, you can see that some weights have become very large ( very thick connections or you can see the weights if you hover above the connections ). This can be rectified by putting some restrictions on the values of weights ( like not allowing the weights to become very high ). This is called **Regularization**. We impose restrictions on the other parameters of the network. 



#### 激活函数

激活函数（activation function）可以使得模型加入非线性因素的。解决非线性问题有两个办法：线性变换、引入非线性函数。

https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/

1. How does the network learn?

通过反向传播算法更新权重与偏置，使用梯度下降法最小化损失函数。

The goal of the training process is to find the weights and bias that minimise the loss function ( ![J](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-bb8905e363b035a97b58399e113c48e8_l3.png) ) over the training set. The idea of using the partial derivatives（偏导数） of a function to iteratively find its local minimum is called the **gradient descent**. In Artificial neural networks the weights are updated using a method called **Backpropagation**.

When the cost function is convex ( i.e. shaped like a bowl ), there is a principled way to iteratively find the best weight by a method called **Gradient Descent**. We are moving down (descending) the curve based on the slope (gradient). When you reach the bottom of the bowl, the gradient or slope goes to zero and that completes your training. These bowl-shaped functions are technically called **convex functions**（凸函数）.

The algorithm used for estimating the gradient of the cost function is called **Backpropagation**. It is simply repetitive application of the **chain rule**.

2. activation function

激活函数用于实现非线性化并将数据映射到有限范围。

The activation function takes the decision of whether or not to pass the signal. The activation function is used as a decision making body at the output of a neuron. The neuron learns **Linear or Non-linear decision boundaries** based on the activation function. It also has a normalizing effect on the neuron output which prevents the output of neurons after several layers to become very large, due to the cascading effect. 

**Linear Activation Function** is a simple linear function of the form ![f(x) = x](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-5837b5b8d8f00eb7c80d172f3e04db9f_l3.png)Basically, the input passes to the output without any modification. **Non-liner Activation Functions** have a non-linear equation that governs the mapping from inputs to outputs. Without the non-linearity introduced by the activation function, multiple layers of a neural network are equivalent to a single layer neural network. 

3. Non-liner Activation Functions

There are three most widely used non-liner activation functions.

- sigmoid：It is also known as Logistic Activation Function. It maps the input ( x axis ) to values between 0 and 1. It is also used in the output layer where our end goal is to predict probability. The three major drawbacks of sigmoid are: **Vanishing gradients**/**Not zero centered**/**Computationally expensive**；

- Tanh：It is also known as the hyperbolic tangent activation function（双曲正切函数）. It is similar to the sigmoid function but maps the input to values between -1 and 1. And it addresses the zero-centered problem in sigmoid. The only drawback of tanh is vanishing gradients；

- Rectified Linear Unit (ReLU)：It allows only positive values to pass through it. The negative values are mapped to zero. It address the vanishing gradient problem. ReLU is computationally very efficient because it is implemented using simple thresholding. The drawbacks of ReLU neuron are Not zero-centered and the killing of gradient during the backward pass if x < 0, thus weights do not get updated, and the network does not learn. 

The activation function to be used in Outpout Layer is different for different problems. For a **binary classification** problem, we want the output to be either 0 or 1. Thus, a sigmoid activation function is used. For a **Multiclass classification** problem, a Softmax( think of it as a generalization of sigmoid to multiple classes ) is used. For a **regression** problem, where the output is not a predefined category, we can simply use a linear unit.

没有激活函数用于输出层，因为这是一个回归问题，我们希望直接预测数值，而不需要采用激活函数进行变换。

The most widely used Hidden Layer is the one which uses a Rectified Linear Unit (ReLU) as the activation function.

softmax层中的softmax 函数是logistic函数在多分类问题上的推广，它将一个N维的实数向量压缩成一个满足特定条件的N维实数向。压缩后的向量满足两个条件：

- 像两种的每个元素的大小都在[0,1]
- 所有向量元素的和为1

因此，softmax适用于多分类问题中对每一个类别的概率判断。



####   主流框架

当下最主流的开源深度学习框架当属TensorFlow，Keras，MXNet，PyTorch。

- TensorFlow主要支持静态计算图的形式，计算图的结构比较直观，但是在调试过程中十分复杂与麻烦，一些错误更加难以发。但是2017年底发布了动态图机制Eager Execution，加入对于动态计算图的支持，但是目前依旧采用原有的静态计算图形式为主。TensorFlow拥有TensorBoard应用，可以监控运行过程，可视化计算图；

- Keras是基于多个不同框架的高级API（Application Programming Interface，应用程序编程接口），可以快速的进行模型的设计和建立，同时支持序贯和函数式两种设计模型方式，可以快速的将想法变为结果，但是由于高度封装的原因，对于已有模型的修改可能不是那么灵活；

- MXNet同时支持命令式和声明式两种编程方式，即同时支持静态计算图和动态计算图，并且具有封装好的训练函数，集灵活与效率于一体，同时已经推出了类似Keras的以MXNet为后端的高级接口Gluon；

- PyTorch为动态计算图的典型代表，便于调试，并且高度模块化，搭建模型十分方便，同时具备及其优秀的GPU支持，数据参数在CPU与GPU之间迁移十分灵活。

Python 3.7与TensorFlow 1.10.0存在一些兼容问题，如无特殊必要，建议使用Python 3.6安装TensorFlow。



####标签编码

数据预处理中将分类值进行特征数字化。

1. [LabelEncoder与OneHotEncoder](https://blog.csdn.net/accumulate_zhang/article/details/78510571)

LabelEncoder 是一个可以用来将标签规范化的工具类，它可以将标签的编码值范围限定在[0,n_classes-1]。可以对分类型特征值进行编码，即对不连续的数值或文本进行编码转换成数值标签（只要它们是可哈希并且可比较的）。

但是，这些整数形式的表示不能直接作为某些机器学习算法输入，因为有些机器学习算法是需要连续型的输入数据，同一列数据之间数值的大小可代表差异程度。如： [0, 1, 3]与[0,1,0]的特征差异比[0, 1, 3]与[0,1,2]之间的差异要大，但事实上它们的差异是一样的，都是浏览器使用不一样。

一个解决办法就是采用OneHotEncoder，这种表示方式将每一个分类特征变量的m个可能的取值转变成m个二值特征，对于每一条数据这m个值中仅有一个特征值为1，其他的都为0。

One-Hot编码，又称为一位有效编码，主要是采用![img](http://latex.codecogs.com/gif.latex?N)位状态寄存器来对![img](http://latex.codecogs.com/gif.latex?N)个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。onehot编码是一种方便计算机处理的二元编码。

To convert categorical features to such integer codes, we can use the OrdinalEncoder. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1).

Such integer representation can, however, not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily).

Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.

2. [one-hot encoding与dummy encoding](https://www.cnblogs.com/lianyingteng/p/7792693.html)

热编码与哑变量编码

pandas使用get_dummies进行one-hot编码的方法

离散特征的编码分为两种情况：

- 离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码

- 离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X:1,XL:2,XXL:3}

3. keras.utils.to_categorical与pd.get_dummies

要求数据类型为int，如果是字符串的话需要转换成int。

Converts a class vector (integers) to binary class matrix. 

E.g. for use with categorical_crossentropy.

也就是说它是对于一个类型的容器（整型）的转化为二元类型矩阵。比如用来计算多类别交叉熵来使用的

~~~python
>>> from sklearn import preprocessing
>>> le = preprocessing.LabelEncoder()
>>> le.fit([1,2,2,6])
LabelEncoder()
>>> le.transform([1,2,2,6])
array([0, 1, 1, 2])
>>> le.fit_transform([1,2,2,6])
array([0, 1, 1, 2])
>>> le.inverse_transform([0,1,1,2])
array([1, 2, 2, 6])
>>> from keras.utils import np_utils
Using TensorFlow backend.
>>> onehot_le = np_utils.to_categorical([0,1,1,2])
>>> onehot_le
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)
>>> type(onehot_le)
<class 'numpy.ndarray'>
>>> import pandas as pd
>>> dummy_le = pd.get_dummies([0,1,1,2])
>>> dummy_le
   0  1  2
0  1  0  0
1  0  1  0
2  0  1  0
3  0  0  1
>>> type(dummy_le)
<class 'pandas.core.frame.DataFrame'>
~~~



### keras

Regression Tutorial with the Keras Deep Learning Library in Python

Keras 是一个深度学习库，它封装了高效的数学运算库 Theano 和 TensorFlow，因此Keras 兼容两种后端，即Theano 和TensorFlow。

Keras的优点如模块化，模型的各个部分，如神经层、成本函数、优化器、初始化、激活函数、规范化都是独立的模块，可以组合在一起来创建模型。 

Keras 的核心数据结构是模型。模型是用来组织网络层的方式。模型有两种，一种叫 Sequential 模型，另一种叫Model 模型。Sequential 模型是一系列网络层按顺序构成的栈，是单输入和单输出的，层与层之间只有相邻关系，是最简单的一种模型。Model 模型是用来建立更复杂的模型的。多输入多输出，层与层之间任意连接。





Keras 源代码中包含很多示例，例如： 

- CIFAR10—图片分类（使用CNN 和实时数据）；
- IMDB—电影评论观点分类（使用LSTM）；
- Reuters—新闻主题分类（使用多层感知器）；
- MNIST—手写数字识别（使用多层感知器和CNN）；
- OCR—识别字符级文本生成（使用LSTM）。



#### 模块结构与流程

keras的模块结构

![âkeras æ¶æ§âçå¾çæç´¢ç»æ](http://www.tensorflownews.com/wp-content/uploads/2018/03/006.jpg)

keras的流程

![img](https://images2015.cnblogs.com/blog/1119747/201707/1119747-20170707133932722-715494711.png)



#### Keras Workflow

Keras provides a very simple workflow for training and evaluating the models. It is described with the following diagram.

![keras-workflow](https://www.learnopencv.com/wp-content/uploads/2017/09/keras-workflow.jpg)



#### one-hot encoding

Convert the labels from integer to categorical ( one-hot ) encoding since that is the format required by Keras to perform multiclass classification. One-hot encoding is a type of boolean representation of integer data. It converts the integer to an array of all zeros except a 1 at the index of the integer.

标签向量（vector）转化为二维矩阵。返回维度为 len(y) * num_classes ，只在出现对应标签的那一列为1，其余为0的布尔矩阵。

~~~python
from keras.utils import np_utils

nb_classes = 4

# ValueError: Error when checking target: expected dense_30 to have shape (4,) but got array with shape (1,)
# one-hot encoding
# 独热编码又称为一位有效位编码，将类别向量转换为独热编码的二进制类别矩阵
print("y_train shape=",y_train.shape)
print("y_test ndim=",y_test.ndim)
y_train = np_utils.to_categorical(y_train,nb_classes)
y_test = np_utils.to_categorical(y_test,nb_classes)
print("y_train shape=",y_train.shape)
print("y_test ndim=",y_test.ndim)
print(y_train[:5])

# y_train shape= (308,)
# y_test ndim= 1
# y_train shape= (308, 4)
# y_test ndim= 2
# [[0. 0. 1. 0.]
#  [0. 0. 1. 0.]
#  [0. 0. 1. 0.]
#  [0. 0. 1. 0.]
#  [0. 0. 1. 0.]]
~~~



#### Keras Models

Keras provides two ways to define a model:

- [Sequential](https://keras.io/getting-started/sequential-model-guide/), used for stacking up layers – Most commonly used.
- [Functional API](https://keras.io/getting-started/functional-api-guide/), used for designing complex model architectures like models with multiple-outputs, shared layers etc.

For creating a Sequential model, we can either pass the list of layers as an argument to the constructor or add the layers sequentially using the model.add() function.

An important thing to note in the model definition is that we need to specify the input shape for the first layer. This is done in the above snippet using the input_shape parameter passed along with the first Dense layer. The shapes of other layers are inferred by the compiler.



#### Keras Layers

[《Keras快速上手：基于Python的深度学习实战》—网络层构造](https://www.jianshu.com/p/e15bda2e4ebf)

在Keras中，定义神经网络的具体结构是通过组织不同的网络层(Layer)来实现的。

Layers can be thought of as the building blocks of a Neural Network. They process the input data and produce different outputs, depending on the type of layer, which are then used by the layers which are connected to them. 

**核心层(Core Layers)**是构成神经网络最常用的网络层的集合，包括：全连接层、激活层、放弃层、扁平化层、重构层、排列层、向量反复层、Lambda 层、激活值正则化层、掩盖层。所有的层都包含一个输入端和一个输出端，中间包含激活函数以及其他相关参数等。

Keras provides a number of core layers which include：

- **Dense layers**, also called fully connected layer, since, each node in the input is connected to every node in the output 在神经网络中最常见的网络层就是全连接层，在这个层中实现对神经网络里面的神经元的激活；
- Activation layer which includes activation functions like ReLU, tanh, sigmoid among others, 激活层是对上一层的输出应用激活函数的网络层，这是除应用activation选项之外，另一种指定激活函数的方式；
- Dropout layer, used for regularization during training 放弃层(Dropout)是对该层的输入向量应用放弃策略。在模型训练更新参数的步骤中，网络的某些隐含层节点按照一定比例随机设置为不更新状态，但是权重仍然保留，从而防止过度拟合；
- Flatten, 用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。扁化层(Flatten)是将一个维度大于或等于3 的高维矩阵按照设定“压扁”为一个二维的低维矩阵。其压缩方法是保留第一个维度的大小，然后将所有剩下的数据压缩到第二个维度中，因此第二个维度的大小是原矩阵第二个维度之后所有维度大小的乘积；
- Reshape, 重构层(Reshape)的功能和Numpy 的Reshape 方法一样，将一定维度的多维矩阵重新排列构造为一个新的保持同样元素数量但是不同维度尺寸的矩阵。其参数为一个元组(tuple)，指定输出向量的维度尺寸，最终的向量输出维度的第一个维度的尺寸是数据批量的大小，从第二个维度开始指定输出向量的维度大小；
- etc.

Apart from these core layers, some important layers are：

- Convolution layers – used for performing convolution；
- Pooling layers – used for down sampling；
- Recurrent layers；
- Locally-connected, normalization, etc.



#### 全连接层 Dense

[**keras层**](https://keras-cn.readthedocs.io/en/latest/layers/core_layer/)

全连接层(Fully Connected layer)就是使用了softmax激励函数作为输出层的多层感知机(Multi-Layer Perceptron)，其他很多分类器如支持向量机也使用了softmax。“全连接”表示上一层的每一个神经元，都和下一层的每一个神经元是相互连接的。

~~~python
keras.layers.core.Dense(units, # 大于0的整数，神经元个数，该层输出数据的维度
input_dim, # 输入数据的维度。这个参数会在模型的第一层中用到
activation=None, # 激活函数，为预定义的激活函数名（参考激活函数），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）
use_bias=True, # 布尔值，是否使用偏置项
kernel_initializer='glorot_uniform', # 权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers; kernel_initializer='uniform'，表示使用均匀分布来初始化权重向量，类似的选项也可以用在偏置项上。
bias_initializer='zeros', # 偏置向量初始化方法
kernel_regularizer=None, # 施加在权重上的正则项，为Regularizer对象
bias_regularizer=None, # 施加在偏置向量上的正则项，为Regularizer对象
activity_regularizer=None, # 施加在输出上的正则项，为Regularizer对象
kernel_constraint=None, # 施加在权重上的约束项，为Constraints对象
bias_constraint=None) # 施加在偏置上的约束项，为Constraints对象

# Dense 的输入输出维度
# inputshape: 2维 tensor(nb_samples, input_dim)
# outputshape: 2维 tensor(nb_samples, output_dim)
~~~

全连接层所实现的运算是output =activation(dot(input, kernel)+bias)。其中activation是逐元素计算的激活函数，kernel是本层的权值矩阵，bias为偏置向量，只有当use_bias=True才会添加。如果本层的输入数据的维度大于2，则会先被压为与kernel相匹配的大小。units参数是大于0的整数，代表该层的输出维度。input_shape代表输入维度，如input_shape=(13,)等价于input_dim=13。



#### dropout

Dropout的做法是在训练过程中随机地忽略一些神经元。这些神经元被随机地“抛弃”了。也就是说它们在正向传播过程中对于下游神经元的贡献效果暂时消失了，反向传播时该神经元也不会有任何权重的更新。

Dropout是Srivastava等人在2014年的一篇论文中提出的一种针对神经网络模型的正则化方法 Dropout: A Simple Way to Prevent Neural Networks from Overfitting。论文提供了一些在标准机器学习问题上得到的实践性结论。这些结论在dropout的实际应用中会带来帮助。

通常丢弃率控制在20%~50%比较好，可以从20%开始尝试。如果比例太低则起不到效果，比例太高则会导致模型的欠学习。在大的网络模型上应用。当dropout用在较大的网络模型时更有可能得到效果的提升，模型有更多的机会学习到多种独立的表征。在输入层（可见层）和隐藏层都使用dropout。在每层都应用dropout被证明会取得好的效果。





#### Configuring the training process

Once the model is ready, we need to configure the learning process. This means

- Specify an Optimizer which determines how the network weights are updated
- Specify the type of cost function or loss function.
- Specify the metrics you want to evaluate during training and testing.
- Create the model graph using the backend.
- Any other advanced configuration.

This is done in Keras using the model.compile() function. 

The mandatory parameters to be specified are the optimizer and the loss function.

如：model.compile(optimizer="rmsprop",loss="mse",metrics=["mse","mae"])



#### 优化器 Optimizer

https://blog.csdn.net/weixin_40170902/article/details/80092628

目标函数就是我们常说的损失函数，优化函数就是我们常说的反调参数的函数，包括：梯度下降函数、随机梯度下降函数等。

机器学习中，有很多优化方法来试图寻找模型的最优解。比如神经网络中可以采取最基本的梯度下降法。

- 梯度下降法。梯度下降法是最基本的一类优化器，目前主要分为三种梯度下降法：标准梯度下降法(GD, Gradient Descent)，随机梯度下降法(SGD, Stochastic Gradient Descent)及批量梯度下降法(BGD, Batch Gradient Descent)。
- 动量优化法。动量优化方法是在梯度下降法的基础上进行的改变，具有加速梯度下降的作用。一般有标准动量优化方法Momentum、NAG（Nesterov accelerated gradient）动量优化方法。
- 自适应学习率优化算法。自适应学习率优化算法针对于机器学习模型的学习率，传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率。极大忽视了学习率其他变化的可能性。然而，学习率对模型的性能有着显著的影响，因此需要采取一些策略来想办法更新学习率，从而提高训练速度。目前的自适应学习率优化算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。

RMSprop is a good choice of optimizer for most problems.



#### Loss functions

损失函数（或称目标函数、优化评分函数）是编译模型时所需的两个参数之一。可以传递一个现有的损失函数名，或者一个 TensorFlow/Theano 符号函数。 该符号函数为每个数据点返回一个标量，有以下两个参数:

- **y_true**: 真实标签。TensorFlow/Theano 张量。
- **y_pred**: 预测值。TensorFlow/Theano 张量，其 shape 与 y_true 相同。

实际的优化目标是所有数据点的输出数组的平均值。

In a supervised learning problem, we have to find the error between the actual values and the predicted value. There can be different metrics which can be used to evaluate this error. This metric is often called loss function or cost function or objective function. There can be more than one loss function depending on what you are doing with the error. In general, we use

- binary-cross-entropy for a binary classification problem, 常说的逻辑回归, 就是常用的交叉熵函数
- categorical-cross-entropy for a multi-class classification problem, 多分类的逻辑， 交叉熵函数的一种变形

- mean-squared-error（mse） for a regression problem and so on. 均方误差，常用的目标函数

交叉熵（cross-entropy）是神经网络中常用的损失函数。交叉熵满足非负性，因此当预测值Yi和真实值Y' 越接近时，两者的乘积越大，coss-entropy越小。如果对于所有的权重和所有的偏置计算交叉熵的偏导数，就得到一个对于给定图像、标签和当前权重和偏置的梯度。

注意当使用 categorical_crossentropy 损失时，目标值应该是分类格式 (即，如果你有 10 个类，每个样本的目标值应该是一个 10 维的向量，这个向量除了表示类别的那个索引为 1，其他均为 0)。 为了将 整数目标值 转换为 分类目标值，可以使用 Keras 实用函数 to_categorical。

~~~python
from keras.utils.np_utils import to_categorical

categorical_labels = to_categorical(int_labels, num_classes=None)
~~~

#### Training

keras的fit方法类似于sklearn中的train方法，就是模型的训练。

Once the model is configured, we can start the training process. This can be done using the model.fit() function in Keras. 

We just need to specify the training data, batch size and number of epochs. Keras automatically figures out how to pass the data iteratively to the optimizer for the number of epochs specified. The rest of the information was already given to the optimizer in the previous step.



model.summary()输出模型各层的参数状况。通过这些参数，可以看到模型各个层的组成（dense表示全连接层）。也能看到数据经过每个层后，输出的数据维度。还能看到Param，它表示每个层参数的个数。

全连接层神经网络的Param参数说明的是每层神经元权重的个数，所以它的计算如下：

Param = （输入数据维度+1）* 神经元个数。之所以要加1，是考虑到每个神经元都有一个Bias。

如第一个Dense层，输入数据维度是4（一维数据），有7个神经元。所以，Param=(4+1)*7=35.



#### validation_data与validation_split

验证集是从训练集中抽取出来用于调参的，而测试集是和训练集无交集的，用于测试所选参数用于该模型的效果。

Every time you use feedback from your validation process to tune your model, you leak information about the validation process into the model. Repeated just a few times, this is innocuous; but done systematically over many iterations, it will eventually cause your model to overfit to the validation process (even though no model is directly trained on any of the validation data). This makes the evaluation process less reliable.

model.fit()中的validation_data并不用于更新权重，只是用是来检测loss和accuracy等指标的。将测试集作为validation_data传入时，即使模型没有直接在validation data上训练，这也会导致信息泄露，模型会对validation data逐渐熟悉，最终导致**过拟合**，正确的做法是：

- 用sklearn的train_test_split来把数据分割为training data和test data.
- 用keras的模型fit时，不要使用validation_data这个参数（因为我们也没有准备validatoin data），而是直接使用validation_split这个参数，把training data中的一部分用来作为validation data就行了。
- 上面两步的目的是用来调参的，必须在validation data上进行验证，输出loss。
- 调参：更改layer，unit，加dropout，使用L2正则化，添加新feature等等
- 等调参结束后，拿着我们满意的参数，再一次在整个training data上进行训练，这一次就不用validation_split了。因为我们已经调好了参数，不需要观察输出的loss。
- 训练完之后，用model.evaluate()在test data上进行预测。

也就是说，实际使用时，validation_data + train_test_split == validation_split。



[训练过程的workflow](https://blog.csdn.net/u010167269/article/details/51340070)：

~~~python
for each epoch
    for each training data instance
        propagate error through the network
        adjust the weights
        calculate the accuracy over training data
    for each validation data instance
        calculate the accuracy over the validation data
    if the threshold validation accuracy is met
        exit training
    else
        continue training
~~~

因此，validation sets并不是用作调整weights，而是用作防止overfitting（过拟合）的。如果由training sets得到的精度随着训练的进行在增加，而这个模型经过validation sets计算后，发现精度与之前保持不变，或者精度反而下降了。这说明，已经产生overfitting了，需要停止训练（early stopping）。也就是让这个model在training sets与validation sets之间trade-off，更balance。

validation sets的作用不仅仅是在训练中防止训练模型过拟合，平衡training accuracy与validation accuracy，而且有“compare their performances and decide which one to take”。



#### batch_size

batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.

Batch Size定义：一次训练所选取的样本数。 

Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况。

[训练神经网络时如何确定batch size？](https://www.jiqizhixin.com/articles/2018-07-12-4)

在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播。在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。

可以一次性将整个数据集喂给神经网络，让神经网络利用全部样本来计算迭代时的梯度（即传统的梯度下降法），也可以一次只喂一个样本（即随机梯度下降法，也称在线梯度下降法），也可以取个折中的方案，即每次喂一部分样本让其完成本轮迭代（即batch梯度下降法）。mini-batch利用噪声梯度，一定程度上缓解了GD算法直接掉进初始点附近的局部最优值。同时梯度准确了，学习率要加大。 

在工程实际中，得益于GPU里面超多的核，超强的并行计算能力，使用GPU训练时，跑一个样本花的时间与跑几十个样本甚至几百个样本的时间是一样。从收敛速度的角度来说，小批量的样本集是最优的，也就是我们所说的mini-batch。这时的batch size往往从几十到几百不等，但一般不会超过几千。

基于默认一阶导数的梯度下降法，样本量很多时，方差小，容易收敛至局部最优点。样本量很小时，方差大，反而有机会去寻找更优点最优点。对于二阶优化算法如共轭梯度法，减小batch换来的收敛速度提升远不如引入大量噪声导致的性能下降，因此在使用二阶优化算法时，往往要采用大batch。此时往往batch设置成几千甚至一两万才能发挥出最佳性能。

听说GPU对2的幂次的batch可以发挥更佳的性能。

Batch Size增大时，要到达相同的准确度，必须要增大epoch。



#### callbacks

callbacks回调函数用于指定在每个epoch开始和结束的时候进行哪种特定操作。callback回调下有很多，如：

- early stop，当训练集上的loss不再减小（即减小的程度小于某个阈值）的时候停止继续训练；
- 最常用的是checkpoint，在每个epoch后保存模型到filepath。可以通过checkpoint来记录训练过程，或者是最好的权重。ModelCheckpoint断点，可以保存最好模型，防止过拟合；
- ReduceLROnPlateau，如果在patience个epoch中看不到模型性能即评价指标提升，则减少学习率。

~~~python
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

### 得到不平衡样本对应的 class_weight
def getClassWeight(y):
    class_weights = compute_class_weight('balanced', np.unique(y), y)
    return class_weights

def train(model, x, y, load_weight, MODEL_PATH):
    x_np = np.array(x)
    y_np = np_utils.to_categorical(y, 4)

    # get training and test dataset
    x_train, x_test, y_train, y_test = train_test_split(x_np, y_np, test_size=0.2)

    EPOCHS = 300
    # BATCH_SIZE = 1024
    # memeory error
    BATCH_SIZE = 256 # 通常是2的幂值

    if (load_weight == True):
        model.load_weights(MODEL_PATH)

    early_stop = EarlyStopping(monitor="val_loss", patience=20)
    # 如果在patience个epoch中看不到模型性能即评价指标提升，则减少学习率
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')
    checkpointer = ModelCheckpoint(filepath="gg.weights.best.hdf5",verbose=1,save_best_only=True)

    class_weights = getClassWeight(y)

    history = model.fit(x_train, y_train,
              class_weight=class_weights,
              epochs=EPOCHS, batch_size=BATCH_SIZE,
              validation_data=(x_test, y_test), # validation_split=0.2
              verbose=2, callbacks=[PrintDot(), early_stop, reduce_lr, checkpointer])
	
    # model.summary()输出模型各层的参数状况
	model.summary()
    # print(history.history.keys())
    historyCurves(history)
    model.save(MODEL_PATH)
    
    # 模型训练，挑选在验证集上准确率最高的模型。其实是从文件中加载权重
    # load the weights that yielded the best validation accuracy
    # by_name=False 的时候按照网络的拓扑结构加载权重，
    # by_name=True 的时候就是按照网络层名称进行加载
    # 我上面的层没有取名字，所以by_name=True的时候估计就是找不到层，
    # 就直接给我随机初始化，才会导致结果很差而且每一次预测结果都不一样。所以改过来之后预测结果就稳定而且还不错
    # model.load_weights("gg.weights.best.hdf5",by_name=True)
~~~

#### sklearn.utils.class_weight

<https://blog.csdn.net/yanhe156/article/details/90732378>

<https://blog.csdn.net/ssswill/article/details/90203428>

~~~python
>>> y = [1,1,1,1,2,2,2,3,3,4]
>>> np.unique(y)
array([1, 2, 3, 4])
>>> from sklearn.utils.class_weight import compute_class_weight
>>> class_weight = compute_class_weight("balanced", np.unique(y), y)
>>> class_weight
array([0.625     , 0.83333333, 1.25      , 2.5       ])
~~~

zip

<https://www.cnblogs.com/anita-harbour/p/9328597.html>

<https://blog.csdn.net/benpaodelulu_guajian/article/details/81869462>



#### history

keras打印history的acc 和loss

在model.fit()训练完以后，其实返回了一个obj，叫做history，保存了训练过程中的所有数据。history类对象包含两个属性，分别为epoch和history，epoch为训练轮数。history.history为字典类型，包含`val_loss`,`val_acc`,`loss`,`acc`四个key值。

The fit() function returns a history object which has a dictionary of all the metrics which were required to be tracked during training. We can use the data in the history object to plot the loss and accuracy curves to check how the training process went.
You can use the history.history.keys() function to check what metrics are present in the history. It should look like the following: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']).

~~~python
import matplotlib.pyplot as plt

# Loss Curves
plt.figure(figsize=[8,6])
plt.plot(history.history["loss"],"r",linewidth=3.0)
plt.plot(history.history["val_loss"],"b",linewidth=3.0)
plt.legend(["Training Loss","Validation Loss"],fontsize=18)
plt.xlabel("Epochs",fontsize=16)
plt.ylabel("Loss",fontsize=16)
plt.title("Loss Curves",fontsize=16)
# plt.show()

# Accuracy Curves
plt.figure(figsize=[8,6])
plt.plot(history.history["acc"],"r",linewidth=3.0)
plt.plot(history.history["val_acc"],"b",linewidth=3.0)
plt.legend(["Training Accuracy","Validation Accuracy"],fontsize=18)
plt.xlabel("Epochs",fontsize=16)
plt.ylabel("Accuracy",fontsize=16)
plt.title("Accuracy Curves",fontsize=16)
~~~

Loss Curves分布如图所示：

![img](data:image/png;base64,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)



#### verbose

在 keras  的fit 和 evaluate 中 都有 verbose 这个参数，参数的取值有所不同。

fit 中的 verbose：

- verbose：日志显示
- verbose = 0 为不在标准输出流输出日志信息
- verbose = 1 为输出进度条记录
- verbose = 2 为每个epoch输出一行记录

注意： 默认为 1。

evaluate 中的 verbose：

- verbose：日志显示
- verbose = 0 为不在标准输出流输出日志信息
- verbose = 1 为输出进度条记录

注意： 只能取 0 和 1；默认为 1



#### Evaluating the model

Once the model is trained, we need to check the accuracy on unseen test data. This can be done in two ways in Keras.

- model.evaluate() – It finds the loss and metrics specified in the model.compile() step. It takes both the test data and labels as input and gives a quantitative measure of the accuracy. It can also be used to perform cross-validation and further finetune the parameters to get the best model.

Return：Keras中model.evaluate()返回的是**损失值**和**选定的指标值**（例如，精度accuracy）。

- model.predict() – It finds the output for the given test data. It is useful for checking the outputs qualitatively.

Return：当使用predict()方法进行预测时，返回值是数值，表示样本属于每一个类别的概率，可以使用numpy.argmax()方法找到样本以最大概率所属的类别作为样本的预测标签。

predict与predict_class



[How does Keras evaluate the accuracy?](https://www.e-learn.cn/content/wangluowenzhang/195401)

**Loss and accuracy** are different things; roughly speaking, the accuracy is what we are actually interested in from a business perspective, while the loss is the objective function that the learning algorithms (optimizers) are trying to minimize from a mathematical perspective. Even more roughly speaking, you can think of the loss as the "translation" of the business objective (accuracy) to the mathematical domain, a translation which is necessary in classification problems (in regression ones, usually the loss and the business objective are the same, or at least can be the same in principle, e.g. the RMSE).

To compute the accuracy, we implicitly set a **threshold** in the predicted probabilities (usually 0.5 in binary classification, but this may differ in the case of highly **imbalanced data**); so, in model.evaluate, Keras actually converts our predictions to 1 if p[i] > 0.5 and to 0 otherwise. Then, the accuracy is computed by simply counting the cases where **y_true==y_pred (correct predictions)** and dividing by the total number of samples, to give a number in [0,1].



In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.

In the case of a multi-class classification problem, the softmax activation function is often used on the output layer and the likelihood of the observation for each class is returned as a vector.



Here’s how to predict with a sklearn model:
https://machinelearningmastery.com/make-predictions-scikit-learn/

Here’s how to predict with a Keras model:
https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/



#### Dense 示例

~~~python
from sklearn.model_selection import train_test_split
import numpy as np

model = baseline_model()
X_train,X_test,Y_train,Y_test = train_test_split(X,dummy_y,test_size=0.3,random_state=0)
model.fit(X_train,Y_train,batch_size=256,epochs=40,verbose=2,validation_split=0.2)
scores = model.evaluate(X_test,Y_test,verbose=2)
# loss + accuracy (compile)
print(scores)
# 预测样本属于每个类别的概率
Y_pred = model.predict(X_test)
# 以最大概率所属的类别作为样本的预测标签，axis=1代表行，axis=0代表列
Y_arg = np.argmax(Y_pred,axis=1)

# 字符串，标签向量，一维
print(Y[:5])
# 数字，LabelEncoder，标签向量，一维
print(encoder_Y[:5])
# one-hot，二维矩阵
print(Y_test[:5])
# 概率，小数的列表，二维矩阵
print(Y_pred[:5])
# one-hot标签向量，一维
print(Y_arg[:5])
# 没有返回字符串
~~~

#### KerasClassifier 示例

~~~python
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import train_test_split,KFold,cross_val_score

estimator = KerasClassifier(build_fn=baseline_model,batch_size=256,epochs=40)
X_train,X_test,Y_train,Y_test = train_test_split(X,dummy_y,test_size=0.3,random_state=0)
estimator.fit(X_train,Y_train)

# 二维矩阵
print(Y_test[:5])
pred = estimator.predict(X_test)
# 数字，返回的是概率列表中的最大值所指代的类 integers
print(pred[:5])
init_labels = encoder.inverse_transform(pred)
# 字符串，返回类 integers 对应的 class values
print(init_labels[:5])
# accuracy
score = estimator.score(X_test,Y_test)
print(score)
~~~

#### 1D CNN 示例





#### scikit-learn

使用交叉验证以更好地验证模型。

keras目前没有提供交叉验证的功能，要向使用交叉验证，就需要与sklearn结合。keras也提供了这样的包装接口。keras.wrappers.scikit_learn通过这个包里面的KerasClassifier或者KerasRegressor就可以结合。

Arguments

- build_fn: callable function or class instance
- sk_params: model parameters & fitting parameters

sk_params takes both model parameters and fitting parameters. Legal model parameters are the arguments of build_fn. Note that like all other **estimators in scikit-learn**, build_fn should provide default values for its arguments, so that you could create the estimator without passing any values to sk_params.

sk_params could also accept parameters for calling fit, predict, predict_proba, and score methods (e.g., epochs, batch_size). 

When using scikit-learn's grid_search API, legal tunable parameters are those you could pass to sk_params, including fitting parameters. In other words, you could use grid_search to search for the best batch_size or epochs as well as the model parameters.

[利用sklearn API GridSearchCV 进行keras参数调优](https://blog.csdn.net/u010159842/article/details/79317828)

参数包括：batch_size、epochs、optimizer、learning_rate、momentum学习速率和动量因子、网络权值初始化、神经元激活函数、Dropout正则化、隐藏层中的神经元的数量等。

为什么GridSearchCV带上scoring="accuracy"参数后会报错？

~~~python

# ValueError: Classification metrics can't handle a mix of multilabel-indicator and binary targets 
#our targets are one-hot encoded that is why sklearn is confused and throwing up an error about multilabel indicator, you need to use raw arrays not one-hot encoded.
scores = cross_val_score(estimator,x_train,y_train,cv=kfold,scoring="accuracy")
# scores = cross_val_score(estimator,x_train,y_train,cv=kfold)

grid_search = GridSearchCV(estimator=estimator,param_grid=tune_params,
#                           scoring="accuracy",
                          	cv=10,)
~~~





### 机器学习未分类

#### 支持向量机

支持向量机（Support Vector Machine,SVM）是由 Vapnik 在 1995 年率先提出的一种基于统计学习理论上的模式识别方法，其主要思想是通过非线性变化将样本数据映射到高维空间，在高维空间建立一个分类超平面作为决策曲面，这个超平面可以将正反两类样本数据分开，使得经验风险最小并且分类间隔最大。其假定为，平行超平面间的距离或差距越大，分类器的总误差越小。

距离分类面最近的训练样本被称为“支持向量”，要使距两个不同的支持向量到分类面的距离和最大，相当于最小化二次项的分母，因此，确定最大间隔的分类面相当于求解一个凸二次规划的问题。

什么叫分隔超平面？如果数据的N维的，分隔超平面就是N-1维的，比如给出的数据点分布在二维平面之中，那分割超平面就是一条直线。当新样本点进来时，计算样本点到分隔超平面的函数间隔，函数间隔为正，分类正确，为负则分类错误。函数间隔的绝对值除以||w||就是几何间隔，几何间隔始终为正，可以理解为样本点到分隔超平面的几何距离。||w||是向量w的2-范数，即向量w元素绝对值的平方和再开方。

支持向量机是通过少数的支撑向量来决定最终决策函数，并且计算的复杂程度不取决于样本数据空间的维数而是由支撑向量个数的多少确定，这种独特优势在某种意义上解决了维度灾难问题。支持向量机法很好解决了神经网络法遇到的局部极值问题，因为它将问题变换成凸二次规划来求解，这样一来所得的极值点即为全局的最优点。

寻求一个最优的惩罚因子C 让超平面统筹训练错误与泛化能力。使得分类器没有无限的追求错分最小化，从而可以使超平面有较大的分类间隔，分类器也具备较强的泛化能力，在应用实际的过程中取得良好的效果。

核函数用于实现映射。由于线性分类器的应用能力有限，解决非线性问题时若不断放松约束条件，会造成样本在分类时众多的错误，此时使用转换将问题变成在某高维空间的线性问题并且在这个空间寻找到最佳分类超平面。核函数就是解决低维空间样本经变换映射到高维空间的内积问题，可以大大减少计算量，避免了“维数灾难”。径向基核函数又被称为高斯（RBF）核函数，是目前使用最广泛的核函数。

相比于传统的交会图法和人工神经元网络，支持向量机在测井解释领域具有很大优越性。支持向量机通过核函数将输入样本映射到维度更高的空间，将一个非线性的问题转换成线性问题来解决。支持向量机在实际应用中还是存在一些问题亟待解决，其中最重要的问题就是，核函数的选取、核函数参数以及惩罚因子 C 的选值问题。选取交叉验证法作为关键参数寻优的方法。

在交叉验证之前，需要对数据体进行归一化处理，因为在测井解释中每口井的各种测井曲线间都会存在量纲不同的问题，若要避免量纲差异对分类器性能的影响，就需要对数据标准化处理（即归一化处理）将训练样本限制到[－1，1]或[0， 1]中，方便了数据的处理而且保证了程序运行时能够更快的收敛。

SVM的缺点：

- 核函数的选择缺乏理论依据；
- 二次规划算法求解支持向量导致数据规模较大；
- 多分类问题。SVM主要针对解决二分类的问题，对于多分类问题，虽然可以通过组合分类器的方式来解决多分类问题，但是效率并不高。

SVM的优势在于，它不用保留所用的训练样本用于计算，而只用保留那几个支持向量就够了，极大地减小了内存的占用，而效果却不差。



### 回归 Regression

回归是用观察使得认知接近真值的过程。



一般来说平均方差(Mean squared error)会用于判断回归(Regression)模型的好坏。



回归分析(Regression analysis)是分析自变量与因变量之间定量的因果关系，并且用回归方程描述。

回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。

最简单的回归方法是最小二乘法。





#### 回归评价指标

http://blog.sina.com.cn/s/blog_628033fa0100kjjy.html

- SSE(和方差、误差平方和)：The sum of squares due to error
- MSE(均方差、方差)：Mean squared error
- RMSE(均方根、标准差)：Root mean squared error
- R-square(确定系数)：Coefficient of determination
- Adjusted R-square：Degree-of-freedom adjusted coefficient of determination

其中前三个参数SSE、MSE、RMSE计算的是预测值(y_hat)和原始值(y)之间的误差(即点对点)，后两个参数R-square、Adjusted R-square计算的是预测数据与原始数据均值之间的误差(即点对全)。

R2公式为：R2=SSR/SST=1-SSE/SST，其中：

- SST (total sum of squares)：总平方和；

- SSR (regression sum of squares)：回归平方和；

- SSE (error sum of squares) ：残差平方和。

R-squared在线性回归以及广义线性回归中，R-squared误差的大小意味着模型的拟合度的好坏。R-squared误差取值范围为0到1，这个值越接近1说明模型的拟合度越好。



#### logistic 回归模型

logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。

线性回归是回归，逻辑回归是分类。逻辑回归通过logistic函数算概率，然后算出来一个样本属于一个类别的概率，概率越大越可能是这个类的样本。逻辑回归是分类问题，score使用准确率做为评价标准。

逻辑回归使用OVR多分类方法，OvR把多元逻辑回归，看做二元逻辑回归。具体做法是，每次选择一类为正例，其余类别为负例，然后做二元逻辑回归，得到第该类的分类模型。最后得出多个二元回归模型。按照各个类别的得分得出分类结果。











逻辑回归与线性回归

https://www.cnblogs.com/tbcaaa8/p/4415429.html

https://blog.51cto.com/12597095/2093869

